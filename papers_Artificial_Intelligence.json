[
    {
        "title": "Computational Theories of Curiosity-Driven Learning",
        "abstract": "What are the functions of curiosity? What are the mechanisms of\ncuriosity-driven learning? We approach these questions about the living using\nconcepts and tools from machine learning and developmental robotics. We argue\nthat curiosity-driven learning enables organisms to make discoveries to solve\ncomplex problems with rare or deceptive rewards. By fostering exploration and\ndiscovery of a diversity of behavioural skills, and ignoring these rewards,\ncuriosity can be efficient to bootstrap learning when there is no information,\nor deceptive information, about local improvement towards these problems. We\nalso explain the key role of curiosity for efficient learning of world models.\nWe review both normative and heuristic computational frameworks used to\nunderstand the mechanisms of curiosity in humans, conceptualizing the child as\na sense-making organism. These frameworks enable us to discuss the\nbi-directional causal links between curiosity and learning, and to provide new\nhypotheses about the fundamental role of curiosity in self-organizing\ndevelopmental structures through curriculum learning. We present various\ndevelopmental robotics experiments that study these mechanisms in action, both\nsupporting these hypotheses to understand better curiosity in humans and\nopening new research avenues in machine learning and artificial intelligence.\nFinally, we discuss challenges for the design of experimental paradigms for\nstudying curiosity in psychology and cognitive neuroscience.\n  Keywords: Curiosity, intrinsic motivation, lifelong learning, predictions,\nworld model, rewards, free-energy principle, learning progress, machine\nlearning, AI, developmental robotics, development, curriculum learning,\nself-organization.",
        "url": "http://arxiv.org/pdf/1802.10546v2.pdf"
    },
    {
        "title": "Biased Embeddings from Wild Data: Measuring, Understanding and Removing",
        "abstract": "Many modern Artificial Intelligence (AI) systems make use of data embeddings,\nparticularly in the domain of Natural Language Processing (NLP). These\nembeddings are learnt from data that has been gathered \"from the wild\" and have\nbeen found to contain unwanted biases. In this paper we make three\ncontributions towards measuring, understanding and removing this problem. We\npresent a rigorous way to measure some of these biases, based on the use of\nword lists created for social psychology applications; we observe how gender\nbias in occupations reflects actual gender bias in the same occupations in the\nreal world; and finally we demonstrate how a simple projection can\nsignificantly reduce the effects of embedding bias. All this is part of an\nongoing effort to understand how trust can be built into AI systems.",
        "url": "http://arxiv.org/pdf/1806.06301v1.pdf"
    },
    {
        "title": "Versatile Auxiliary Classifier with Generative Adversarial Network (VAC+GAN)",
        "abstract": "One of the most interesting challenges in Artificial Intelligence is to train\nconditional generators which are able to provide labeled adversarial samples\ndrawn from a specific distribution. In this work, a new framework is presented\nto train a deep conditional generator by placing a classifier in parallel with\nthe discriminator and back propagate the classification error through the\ngenerator network. The method is versatile and is applicable to any variations\nof Generative Adversarial Network (GAN) implementation, and also gives superior\nresults compared to similar methods.",
        "url": "http://arxiv.org/pdf/1805.00316v3.pdf"
    },
    {
        "title": "Hardware Trojan Attacks on Neural Networks",
        "abstract": "With the rising popularity of machine learning and the ever increasing demand\nfor computational power, there is a growing need for hardware optimized\nimplementations of neural networks and other machine learning models. As the\ntechnology evolves, it is also plausible that machine learning or artificial\nintelligence will soon become consumer electronic products and military\nequipment, in the form of well-trained models. Unfortunately, the modern\nfabless business model of manufacturing hardware, while economic, leads to\ndeficiencies in security through the supply chain. In this paper, we illuminate\nthese security issues by introducing hardware Trojan attacks on neural\nnetworks, expanding the current taxonomy of neural network security to\nincorporate attacks of this nature. To aid in this, we develop a novel\nframework for inserting malicious hardware Trojans in the implementation of a\nneural network classifier. We evaluate the capabilities of the adversary in\nthis setting by implementing the attack algorithm on convolutional neural\nnetworks while controlling a variety of parameters available to the adversary.\nOur experimental results show that the proposed algorithm could effectively\nclassify a selected input trigger as a specified class on the MNIST dataset by\ninjecting hardware Trojans into $0.03\\%$, on average, of neurons in the 5th\nhidden layer of arbitrary 7-layer convolutional neural networks, while\nundetectable under the test data. Finally, we discuss the potential defenses to\nprotect neural networks against hardware Trojan attacks.",
        "url": "http://arxiv.org/pdf/1806.05768v1.pdf"
    },
    {
        "title": "The IQ of Artificial Intelligence",
        "abstract": "All it takes to identify the computer programs which are Artificial\nIntelligence is to give them a test and award AI to those that pass the test.\nLet us say that the scores they earn at the test will be called IQ. We cannot\npinpoint a minimum IQ threshold that a program has to cover in order to be AI,\nhowever, we will choose a certain value. Thus, our definition for AI will be\nany program the IQ of which is above the chosen value. While this idea has\nalready been implemented in [3], here we will revisit this construct in order\nto introduce certain improvements.",
        "url": "http://arxiv.org/pdf/1806.04915v1.pdf"
    },
    {
        "title": "Lecture Notes on Fair Division",
        "abstract": "Fair division is the problem of dividing one or several goods amongst two or\nmore agents in a way that satisfies a suitable fairness criterion. These Notes\nprovide a succinct introduction to the field. We cover three main topics.\nFirst, we need to define what is to be understood by a \"fair\" allocation of\ngoods to individuals. We present an overview of the most important fairness\ncriteria (as well as the closely related criteria for economic efficiency)\ndeveloped in the literature, together with a short discussion of their\naxiomatic foundations. Second, we give an introduction to cake-cutting\nprocedures as an example of methods for fairly dividing a single divisible\nresource amongst a group of individuals. Third, we discuss the combinatorial\noptimisation problem of fairly allocating a set of indivisible goods to a group\nof agents, covering both centralised algorithms (similar to auctions) and a\ndistributed approach based on negotiation.\n  While the classical literature on fair division has largely developed within\nEconomics, these Notes are specifically written for readers with a background\nin Computer Science or similar, and who may be (or may wish to be) engaged in\nresearch in Artificial Intelligence, Multiagent Systems, or Computational\nSocial Choice. References for further reading, as well as a small number of\nexercises, are included.\n  Notes prepared for a tutorial at the 11th European Agent Systems Summer\nSchool (EASSS-2009), Torino, Italy, 31 August and 1 September 2009. Updated for\na tutorial at the COST-ADT Doctoral School on Computational Social Choice,\nEstoril, Portugal, 9--14 April 2010.",
        "url": "http://arxiv.org/pdf/1806.04234v1.pdf"
    },
    {
        "title": "Relational inductive biases, deep learning, and graph networks",
        "abstract": "Artificial intelligence (AI) has undergone a renaissance recently, making\nmajor progress in key domains such as vision, language, control, and\ndecision-making. This has been due, in part, to cheap data and cheap compute\nresources, which have fit the natural strengths of deep learning. However, many\ndefining characteristics of human intelligence, which developed under much\ndifferent pressures, remain out of reach for current approaches. In particular,\ngeneralizing beyond one's experiences--a hallmark of human intelligence from\ninfancy--remains a formidable challenge for modern AI.\n  The following is part position paper, part review, and part unification. We\nargue that combinatorial generalization must be a top priority for AI to\nachieve human-like abilities, and that structured representations and\ncomputations are key to realizing this objective. Just as biology uses nature\nand nurture cooperatively, we reject the false choice between\n\"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an\napproach which benefits from their complementary strengths. We explore how\nusing relational inductive biases within deep learning architectures can\nfacilitate learning about entities, relations, and rules for composing them. We\npresent a new building block for the AI toolkit with a strong relational\ninductive bias--the graph network--which generalizes and extends various\napproaches for neural networks that operate on graphs, and provides a\nstraightforward interface for manipulating structured knowledge and producing\nstructured behaviors. We discuss how graph networks can support relational\nreasoning and combinatorial generalization, laying the foundation for more\nsophisticated, interpretable, and flexible patterns of reasoning. As a\ncompanion to this paper, we have released an open-source software library for\nbuilding graph networks, with demonstrations of how to use them in practice.",
        "url": "http://arxiv.org/pdf/1806.01261v3.pdf"
    },
    {
        "title": "Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets",
        "abstract": "Visual question answering (Visual QA) has attracted a lot of attention\nlately, seen essentially as a form of (visual) Turing test that artificial\nintelligence should strive to achieve. In this paper, we study a crucial\ncomponent of this task: how can we design good datasets for the task? We focus\non the design of multiple-choice based datasets where the learner has to select\nthe right answer from a set of candidate ones including the target (\\ie the\ncorrect one) and the decoys (\\ie the incorrect ones). Through careful analysis\nof the results attained by state-of-the-art learning models and human\nannotators on existing datasets, we show that the design of the decoy answers\nhas a significant impact on how and what the learning models learn from the\ndatasets. In particular, the resulting learner can ignore the visual\ninformation, the question, or both while still doing well on the task. Inspired\nby this, we propose automatic procedures to remedy such design deficiencies. We\napply the procedures to re-construct decoy answers for two popular Visual QA\ndatasets as well as to create a new Visual QA dataset from the Visual Genome\nproject, resulting in the largest dataset for this task. Extensive empirical\nstudies show that the design deficiencies have been alleviated in the remedied\ndatasets and the performance on them is likely a more faithful indicator of the\ndifference among learning models. The datasets are released and publicly\navailable via http://www.teds.usc.edu/website_vqa/.",
        "url": "http://arxiv.org/pdf/1704.07121v2.pdf"
    },
    {
        "title": "Assessing the impact of machine intelligence on human behaviour: an interdisciplinary endeavour",
        "abstract": "This document contains the outcome of the first Human behaviour and machine\nintelligence (HUMAINT) workshop that took place 5-6 March 2018 in Barcelona,\nSpain. The workshop was organized in the context of a new research programme at\nthe Centre for Advanced Studies, Joint Research Centre of the European\nCommission, which focuses on studying the potential impact of artificial\nintelligence on human behaviour. The workshop gathered an interdisciplinary\ngroup of experts to establish the state of the art research in the field and a\nlist of future research challenges to be addressed on the topic of human and\nmachine intelligence, algorithm's potential impact on human cognitive\ncapabilities and decision making, and evaluation and regulation needs. The\ndocument is made of short position statements and identification of challenges\nprovided by each expert, and incorporates the result of the discussions carried\nout during the workshop. In the conclusion section, we provide a list of\nemerging research topics and strategies to be addressed in the near future.",
        "url": "http://arxiv.org/pdf/1806.03192v1.pdf"
    },
    {
        "title": "Quantitative Phase Imaging and Artificial Intelligence: A Review",
        "abstract": "Recent advances in quantitative phase imaging (QPI) and artificial\nintelligence (AI) have opened up the possibility of an exciting frontier. The\nfast and label-free nature of QPI enables the rapid generation of large-scale\nand uniform-quality imaging data in two, three, and four dimensions.\nSubsequently, the AI-assisted interrogation of QPI data using data-driven\nmachine learning techniques results in a variety of biomedical applications.\nAlso, machine learning enhances QPI itself. Herein, we review the synergy\nbetween QPI and machine learning with a particular focus on deep learning.\nFurther, we provide practical guidelines and perspectives for further\ndevelopment.",
        "url": "http://arxiv.org/pdf/1806.03982v2.pdf"
    },
    {
        "title": "Human-aided Multi-Entity Bayesian Networks Learning from Relational Data",
        "abstract": "An Artificial Intelligence (AI) system is an autonomous system which emulates\nhuman mental and physical activities such as Observe, Orient, Decide, and Act,\ncalled the OODA process. An AI system performing the OODA process requires a\nsemantically rich representation to handle a complex real world situation and\nability to reason under uncertainty about the situation. Multi-Entity Bayesian\nNetworks (MEBNs) combines First-Order Logic with Bayesian Networks for\nrepresenting and reasoning about uncertainty in complex, knowledge-rich\ndomains. MEBN goes beyond standard Bayesian networks to enable reasoning about\nan unknown number of entities interacting with each other in various types of\nrelationships, a key requirement for the OODA process of an AI system. MEBN\nmodels have heretofore been constructed manually by a domain expert. However,\nmanual MEBN modeling is labor-intensive and insufficiently agile. To address\nthese problems, an efficient method is needed for MEBN modeling. One of the\nmethods is to use machine learning to learn a MEBN model in whole or in part\nfrom data. In the era of Big Data, data-rich environments, characterized by\nuncertainty and complexity, have become ubiquitous. The larger the data sample\nis, the more accurate the results of the machine learning approach can be.\nTherefore, machine learning has potential to improve the quality of MEBN models\nas well as the effectiveness for MEBN modeling. In this research, we study a\nMEBN learning framework to develop a MEBN model from a combination of domain\nexpert's knowledge and data. To evaluate the MEBN learning framework, we\nconduct an experiment to compare the MEBN learning framework and the existing\nmanual MEBN modeling in terms of development efficiency.",
        "url": "http://arxiv.org/pdf/1806.02421v1.pdf"
    },
    {
        "title": "Human-like generalization in a machine through predicate learning",
        "abstract": "Humans readily generalize, applying prior knowledge to novel situations and\nstimuli. Advances in machine learning and artificial intelligence have begun to\napproximate and even surpass human performance, but machine systems reliably\nstruggle to generalize information to untrained situations. We describe a\nneural network model that is trained to play one video game (Breakout) and\ndemonstrates one-shot generalization to a new game (Pong). The model\ngeneralizes by learning representations that are functionally and formally\nsymbolic from training data, without feedback, and without requiring that\nstructured representations be specified a priori. The model uses unsupervised\ncomparison to discover which characteristics of the input are invariant, and to\nlearn relational predicates; it then applies these predicates to arguments in a\nsymbolic fashion, using oscillatory regularities in network firing to\ndynamically bind predicates to arguments. We argue that models of human\ncognition must account for far-reaching and flexible generalization, and that\nin order to do so, models must be able to discover symbolic representations\nfrom unstructured data, a process we call predicate learning. Only then can\nmodels begin to adequately explain where human-like representations come from,\nwhy human cognition is the way it is, and why it continues to differ from\nmachine intelligence in crucial ways.",
        "url": "http://arxiv.org/pdf/1806.01709v3.pdf"
    },
    {
        "title": "A New Framework for Machine Intelligence: Concepts and Prototype",
        "abstract": "Machine learning (ML) and artificial intelligence (AI) have become hot topics\nin many information processing areas, from chatbots to scientific data\nanalysis. At the same time, there is uncertainty about the possibility of\nextending predominant ML technologies to become general solutions with\ncontinuous learning capabilities. Here, a simple, yet comprehensive,\ntheoretical framework for intelligent systems is presented. A combination of\nMirror Compositional Representations (MCR) and a Solution-Critic Loop (SCL) is\nproposed as a generic approach for different types of problems. A prototype\nimplementation is presented for document comparison using English Wikipedia\ncorpus.",
        "url": "http://arxiv.org/pdf/1806.02137v1.pdf"
    },
    {
        "title": "Good and safe uses of AI Oracles",
        "abstract": "It is possible that powerful and potentially dangerous artificial\nintelligence (AI) might be developed in the future. An Oracle is a design which\naims to restrain the impact of a potentially dangerous AI by restricting the\nagent to no actions besides answering questions. Unfortunately, most Oracles\nwill be motivated to gain more control over the world by manipulating users\nthrough the content of their answers, and Oracles of potentially high\nintelligence might be very successful at this\n\\citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two\ndesigns for Oracles which, even under pessimistic assumptions, will not\nmanipulate their users into releasing them and yet will still be incentivised\nto provide their users with helpful answers. The first design is the\ncounterfactual Oracle -- which choses its answer as if it expected nobody to\never read it. The second design is the low-bandwidth Oracle -- which is limited\nby the quantity of information it can transmit.",
        "url": "http://arxiv.org/pdf/1711.05541v5.pdf"
    },
    {
        "title": "Past Visions of Artificial Futures: One Hundred and Fifty Years under the Spectre of Evolving Machines",
        "abstract": "The influence of Artificial Intelligence (AI) and Artificial Life (ALife)\ntechnologies upon society, and their potential to fundamentally shape the\nfuture evolution of humankind, are topics very much at the forefront of current\nscientific, governmental and public debate. While these might seem like very\nmodern concerns, they have a long history that is often disregarded in\ncontemporary discourse. Insofar as current debates do acknowledge the history\nof these ideas, they rarely look back further than the origin of the modern\ndigital computer age in the 1940s-50s. In this paper we explore the earlier\nhistory of these concepts. We focus in particular on the idea of\nself-reproducing and evolving machines, and potential implications for our own\nspecies. We show that discussion of these topics arose in the 1860s, within a\ndecade of the publication of Darwin's The Origin of Species, and attracted\nincreasing interest from scientists, novelists and the general public in the\nearly 1900s. After introducing the relevant work from this period, we\ncategorise the various visions presented by these authors of the future\nimplications of evolving machines for humanity. We suggest that current debates\non the co-evolution of society and technology can be enriched by a proper\nappreciation of the long history of the ideas involved.",
        "url": "http://arxiv.org/pdf/1806.01322v1.pdf"
    },
    {
        "title": "The Anatomy of a Modular System for Media Content Analysis",
        "abstract": "Intelligent systems for the annotation of media content are increasingly\nbeing used for the automation of parts of social science research. In this\ndomain the problem of integrating various Artificial Intelligence (AI)\nalgorithms into a single intelligent system arises spontaneously. As part of\nour ongoing effort in automating media content analysis for the social\nsciences, we have built a modular system by combining multiple AI modules into\na flexible framework in which they can cooperate in complex tasks. Our system\ncombines data gathering, machine translation, topic classification, extraction\nand annotation of entities and social networks, as well as many other tasks\nthat have been perfected over the past years of AI research. Over the last few\nyears, it has allowed us to realise a series of scientific studies over a vast\nrange of applications including comparative studies between news outlets and\nmedia content in different countries, modelling of user preferences, and\nmonitoring public mood. The framework is flexible and allows the design and\nimplementation of modular agents, where simple modules cooperate in the\nannotation of a large dataset without central coordination.",
        "url": "http://arxiv.org/pdf/1402.6208v2.pdf"
    },
    {
        "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning",
        "abstract": "There has recently been a surge of work in explanatory artificial\nintelligence (XAI). This research area tackles the important problem that\ncomplex machines and algorithms often cannot provide insights into their\nbehavior and thought processes. XAI allows users and parts of the internal\nsystem to be more transparent, providing explanations of their decisions in\nsome level of detail. These explanations are important to ensure algorithmic\nfairness, identify potential bias/problems in the training data, and to ensure\nthat the algorithms perform as expected. However, explanations produced by\nthese systems is neither standardized nor systematically assessed. In an effort\nto create best practices and identify open challenges, we provide our\ndefinition of explainability and show how it can be used to classify existing\nliterature. We discuss why current approaches to explanatory methods especially\nfor deep neural networks are insufficient. Finally, based on our survey, we\nconclude with suggested future research directions for explanatory artificial\nintelligence.",
        "url": "http://arxiv.org/pdf/1806.00069v3.pdf"
    },
    {
        "title": "The Actor Search Tree Critic (ASTC) for Off-Policy POMDP Learning in Medical Decision Making",
        "abstract": "Off-policy reinforcement learning enables near-optimal policy from suboptimal\nexperience, thereby provisions opportunity for artificial intelligence\napplications in healthcare. Previous works have mainly framed patient-clinician\ninteractions as Markov decision processes, while true physiological states are\nnot necessarily fully observable from clinical data. We capture this situation\nwith partially observable Markov decision process, in which an agent optimises\nits actions in a belief represented as a distribution of patient states\ninferred from individual history trajectories. A Gaussian mixture model is\nfitted for the observed data. Moreover, we take into account the fact that\nnuance in pharmaceutical dosage could presumably result in significantly\ndifferent effect by modelling a continuous policy through a Gaussian\napproximator directly in the policy space, i.e. the actor. To address the\nchallenge of infinite number of possible belief states which renders exact\nvalue iteration intractable, we evaluate and plan for only every encountered\nbelief, through heuristic search tree by tightly maintaining lower and upper\nbounds of the true value of belief. We further resort to function\napproximations to update value bounds estimation, i.e. the critic, so that the\ntree search can be improved through more compact bounds at the fringe nodes\nthat will be back-propagated to the root. Both actor and critic parameters are\nlearned via gradient-based approaches. Our proposed policy trained from real\nintensive care unit data is capable of dictating dosing on vasopressors and\nintravenous fluids for sepsis patients that lead to the best patient outcomes.",
        "url": "http://arxiv.org/pdf/1805.11548v3.pdf"
    },
    {
        "title": "Deep Pepper: Expert Iteration based Chess agent in the Reinforcement Learning Setting",
        "abstract": "An almost-perfect chess playing agent has been a long standing challenge in\nthe field of Artificial Intelligence. Some of the recent advances demonstrate\nwe are approaching that goal. In this project, we provide methods for faster\ntraining of self-play style algorithms, mathematical details of the algorithm\nused, various potential future directions, and discuss most of the relevant\nwork in the area of computer chess. Deep Pepper uses embedded knowledge to\naccelerate the training of the chess engine over a \"tabula rasa\" system such as\nAlpha Zero. We also release our code to promote further research.",
        "url": "http://arxiv.org/pdf/1806.00683v2.pdf"
    },
    {
        "title": "Emotion Detection in Text: a Review",
        "abstract": "In recent years, emotion detection in text has become more popular due to its\nvast potential applications in marketing, political science, psychology,\nhuman-computer interaction, artificial intelligence, etc. Access to a huge\namount of textual data, especially opinionated and self-expression text also\nplayed a special role to bring attention to this field. In this paper, we\nreview the work that has been done in identifying emotion expressions in text\nand argue that although many techniques, methodologies, and models have been\ncreated to detect emotion in text, there are various reasons that make these\nmethods insufficient. Although, there is an essential need to improve the\ndesign and architecture of current systems, factors such as the complexity of\nhuman emotions, and the use of implicit and metaphorical language in expressing\nit, lead us to think that just re-purposing standard methodologies will not be\nenough to capture these complexities, and it is important to pay attention to\nthe linguistic intricacies of emotion expression.",
        "url": "http://arxiv.org/pdf/1806.00674v1.pdf"
    },
    {
        "title": "SCAN: Sliding Convolutional Attention Network for Scene Text Recognition",
        "abstract": "Scene text recognition has drawn great attentions in the community of\ncomputer vision and artificial intelligence due to its challenges and wide\napplications. State-of-the-art recurrent neural networks (RNN) based models map\nan input sequence to a variable length output sequence, but are usually applied\nin a black box manner and lack of transparency for further improvement, and the\nmaintaining of the entire past hidden states prevents parallel computation in a\nsequence. In this paper, we investigate the intrinsic characteristics of text\nrecognition, and inspired by human cognition mechanisms in reading texts, we\npropose a scene text recognition method with sliding convolutional attention\nnetwork (SCAN). Similar to the eye movement during reading, the process of SCAN\ncan be viewed as an alternation between saccades and visual fixations. Compared\nto the previous recurrent models, computations over all elements of SCAN can be\nfully parallelized during training. Experimental results on several challenging\nbenchmarks, including the IIIT5k, SVT and ICDAR 2003/2013 datasets, demonstrate\nthe superiority of SCAN over state-of-the-art methods in terms of both the\nmodel interpretability and performance.",
        "url": "http://arxiv.org/pdf/1806.00578v1.pdf"
    },
    {
        "title": "SemTK: An Ontology-first, Open Source Semantic Toolkit for Managing and Querying Knowledge Graphs",
        "abstract": "The relatively recent adoption of Knowledge Graphs as an enabling technology\nin multiple high-profile artificial intelligence and cognitive applications has\nled to growing interest in the Semantic Web technology stack. Many\nsemantics-related tools, however, are focused on serving experts with a deep\nunderstanding of semantic technologies. For example, triplification of\nrelational data is available but there is no open source tool that allows a\nuser unfamiliar with OWL/RDF to import data into a semantic triple store in an\nintuitive manner. Further, many tools require users to have a working\nunderstanding of SPARQL to query data. Casual users interested in benefiting\nfrom the power of Knowledge Graphs have few tools available for exploring,\nquerying, and managing semantic data. We present SemTK, the Semantics Toolkit,\na user-friendly suite of tools that allow both expert and non-expert semantics\nusers convenient ingestion of relational data, simplified query generation, and\nmore. The exploration of ontologies and instance data is performed through\nSPARQLgraph, an intuitive web-based user interface in SemTK understandable and\nnavigable by a lay user. The open source version of SemTK is available at\nhttp://semtk.research.ge.com",
        "url": "http://arxiv.org/pdf/1710.11531v2.pdf"
    },
    {
        "title": "Producing radiologist-quality reports for interpretable artificial intelligence",
        "abstract": "Current approaches to explaining the decisions of deep learning systems for\nmedical tasks have focused on visualising the elements that have contributed to\neach decision. We argue that such approaches are not enough to \"open the black\nbox\" of medical decision making systems because they are missing a key\ncomponent that has been used as a standard communication tool between doctors\nfor centuries: language. We propose a model-agnostic interpretability method\nthat involves training a simple recurrent neural network model to produce\ndescriptive sentences to clarify the decision of deep learning classifiers.\n  We test our method on the task of detecting hip fractures from frontal pelvic\nx-rays. This process requires minimal additional labelling despite producing\ntext containing elements that the original deep learning classification model\nwas not specifically trained to detect.\n  The experimental results show that: 1) the sentences produced by our method\nconsistently contain the desired information, 2) the generated sentences are\npreferred by doctors compared to current tools that create saliency maps, and\n3) the combination of visualisations and generated text is better than either\nalone.",
        "url": "http://arxiv.org/pdf/1806.00340v1.pdf"
    },
    {
        "title": "Learning convex bounds for linear quadratic control policy synthesis",
        "abstract": "Learning to make decisions from observed data in dynamic environments remains\na problem of fundamental importance in a number of fields, from artificial\nintelligence and robotics, to medicine and finance. This paper concerns the\nproblem of learning control policies for unknown linear dynamical systems so as\nto maximize a quadratic reward function. We present a method to optimize the\nexpected value of the reward over the posterior distribution of the unknown\nsystem parameters, given data. The algorithm involves sequential convex\nprograming, and enjoys reliable local convergence and robust stability\nguarantees. Numerical simulations and stabilization of a real-world inverted\npendulum are used to demonstrate the approach, with strong performance and\nrobustness properties observed in both.",
        "url": "http://arxiv.org/pdf/1806.00319v1.pdf"
    },
    {
        "title": "Efficient Low-rank Multimodal Fusion with Modality-Specific Factors",
        "abstract": "Multimodal research is an emerging field of artificial intelligence, and one\nof the main research problems in this field is multimodal fusion. The fusion of\nmultimodal data is the process of integrating multiple unimodal representations\ninto one compact multimodal representation. Previous research in this field has\nexploited the expressiveness of tensors for multimodal representation. However,\nthese methods often suffer from exponential increase in dimensions and in\ncomputational complexity introduced by transformation of input into tensor. In\nthis paper, we propose the Low-rank Multimodal Fusion method, which performs\nmultimodal fusion using low-rank tensors to improve efficiency. We evaluate our\nmodel on three different tasks: multimodal sentiment analysis, speaker trait\nanalysis, and emotion recognition. Our model achieves competitive results on\nall these tasks while drastically reducing computational complexity. Additional\nexperiments also show that our model can perform robustly for a wide range of\nlow-rank settings, and is indeed much more efficient in both training and\ninference compared to other methods that utilize tensor representations.",
        "url": "http://arxiv.org/pdf/1806.00064v1.pdf"
    },
    {
        "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform",
        "abstract": "This paper describes AllenNLP, a platform for research on deep learning\nmethods in natural language understanding. AllenNLP is designed to support\nresearchers who want to build novel language understanding models quickly and\neasily. It is built on top of PyTorch, allowing for dynamic computation graphs,\nand provides (1) a flexible data API that handles intelligent batching and\npadding, (2) high-level abstractions for common operations in working with\ntext, and (3) a modular and extensible experiment framework that makes doing\ngood science easy. It also includes reference implementations of high quality\napproaches for both core semantic problems (e.g. semantic role labeling (Palmer\net al., 2005)) and language understanding applications (e.g. machine\ncomprehension (Rajpurkar et al., 2016)). AllenNLP is an ongoing open-source\neffort maintained by engineers and researchers at the Allen Institute for\nArtificial Intelligence.",
        "url": "http://arxiv.org/pdf/1803.07640v2.pdf"
    },
    {
        "title": "Reinforced Continual Learning",
        "abstract": "Most artificial intelligence models have limiting ability to solve new tasks\nfaster, without forgetting previously acquired knowledge. The recently emerging\nparadigm of continual learning aims to solve this issue, in which the model\nlearns various tasks in a sequential fashion. In this work, a novel approach\nfor continual learning is proposed, which searches for the best neural\narchitecture for each coming task via sophisticatedly designed reinforcement\nlearning strategies. We name it as Reinforced Continual Learning. Our method\nnot only has good performance on preventing catastrophic forgetting but also\nfits new tasks well. The experiments on sequential classification tasks for\nvariants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach\noutperforms existing continual learning alternatives for deep networks.",
        "url": "http://arxiv.org/pdf/1805.12369v1.pdf"
    },
    {
        "title": "Deep Learning with Cinematic Rendering: Fine-Tuning Deep Neural Networks Using Photorealistic Medical Images",
        "abstract": "Deep learning has emerged as a powerful artificial intelligence tool to\ninterpret medical images for a growing variety of applications. However, the\npaucity of medical imaging data with high-quality annotations that is necessary\nfor training such methods ultimately limits their performance. Medical data is\nchallenging to acquire due to privacy issues, shortage of experts available for\nannotation, limited representation of rare conditions and cost. This problem\nhas previously been addressed by using synthetically generated data. However,\nnetworks trained on synthetic data often fail to generalize to real data.\nCinematic rendering simulates the propagation and interaction of light passing\nthrough tissue models reconstructed from CT data, enabling the generation of\nphotorealistic images. In this paper, we present one of the first applications\nof cinematic rendering in deep learning, in which we propose to fine-tune\nsynthetic data-driven networks using cinematically rendered CT data for the\ntask of monocular depth estimation in endoscopy. Our experiments demonstrate\nthat: (a) Convolutional Neural Networks (CNNs) trained on synthetic data and\nfine-tuned on photorealistic cinematically rendered data adapt better to real\nmedical images and demonstrate more robust performance when compared to\nnetworks with no fine-tuning, (b) these fine-tuned networks require less\ntraining data to converge to an optimal solution, and (c) fine-tuning with data\nfrom a variety of photorealistic rendering conditions of the same scene\nprevents the network from learning patient-specific information and aids in\ngeneralizability of the model. Our empirical evaluation demonstrates that\nnetworks fine-tuned with cinematically rendered data predict depth with 56.87%\nless error for rendered endoscopy images and 27.49% less error for real porcine\ncolon endoscopy images.",
        "url": "http://arxiv.org/pdf/1805.08400v3.pdf"
    },
    {
        "title": "Deep Segment Hash Learning for Music Generation",
        "abstract": "Music generation research has grown in popularity over the past decade,\nthanks to the deep learning revolution that has redefined the landscape of\nartificial intelligence. In this paper, we propose a novel approach to music\ngeneration inspired by musical segment concatenation methods and hash learning\nalgorithms. Given a segment of music, we use a deep recurrent neural network\nand ranking-based hash learning to assign a forward hash code to the segment to\nretrieve candidate segments for continuation with matching backward hash codes.\nThe proposed method is thus called Deep Segment Hash Learning (DSHL). To the\nbest of our knowledge, DSHL is the first end-to-end segment hash learning\nmethod for music generation, and the first to use pair-wise training with\nsegments of music. We demonstrate that this method is capable of generating\nmusic which is both original and enjoyable, and that DSHL offers a promising\nnew direction for music generation research.",
        "url": "http://arxiv.org/pdf/1805.12176v1.pdf"
    },
    {
        "title": "Problem-Adapted Artificial Intelligence for Online Network Optimization",
        "abstract": "Future 5G wireless networks will rely on agile and automated network\nmanagement, where the usage of diverse resources must be jointly optimized with\nsurgical accuracy. A number of key wireless network functionalities (e.g.,\ntraffic steering, power control) give rise to hard optimization problems. What\nis more, high spatio-temporal traffic variability coupled with the need to\nsatisfy strict per slice/service SLAs in modern networks, suggest that these\nproblems must be constantly (re-)solved, to maintain close-to-optimal\nperformance. To this end, we propose the framework of Online Network\nOptimization (ONO), which seeks to maintain both agile and efficient control\nover time, using an arsenal of data-driven, online learning, and AI-based\ntechniques. Since the mathematical tools and the studied regimes vary widely\namong these methodologies, a theoretical comparison is often out of reach.\nTherefore, the important question `what is the right ONO technique?' remains\nopen to date. In this paper, we discuss the pros and cons of each technique and\npresent a direct quantitative comparison for a specific use case, using real\ndata. Our results suggest that carefully combining the insights of problem\nmodeling with state-of-the-art AI techniques provides significant advantages at\nreasonable complexity.",
        "url": "http://arxiv.org/pdf/1805.12090v2.pdf"
    },
    {
        "title": "RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records",
        "abstract": "We have recently seen many successful applications of recurrent neural\nnetworks (RNNs) on electronic medical records (EMRs), which contain histories\nof patients' diagnoses, medications, and other various events, in order to\npredict the current and future states of patients. Despite the strong\nperformance of RNNs, it is often challenging for users to understand why the\nmodel makes a particular prediction. Such black-box nature of RNNs can impede\nits wide adoption in clinical practice. Furthermore, we have no established\nmethods to interactively leverage users' domain expertise and prior knowledge\nas inputs for steering the model. Therefore, our design study aims to provide a\nvisual analytics solution to increase interpretability and interactivity of\nRNNs via a joint effort of medical experts, artificial intelligence scientists,\nand visual analytics researchers. Following the iterative design process\nbetween the experts, we design, implement, and evaluate a visual analytics tool\ncalled RetainVis, which couples a newly improved, interpretable and interactive\nRNN-based model called RetainEX and visualizations for users' exploration of\nEMR data in the context of prediction tasks. Our study shows the effective use\nof RetainVis for gaining insights into how individual medical codes contribute\nto making risk predictions, using EMRs of patients with heart failure and\ncataract symptoms. Our study also demonstrates how we made substantial changes\nto the state-of-the-art RNN model called RETAIN in order to make use of\ntemporal information and increase interactivity. This study will provide a\nuseful guideline for researchers that aim to design an interpretable and\ninteractive visual analytics tool for RNNs.",
        "url": "http://arxiv.org/pdf/1805.10724v3.pdf"
    },
    {
        "title": "Automating Personnel Rostering by Learning Constraints Using Tensors",
        "abstract": "Many problems in operations research require that constraints be specified in\nthe model. Determining the right constraints is a hard and laborsome task. We\npropose an approach to automate this process using artificial intelligence and\nmachine learning principles. So far there has been only little work on learning\nconstraints within the operations research community. We focus on personnel\nrostering and scheduling problems in which there are often past schedules\navailable and show that it is possible to automatically learn constraints from\nsuch examples. To realize this, we adapted some techniques from the constraint\nprogramming community and we have extended them in order to cope with\nmultidimensional examples. The method uses a tensor representation of the\nexample, which helps in capturing the dimensionality as well as the structure\nof the example, and applies tensor operations to find the constraints that are\nsatisfied by the example. To evaluate the proposed algorithm, we used\nconstraints from the Nurse Rostering Competition and generated solutions that\nsatisfy these constraints; these solutions were then used as examples to learn\nconstraints. Experiments demonstrate that the proposed algorithm is capable of\nproducing human readable constraints that capture the underlying\ncharacteristics of the examples.",
        "url": "http://arxiv.org/pdf/1805.11375v1.pdf"
    },
    {
        "title": "Overcoming catastrophic forgetting with hard attention to the task",
        "abstract": "Catastrophic forgetting occurs when a neural network loses the information\nlearned in a previous task after training on subsequent tasks. This problem\nremains a hurdle for artificial intelligence systems with sequential learning\ncapabilities. In this paper, we propose a task-based hard attention mechanism\nthat preserves previous tasks' information without affecting the current task's\nlearning. A hard attention mask is learned concurrently to every task, through\nstochastic gradient descent, and previous masks are exploited to condition such\nlearning. We show that the proposed mechanism is effective for reducing\ncatastrophic forgetting, cutting current rates by 45 to 80%. We also show that\nit is robust to different hyperparameter choices, and that it offers a number\nof monitoring capabilities. The approach features the possibility to control\nboth the stability and compactness of the learned knowledge, which we believe\nmakes it also attractive for online learning or network compression\napplications.",
        "url": "http://arxiv.org/pdf/1801.01423v3.pdf"
    },
    {
        "title": "Propositional Knowledge Representation and Reasoning in Restricted Boltzmann Machines",
        "abstract": "While knowledge representation and reasoning are considered the keys for\nhuman-level artificial intelligence, connectionist networks have been shown\nsuccessful in a broad range of applications due to their capacity for robust\nlearning and flexible inference under uncertainty. The idea of representing\nsymbolic knowledge in connectionist networks has been well-received and\nattracted much attention from research community as this can establish a\nfoundation for integration of scalable learning and sound reasoning. In\nprevious work, there exist a number of approaches that map logical inference\nrules with feed-forward propagation of artificial neural networks (ANN).\nHowever, the discriminative structure of an ANN requires the separation of\ninput/output variables which makes it difficult for general reasoning where any\nvariables should be inferable. Other approaches address this issue by employing\ngenerative models such as symmetric connectionist networks, however, they are\ndifficult and convoluted. In this paper we propose a novel method to represent\npropositional formulas in restricted Boltzmann machines which is less complex,\nespecially in the cases of logical implications and Horn clauses. An\nintegration system is then developed and evaluated in real datasets which shows\npromising results.",
        "url": "http://arxiv.org/pdf/1705.10899v3.pdf"
    },
    {
        "title": "Convolutional neural network compression for natural language processing",
        "abstract": "Convolutional neural networks are modern models that are very efficient in\nmany classification tasks. They were originally created for image processing\npurposes. Then some trials were performed to use them in different domains like\nnatural language processing. The artificial intelligence systems (like humanoid\nrobots) are very often based on embedded systems with constraints on memory,\npower consumption etc. Therefore convolutional neural network because of its\nmemory capacity should be reduced to be mapped to given hardware. In this\npaper, results are presented of compressing the efficient convolutional neural\nnetworks for sentiment analysis. The main steps are quantization and pruning\nprocesses. The method responsible for mapping compressed network to FPGA and\nresults of this implementation are presented. The described simulations showed\nthat 5-bit width is enough to have no drop in accuracy from floating point\nversion of the network. Additionally, significant memory footprint reduction\nwas achieved (from 85% up to 93%).",
        "url": "http://arxiv.org/pdf/1805.10796v1.pdf"
    },
    {
        "title": "Designing for Democratization: Introducing Novices to Artificial Intelligence Via Maker Kits",
        "abstract": "Existing research highlight the myriad of benefits realized when technology\nis sufficiently democratized and made accessible to non-technical or novice\nusers. However, democratizing complex technologies such as artificial\nintelligence (AI) remains hard. In this work, we draw on theoretical\nunderpinnings from the democratization of innovation, in exploring the design\nof maker kits that help introduce novice users to complex technologies. We\nreport on our work designing TJBot: an open source cardboard robot that can be\nprogrammed using pre-built AI services. We highlight principles we adopted in\nthis process (approachable design, simplicity, extensibility and\naccessibility), insights we learned from showing the kit at workshops (66\nparticipants) and how users interacted with the project on GitHub over a\n12-month period (Nov 2016 - Nov 2017). We find that the project succeeds in\nattracting novice users (40% of users who forked the project are new to GitHub)\nand a variety of demographics are interested in prototyping use cases such as\nhome automation, task delegation, teaching and learning.",
        "url": "http://arxiv.org/pdf/1805.10723v3.pdf"
    },
    {
        "title": "Deep Convolutional Neural Networks for Map-Type Classification",
        "abstract": "Maps are an important medium that enable people to comprehensively understand\nthe configuration of cultural activities and natural elements over different\ntimes and places. Although massive maps are available in the digital era, how\nto effectively and accurately access the required map remains a challenge\ntoday. Previous works partially related to map-type classification mainly\nfocused on map comparison and map matching at the local scale. The features\nderived from local map areas might be insufficient to characterize map content.\nTo facilitate establishing an automatic approach for accessing the needed map,\nthis paper reports our investigation into using deep learning techniques to\nrecognize seven types of map, including topographic map, terrain map, physical\nmap, urban scene map, the National Map, 3D map, nighttime map, orthophoto map,\nand land cover classification map. Experimental results show that the\nstate-of-the-art deep convolutional neural networks can support automatic\nmap-type classification. Additionally, the classification accuracy varies\naccording to different map-types. We hope our work can contribute to the\nimplementation of deep learning techniques in cartographical community and\nadvance the progress of Geographical Artificial Intelligence (GeoAI).",
        "url": "http://arxiv.org/pdf/1805.10402v1.pdf"
    },
    {
        "title": "Self-Net: Lifelong Learning via Continual Self-Modeling",
        "abstract": "Learning a set of tasks over time, also known as continual learning (CL), is one of the most challenging problems in artificial intelligence. While recent approaches achieve some degree of CL in deep neural networks, they either (1) grow the network parameters linearly with the number of tasks, (2) require storing training data from previous tasks, or (3) restrict the network's ability to learn new tasks. To address these issues, we propose a novel framework, Self-Net, that uses an autoencoder to learn a set of low-dimensional representations of the weights learned for different tasks. We demonstrate that these low-dimensional vectors can then be used to generate high-fidelity recollections of the original weights. Self-Net can incorporate new tasks over time with little retraining and with minimal loss in performance for older tasks. Our system does not require storing prior training data and its parameters grow only logarithmically with the number of tasks. We show that our technique outperforms current state-of-the-art approaches on numerous datasets---including continual versions of MNIST, CIFAR10, CIFAR100, and Atari---and we demonstrate that our method can achieve over 10X storage compression in a continual fashion. To the best of our knowledge, we are the first to use autoencoders to sequentially encode sets of network weights to enable continual learning.",
        "url": "https://arxiv.org/pdf/1805.10354v3.pdf"
    },
    {
        "title": "ORGaNICs: A Theory of Working Memory in Brains and Machines",
        "abstract": "Working memory is a cognitive process that is responsible for temporarily\nholding and manipulating information. Most of the empirical neuroscience\nresearch on working memory has focused on measuring sustained activity in\nprefrontal cortex (PFC) and/or parietal cortex during simple delayed-response\ntasks, and most of the models of working memory have been based on neural\nintegrators. But working memory means much more than just holding a piece of\ninformation online. We describe a new theory of working memory, based on a\nrecurrent neural circuit that we call ORGaNICs (Oscillatory Recurrent GAted\nNeural Integrator Circuits). ORGaNICs are a variety of Long Short Term Memory\nunits (LSTMs), imported from machine learning and artificial intelligence.\nORGaNICs can be used to explain the complex dynamics of delay-period activity\nin prefrontal cortex (PFC) during a working memory task. The theory is\nanalytically tractable so that we can characterize the dynamics, and the theory\nprovides a means for reading out information from the dynamically varying\nresponses at any point in time, in spite of the complex dynamics. ORGaNICs can\nbe implemented with a biophysical (electrical circuit) model of pyramidal\ncells, combined with shunting inhibition via a thalamocortical loop. Although\nintroduced as a computational theory of working memory, ORGaNICs are also\napplicable to models of sensory processing, motor preparation and motor\ncontrol. ORGaNICs offer computational advantages compared to other varieties of\nLSTMs that are commonly used in AI applications. Consequently, ORGaNICs are a\nframework for canonical computation in brains and machines.",
        "url": "http://arxiv.org/pdf/1803.06288v4.pdf"
    },
    {
        "title": "Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction",
        "abstract": "Machine understanding of complex images is a key goal of artificial\nintelligence. One challenge underlying this task is that visual scenes contain\nmultiple inter-related objects, and that global context plays an important role\nin interpreting the scene. A natural modeling framework for capturing such\neffects is structured prediction, which optimizes over complex labels, while\nmodeling within-label interactions. However, it is unclear what principles\nshould guide the design of a structured prediction model that utilizes the\npower of deep learning components. Here we propose a design principle for such\narchitectures that follows from a natural requirement of permutation\ninvariance. We prove a necessary and sufficient characterization for\narchitectures that follow this invariance, and discuss its implication on model\ndesign. Finally, we show that the resulting model achieves new state of the art\nresults on the Visual Genome scene graph labeling benchmark, outperforming all\nrecent approaches.",
        "url": "http://arxiv.org/pdf/1802.05451v4.pdf"
    },
    {
        "title": "Neural Network Quine",
        "abstract": "Self-replication is a key aspect of biological life that has been largely\noverlooked in Artificial Intelligence systems. Here we describe how to build\nand train self-replicating neural networks. The network replicates itself by\nlearning to output its own weights. The network is designed using a loss\nfunction that can be optimized with either gradient-based or non-gradient-based\nmethods. We also describe a method we call regeneration to train the network\nwithout explicit optimization, by injecting the network with predictions of its\nown parameters. The best solution for a self-replicating network was found by\nalternating between regeneration and optimization steps. Finally, we describe a\ndesign for a self-replicating neural network that can solve an auxiliary task\nsuch as MNIST image classification. We observe that there is a trade-off\nbetween the network's ability to classify images and its ability to replicate,\nbut training is biased towards increasing its specialization at image\nclassification at the expense of replication. This is analogous to the\ntrade-off between reproduction and other tasks observed in nature. We suggest\nthat a self-replication mechanism for artificial intelligence is useful because\nit introduces the possibility of continual improvement through natural\nselection.",
        "url": "http://arxiv.org/pdf/1803.05859v4.pdf"
    },
    {
        "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences",
        "abstract": "There has been a recent resurgence in the area of explainable artificial\nintelligence as researchers and practitioners seek to make their algorithms\nmore understandable. Much of this research is focused on explicitly explaining\ndecisions or actions to a human observer, and it should not be controversial to\nsay that looking at how humans explain to each other can serve as a useful\nstarting point for explanation in artificial intelligence. However, it is fair\nto say that most work in explainable artificial intelligence uses only the\nresearchers' intuition of what constitutes a `good' explanation. There exists\nvast and valuable bodies of research in philosophy, psychology, and cognitive\nscience of how people define, generate, select, evaluate, and present\nexplanations, which argues that people employ certain cognitive biases and\nsocial expectations towards the explanation process. This paper argues that the\nfield of explainable artificial intelligence should build on this existing\nresearch, and reviews relevant papers from philosophy, cognitive\npsychology/science, and social psychology, which study these topics. It draws\nout some important findings, and discusses ways that these can be infused with\nwork on explainable artificial intelligence.",
        "url": "http://arxiv.org/pdf/1706.07269v3.pdf"
    },
    {
        "title": "Empirical Analysis of Foundational Distinctions in Linked Open Data",
        "abstract": "The Web and its Semantic extension (i.e. Linked Open Data) contain open\nglobal-scale knowledge and make it available to potentially intelligent\nmachines that want to benefit from it. Nevertheless, most of Linked Open Data\nlack ontological distinctions and have sparse axiomatisation. For example,\ndistinctions such as whether an entity is inherently a class or an individual,\nor whether it is a physical object or not, are hardly expressed in the data,\nalthough they have been largely studied and formalised by foundational\nontologies (e.g. DOLCE, SUMO). These distinctions belong to common sense too,\nwhich is relevant for many artificial intelligence tasks such as natural\nlanguage understanding, scene recognition, and the like. There is a gap between\nfoundational ontologies, that often formalise or are inspired by pre-existing\nphilosophical theories and are developed with a top-down approach, and Linked\nOpen Data that mostly derive from existing databases or crowd-based effort\n(e.g. DBpedia, Wikidata). We investigate whether machines can learn\nfoundational distinctions over Linked Open Data entities, and if they match\ncommon sense. We want to answer questions such as \"does the DBpedia entity for\ndog refer to a class or to an instance?\". We report on a set of experiments\nbased on machine learning and crowdsourcing that show promising results.",
        "url": "http://arxiv.org/pdf/1803.09840v2.pdf"
    },
    {
        "title": "QBF as an Alternative to Courcelle's Theorem",
        "abstract": "We propose reductions to quantified Boolean formulas (QBF) as a new approach\nto showing fixed-parameter linear algorithms for problems parameterized by\ntreewidth. We demonstrate the feasibility of this approach by giving new\nalgorithms for several well-known problems from artificial intelligence that\nare in general complete for the second level of the polynomial hierarchy. By\nreduction from QBF we show that all resulting algorithms are essentially\noptimal in their dependence on the treewidth. Most of the problems that we\nconsider were already known to be fixed-parameter linear by using Courcelle's\nTheorem or dynamic programming, but we argue that our approach has clear\nadvantages over these techniques: on the one hand, in contrast to Courcelle's\nTheorem, we get concrete and tight guarantees for the runtime dependence on the\ntreewidth. On the other hand, we avoid tedious dynamic programming and, after\nshowing some normalization results for CNF-formulas, our upper bounds often\nboil down to a few lines.",
        "url": "http://arxiv.org/pdf/1805.08456v1.pdf"
    },
    {
        "title": "Wikipedia for Smart Machines and Double Deep Machine Learning",
        "abstract": "Very important breakthroughs in data centric deep learning algorithms led to\nimpressive performance in transactional point applications of Artificial\nIntelligence (AI) such as Face Recognition, or EKG classification. With all due\nappreciation, however, knowledge blind data only machine learning algorithms\nhave severe limitations for non-transactional AI applications, such as medical\ndiagnosis beyond the EKG results. Such applications require deeper and broader\nknowledge in their problem solving capabilities, e.g. integrating anatomy and\nphysiology knowledge with EKG results and other patient findings. Following a\nreview and illustrations of such limitations for several real life AI\napplications, we point at ways to overcome them. The proposed Wikipedia for\nSmart Machines initiative aims at building repositories of software structures\nthat represent humanity science & technology knowledge in various parts of\nlife; knowledge that we all learn in schools, universities and during our\nprofessional life. Target readers for these repositories are smart machines;\nnot human. AI software developers will have these Reusable Knowledge structures\nreadily available, hence, the proposed name ReKopedia. Big Data is by now a\nmature technology, it is time to focus on Big Knowledge. Some will be derived\nfrom data, some will be obtained from mankind gigantic repository of knowledge.\nWikipedia for smart machines along with the new Double Deep Learning approach\noffer a paradigm for integrating datacentric deep learning algorithms with\nalgorithms that leverage deep knowledge, e.g. evidential reasoning and\ncausality reasoning. For illustration, a project is described to produce\nReKopedia knowledge modules for medical diagnosis of about 1,000 disorders.\nData is important, but knowledge deep, basic, and commonsense is equally\nimportant.",
        "url": "http://arxiv.org/pdf/1711.06517v2.pdf"
    },
    {
        "title": "Machine Learning: Basic Principles",
        "abstract": "This tutorial is based on the lecture notes for, and the plentiful student\nfeedback received from, the courses \"Machine Learning: Basic Principles\" and\n\"Artificial Intelligence\", which I have co-taught since 2015 at Aalto\nUniversity. The aim is to provide an accessible introduction to some of the\nmain concepts and methods within machine learning. Many of the current systems\nwhich are considered as (artificially) intelligent are based on combinations of\nfew basic machine learning methods. After formalizing the main building blocks\nof a machine learning problem, some popular algorithmic design patterns for\nmachine learning methods are discussed in some detail.",
        "url": "http://arxiv.org/pdf/1805.05052v9.pdf"
    },
    {
        "title": "Incept-N: A Convolutional Neural Network based Classification Approach for Predicting Nationality from Facial Features",
        "abstract": "The nationality of a human being is a well-known identifying characteristic\nused for every major authentication purpose in every country. Albeit advances\nin the application of Artificial Intelligence and Computer Vision in different\naspects, its contribution to this specific security procedure is yet to be\ncultivated. With a goal to successfully applying computer vision techniques to\npredict the nationality of a person based on his facial features, we have\nproposed this novel method and have achieved an average of 93.6% accuracy with\nvery low misclassification rate.",
        "url": "http://arxiv.org/pdf/1805.07426v1.pdf"
    },
    {
        "title": "Neural language representations predict outcomes of scientific research",
        "abstract": "Many research fields codify their findings in standard formats, often by\nreporting correlations between quantities of interest. But the space of all\ntestable correlates is far larger than scientific resources can currently\naddress, so the ability to accurately predict correlations would be useful to\nplan research and allocate resources. Using a dataset of approximately 170,000\ncorrelational findings extracted from leading social science journals, we show\nthat a trained neural network can accurately predict the reported correlations\nusing only the text descriptions of the correlates. Accurate predictive models\nsuch as these can guide scientists towards promising untested correlates,\nbetter quantify the information gained from new findings, and has implications\nfor moving artificial intelligence systems from predicting structures to\npredicting relationships in the real world.",
        "url": "http://arxiv.org/pdf/1805.06879v1.pdf"
    },
    {
        "title": "A Formulation of Recursive Self-Improvement and Its Possible Efficiency",
        "abstract": "Recursive self-improving (RSI) systems have been dreamed of since the early\ndays of computer science and artificial intelligence. However, many existing\nstudies on RSI systems remain philosophical, and lacks clear formulation and\nresults. In this paper, we provide a formal definition for one class of RSI\nsystems, and then demonstrate the existence of computable and efficient RSI\nsystems on a restricted version. We use simulation to empirically show that we\nachieve logarithmic runtime complexity with respect to the size of the search\nspace, and these results suggest it is possible to achieve an efficient\nrecursive self-improvement.",
        "url": "http://arxiv.org/pdf/1805.06610v1.pdf"
    },
    {
        "title": "Crick-net: A Convolutional Neural Network based Classification Approach for Detecting Waist High No Balls in Cricket",
        "abstract": "Cricket is undoubtedly one of the most popular games in this modern era. As human beings are prone to error, there remains a constant need for automated analysis and decision making of different events in this game. Simultaneously, with advent and advances in Artificial Intelligence and Computer Vision, application of these two in different domains has become an emerging trend. Applying several computer vision techniques in analyzing different Cricket events and automatically coming into decisions has become popular in recent days. In this paper, we have deployed a CNN based classification method with Inception V3 in order to automatically detect and differentiate waist high no balls with fair balls. Our approach achieves an overall average accuracy of 88% with a fairly low cross-entropy value.",
        "url": "https://arxiv.org/pdf/1805.05974v2.pdf"
    },
    {
        "title": "Feedback-Based Tree Search for Reinforcement Learning",
        "abstract": "Inspired by recent successes of Monte-Carlo tree search (MCTS) in a number of\nartificial intelligence (AI) application domains, we propose a model-based\nreinforcement learning (RL) technique that iteratively applies MCTS on batches\nof small, finite-horizon versions of the original infinite-horizon Markov\ndecision process. The terminal condition of the finite-horizon problems, or the\nleaf-node evaluator of the decision tree generated by MCTS, is specified using\na combination of an estimated value function and an estimated policy function.\nThe recommendations generated by the MCTS procedure are then provided as\nfeedback in order to refine, through classification and regression, the\nleaf-node evaluator for the next iteration. We provide the first sample\ncomplexity bounds for a tree search-based RL algorithm. In addition, we show\nthat a deep neural network implementation of the technique can create a\ncompetitive AI agent for the popular multi-player online battle arena (MOBA)\ngame King of Glory.",
        "url": "http://arxiv.org/pdf/1805.05935v1.pdf"
    },
    {
        "title": "TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation",
        "abstract": "The field of Natural Language Processing (NLP) is growing rapidly, with new\nresearch published daily along with an abundance of tutorials, codebases and\nother online resources. In order to learn this dynamic field or stay up-to-date\non the latest research, students as well as educators and researchers must\nconstantly sift through multiple sources to find valuable, relevant\ninformation. To address this situation, we introduce TutorialBank, a new,\npublicly available dataset which aims to facilitate NLP education and research.\nWe have manually collected and categorized over 6,300 resources on NLP as well\nas the related fields of Artificial Intelligence (AI), Machine Learning (ML)\nand Information Retrieval (IR). Our dataset is notably the largest\nmanually-picked corpus of resources intended for NLP education which does not\ninclude only academic papers. Additionally, we have created both a search\nengine and a command-line tool for the resources and have annotated the corpus\nto include lists of research topics, relevant resources for each topic,\nprerequisite relations among topics, relevant sub-parts of individual\nresources, among other annotations. We are releasing the dataset and present\nseveral avenues for further research.",
        "url": "http://arxiv.org/pdf/1805.04617v1.pdf"
    },
    {
        "title": "Deep Nets: What have they ever done for Vision?",
        "abstract": "This is an opinion paper about the strengths and weaknesses of Deep Nets for vision. They are at the heart of the enormous recent progress in artificial intelligence and are of growing importance in cognitive science and neuroscience. They have had many successes but also have several limitations and there is limited understanding of their inner workings. At present Deep Nets perform very well on specific visual tasks with benchmark datasets but they are much less general purpose, flexible, and adaptive than the human visual system. We argue that Deep Nets in their current form are unlikely to be able to overcome the fundamental problem of computer vision, namely how to deal with the combinatorial explosion, caused by the enormous complexity of natural images, and obtain the rich understanding of visual scenes that the human visual achieves. We argue that this combinatorial explosion takes us into a regime where \"big data is not enough\" and where we need to rethink our methods for benchmarking performance and evaluating vision algorithms. We stress that, as vision algorithms are increasingly used in real world applications, that performance evaluation is not merely an academic exercise but has important consequences in the real world. It is impractical to review the entire Deep Net literature so we restrict ourselves to a limited range of topics and references which are intended as entry points into the literature. The views expressed in this paper are our own and do not necessarily represent those of anybody else in the computer vision community.",
        "url": "https://arxiv.org/pdf/1805.04025v4.pdf"
    },
    {
        "title": "Category Theoretic Analysis of Photon-based Decision Making",
        "abstract": "Decision making is a vital function in this age of machine learning and\nartificial intelligence, yet its physical realization and theoretical\nfundamentals are still not completely understood. In our former study, we\ndemonstrated that single-photons can be used to make decisions in uncertain,\ndynamically changing environments. The two-armed bandit problem was\nsuccessfully solved using the dual probabilistic and particle attributes of\nsingle photons. In this study, we present a category theoretic modeling and\nanalysis of single-photon-based decision making, including a quantitative\nanalysis that is in agreement with the experimental results. A category\ntheoretic model reveals the complex interdependencies of subject matter\nentities in a simplified manner, even in dynamically changing environments. In\nparticular, the octahedral and braid structures in triangulated categories\nprovide a better understanding and quantitative metrics of the underlying\nmechanisms of a single-photon decision maker. This study provides both insight\nand a foundation for analyzing more complex and uncertain problems, to further\nmachine learning and artificial intelligence.",
        "url": "http://arxiv.org/pdf/1602.08199v3.pdf"
    },
    {
        "title": "Neural Networks for Predicting Algorithm Runtime Distributions",
        "abstract": "Many state-of-the-art algorithms for solving hard combinatorial problems in\nartificial intelligence (AI) include elements of stochasticity that lead to\nhigh variations in runtime, even for a fixed problem instance. Knowledge about\nthe resulting runtime distributions (RTDs) of algorithms on given problem\ninstances can be exploited in various meta-algorithmic procedures, such as\nalgorithm selection, portfolios, and randomized restarts. Previous work has\nshown that machine learning can be used to individually predict mean, median\nand variance of RTDs. To establish a new state-of-the-art in predicting RTDs,\nwe demonstrate that the parameters of an RTD should be learned jointly and that\nneural networks can do this well by directly optimizing the likelihood of an\nRTD given runtime observations. In an empirical study involving five algorithms\nfor SAT solving and AI planning, we show that neural networks predict the true\nRTDs of unseen instances better than previous methods, and can even do so when\nonly few runtime observations are available per training instance.",
        "url": "http://arxiv.org/pdf/1709.07615v3.pdf"
    },
    {
        "title": "Learning to Teach",
        "abstract": "Teaching plays a very important role in our society, by spreading human\nknowledge and educating our next generations. A good teacher will select\nappropriate teaching materials, impact suitable methodologies, and set up\ntargeted examinations, according to the learning behaviors of the students. In\nthe field of artificial intelligence, however, one has not fully explored the\nrole of teaching, and pays most attention to machine \\emph{learning}. In this\npaper, we argue that equal attention, if not more, should be paid to teaching,\nand furthermore, an optimization framework (instead of heuristics) should be\nused to obtain good teaching strategies. We call this approach `learning to\nteach'. In the approach, two intelligent agents interact with each other: a\nstudent model (which corresponds to the learner in traditional machine learning\nalgorithms), and a teacher model (which determines the appropriate data, loss\nfunction, and hypothesis space to facilitate the training of the student\nmodel). The teacher model leverages the feedback from the student model to\noptimize its own teaching strategies by means of reinforcement learning, so as\nto achieve teacher-student co-evolution. To demonstrate the practical value of\nour proposed approach, we take the training of deep neural networks (DNN) as an\nexample, and show that by using the learning to teach techniques, we are able\nto use much less training data and fewer iterations to achieve almost the same\naccuracy for different kinds of DNN models (e.g., multi-layer perceptron,\nconvolutional neural networks and recurrent neural networks) under various\nmachine learning tasks (e.g., image classification and text understanding).",
        "url": "http://arxiv.org/pdf/1805.03643v1.pdf"
    },
    {
        "title": "A Memristor based Unsupervised Neuromorphic System Towards Fast and Energy-Efficient GAN",
        "abstract": "Deep Learning has gained immense success in pushing today's artificial intelligence forward. To solve the challenge of limited labeled data in the supervised learning world, unsupervised learning has been proposed years ago while low accuracy hinters its realistic applications. Generative adversarial network (GAN) emerges as an unsupervised learning approach with promising accuracy and are under extensively study. However, the execution of GAN is extremely memory and computation intensive and results in ultra-low speed and high-power consumption. In this work, we proposed a holistic solution for fast and energy-efficient GAN computation through a memristor-based neuromorphic system. First, we exploited a hardware and software co-design approach to map the computation blocks in GAN efficiently. We also proposed an efficient data flow for optimal parallelism training and testing, depending on the computation correlations between different computing blocks. To compute the unique and complex loss of GAN, we developed a diff-block with optimized accuracy and performance. The experiment results on big data show that our design achieves 2.8x speedup and 6.1x energy-saving compared with the traditional GPU accelerator, as well as 5.5x speedup and 1.4x energy-saving compared with the previous FPGA-based accelerator.",
        "url": "https://arxiv.org/pdf/1806.01775v4.pdf"
    },
    {
        "title": "Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog",
        "abstract": "Creating an intelligent conversational system that understands vision and\nlanguage is one of the ultimate goals in Artificial Intelligence\n(AI)~\\cite{winograd1972understanding}. Extensive research has focused on\nvision-to-language generation, however, limited research has touched on\ncombining these two modalities in a goal-driven dialog context. We propose a\nmultimodal hierarchical reinforcement learning framework that dynamically\nintegrates vision and language for task-oriented visual dialog. The framework\njointly learns the multimodal dialog state representation and the hierarchical\ndialog policy to improve both dialog task success and efficiency. We also\npropose a new technique, state adaptation, to integrate context awareness in\nthe dialog state representation. We evaluate the proposed framework and the\nstate adaptation technique in an image guessing game and achieve promising\nresults.",
        "url": "http://arxiv.org/pdf/1805.03257v1.pdf"
    },
    {
        "title": "Towards Accurate and High-Speed Spiking Neuromorphic Systems with Data Quantization-Aware Deep Networks",
        "abstract": "Deep Neural Networks (DNNs) have gained immense success in cognitive applications and greatly pushed today's artificial intelligence forward. The biggest challenge in executing DNNs is their extremely data-extensive computations. The computing efficiency in speed and energy is constrained when traditional computing platforms are employed in such computational hungry executions. Spiking neuromorphic computing (SNC) has been widely investigated in deep networks implementation own to their high efficiency in computation and communication. However, weights and signals of DNNs are required to be quantized when deploying the DNNs on the SNC, which results in unacceptable accuracy loss. %However, the system accuracy is limited by quantizing data directly in deep networks deployment. Previous works mainly focus on weights discretize while inter-layer signals are mainly neglected. In this work, we propose to represent DNNs with fixed integer inter-layer signals and fixed-point weights while holding good accuracy. We implement the proposed DNNs on the memristor-based SNC system as a deployment example. With 4-bit data representation, our results show that the accuracy loss can be controlled within 0.02% (2.3%) on MNIST (CIFAR-10). Compared with the 8-bit dynamic fixed-point DNNs, our system can achieve more than 9.8x speedup, 89.1% energy saving, and 30% area saving.",
        "url": "https://arxiv.org/pdf/1805.03054v3.pdf"
    },
    {
        "title": "Can Computers Create Art?",
        "abstract": "This essay discusses whether computers, using Artificial Intelligence (AI),\ncould create art. First, the history of technologies that automated aspects of\nart is surveyed, including photography and animation. In each case, there were\ninitial fears and denial of the technology, followed by a blossoming of new\ncreative and professional opportunities for artists. The current hype and\nreality of Artificial Intelligence (AI) tools for art making is then discussed,\ntogether with predictions about how AI tools will be used. It is then\nspeculated about whether it could ever happen that AI systems could be credited\nwith authorship of artwork. It is theorized that art is something created by\nsocial agents, and so computers cannot be credited with authorship of art in\nour current understanding. A few ways that this could change are also\nhypothesized.",
        "url": "http://arxiv.org/pdf/1801.04486v6.pdf"
    },
    {
        "title": "Verisimilar Percept Sequences Tests for Autonomous Driving Intelligent Agent Assessment",
        "abstract": "The autonomous car technology promises to replace human drivers with safer\ndriving systems. But although autonomous cars can become safer than human\ndrivers this is a long process that is going to be refined over time. Before\nthese vehicles are deployed on urban roads a minimum safety level must be\nassured. Since the autonomous car technology is still under development there\nis no standard methodology to evaluate such systems. It is important to\ncompletely understand the technology that is being developed to design\nefficient means to evaluate it. In this paper we assume safety-critical systems\nreliability as a safety measure. We model an autonomous road vehicle as an\nintelligent agent and we approach its evaluation from an artificial\nintelligence perspective. Our focus is the evaluation of perception and\ndecision making systems and also to propose a systematic method to evaluate\ntheir integration in the vehicle. We identify critical aspects of the data\ndependency from the artificial intelligence state of the art models and we also\npropose procedures to reproduce them.",
        "url": "http://arxiv.org/pdf/1805.02754v1.pdf"
    },
    {
        "title": "A 64mW DNN-based Visual Navigation Engine for Autonomous Nano-Drones",
        "abstract": "Fully-autonomous miniaturized robots (e.g., drones), with artificial intelligence (AI) based visual navigation capabilities are extremely challenging drivers of Internet-of-Things edge intelligence capabilities. Visual navigation based on AI approaches, such as deep neural networks (DNNs) are becoming pervasive for standard-size drones, but are considered out of reach for nanodrones with size of a few cm${}^\\mathrm{2}$. In this work, we present the first (to the best of our knowledge) demonstration of a navigation engine for autonomous nano-drones capable of closed-loop end-to-end DNN-based visual navigation. To achieve this goal we developed a complete methodology for parallel execution of complex DNNs directly on-bard of resource-constrained milliwatt-scale nodes. Our system is based on GAP8, a novel parallel ultra-low-power computing platform, and a 27 g commercial, open-source CrazyFlie 2.0 nano-quadrotor. As part of our general methodology we discuss the software mapping techniques that enable the state-of-the-art deep convolutional neural network presented in [1] to be fully executed on-board within a strict 6 fps real-time constraint with no compromise in terms of flight results, while all processing is done with only 64 mW on average. Our navigation engine is flexible and can be used to span a wide performance range: at its peak performance corner it achieves 18 fps while still consuming on average just 3.5% of the power envelope of the deployed nano-aircraft.",
        "url": "https://arxiv.org/pdf/1805.01831v4.pdf"
    },
    {
        "title": "Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation",
        "abstract": "There are many machine translation (MT) papers that propose novel approaches\nand show improvements over their self-defined baselines. The experimental\nsetting in each paper often differs from one another. As such, it is hard to\ndetermine if a proposed approach is really useful and advances the state of the\nart. Chinese-to-English translation is a common translation direction in MT\npapers, although there is not one widely accepted experimental setting in\nChinese-to-English MT. Our goal in this paper is to propose a benchmark in\nevaluation setup for Chinese-to-English machine translation, such that the\neffectiveness of a new proposed MT approach can be directly compared to\nprevious approaches. Towards this end, we also built a highly competitive\nstate-of-the-art MT system trained on a large-scale training set. Our system\noutperforms reported results on NIST OpenMT test sets in almost all papers\npublished in major conferences and journals in computational linguistics and\nartificial intelligence in the past 11 years. We argue that a standardized\nbenchmark on data and performance is important for meaningful comparison.",
        "url": "http://arxiv.org/pdf/1805.01676v1.pdf"
    },
    {
        "title": "When Will AI Exceed Human Performance? Evidence from AI Experts",
        "abstract": "Advances in artificial intelligence (AI) will transform modern life by\nreshaping transportation, health, science, finance, and the military. To adapt\npublic policy, we need to better anticipate these advances. Here we report the\nresults from a large survey of machine learning researchers on their beliefs\nabout progress in AI. Researchers predict AI will outperform humans in many\nactivities in the next ten years, such as translating languages (by 2024),\nwriting high-school essays (by 2026), driving a truck (by 2027), working in\nretail (by 2031), writing a bestselling book (by 2049), and working as a\nsurgeon (by 2053). Researchers believe there is a 50% chance of AI\noutperforming humans in all tasks in 45 years and of automating all human jobs\nin 120 years, with Asian respondents expecting these dates much sooner than\nNorth Americans. These results will inform discussion amongst researchers and\npolicymakers about anticipating and managing trends in AI.",
        "url": "http://arxiv.org/pdf/1705.08807v3.pdf"
    },
    {
        "title": "InceptB: A CNN Based Classification Approach for Recognizing Traditional Bengali Games",
        "abstract": "Sports activities are an integral part of our day to day life. Introducing\nautonomous decision making and predictive models to recognize and analyze\ndifferent sports events and activities has become an emerging trend in computer\nvision arena. Albeit the advances and vivid applications of artificial\nintelligence and computer vision in recognizing different popular western\ngames, there remains a very minimal amount of efforts in the application of\ncomputer vision in recognizing traditional Bangladeshi games. We, in this\npaper, have described a novel Deep Learning based approach for recognizing\ntraditional Bengali games. We have retrained the final layer of the renowned\nInception V3 architecture developed by Google for our classification approach.\nOur approach shows promising results with an average accuracy of 80%\napproximately in correctly recognizing among 5 traditional Bangladeshi sports\nevents.",
        "url": "http://arxiv.org/pdf/1805.01442v2.pdf"
    },
    {
        "title": "Incremental and Iterative Learning of Answer Set Programs from Mutually Distinct Examples",
        "abstract": "Over the years the Artificial Intelligence (AI) community has produced\nseveral datasets which have given the machine learning algorithms the\nopportunity to learn various skills across various domains. However, a subclass\nof these machine learning algorithms that aimed at learning logic programs,\nnamely the Inductive Logic Programming algorithms, have often failed at the\ntask due to the vastness of these datasets. This has impacted the usability of\nknowledge representation and reasoning techniques in the development of AI\nsystems. In this research, we try to address this scalability issue for the\nalgorithms that learn answer set programs. We present a sound and complete\nalgorithm which takes the input in a slightly different manner and performs an\nefficient and more user controlled search for a solution. We show via\nexperiments that our algorithm can learn from two popular datasets from machine\nlearning community, namely bAbl (a question answering dataset) and MNIST (a\ndataset for handwritten digit recognition), which to the best of our knowledge\nwas not previously possible. The system is publicly available at\nhttps://goo.gl/KdWAcV. This paper is under consideration for acceptance in\nTPLP.",
        "url": "http://arxiv.org/pdf/1802.07966v2.pdf"
    },
    {
        "title": "PANDA: Facilitating Usable AI Development",
        "abstract": "Recent advances in artificial intelligence (AI) and machine learning have\ncreated a general perception that AI could be used to solve complex problems,\nand in some situations over-hyped as a tool that can be so easily used.\nUnfortunately, the barrier to realization of mass adoption of AI on various\nbusiness domains is too high because most domain experts have no background in\nAI. Developing AI applications involves multiple phases, namely data\npreparation, application modeling, and product deployment. The effort of AI\nresearch has been spent mostly on new AI models (in the model training stage)\nto improve the performance of benchmark tasks such as image recognition. Many\nother factors such as usability, efficiency and security of AI have not been\nwell addressed, and therefore form a barrier to democratizing AI. Further, for\nmany real world applications such as healthcare and autonomous driving,\nlearning via huge amounts of possibility exploration is not feasible since\nhumans are involved. In many complex applications such as healthcare, subject\nmatter experts (e.g. Clinicians) are the ones who appreciate the importance of\nfeatures that affect health, and their knowledge together with existing\nknowledge bases are critical to the end results. In this paper, we take a new\nperspective on developing AI solutions, and present a solution for making AI\nusable. We hope that this resolution will enable all subject matter experts\n(eg. Clinicians) to exploit AI like data scientists.",
        "url": "http://arxiv.org/pdf/1804.09997v1.pdf"
    },
    {
        "title": "The Intelligent ICU Pilot Study: Using Artificial Intelligence Technology for Autonomous Patient Monitoring",
        "abstract": "Currently, many critical care indices are repetitively assessed and recorded\nby overburdened nurses, e.g. physical function or facial pain expressions of\nnonverbal patients. In addition, many essential information on patients and\ntheir environment are not captured at all, or are captured in a non-granular\nmanner, e.g. sleep disturbance factors such as bright light, loud background\nnoise, or excessive visitations. In this pilot study, we examined the\nfeasibility of using pervasive sensing technology and artificial intelligence\nfor autonomous and granular monitoring of critically ill patients and their\nenvironment in the Intensive Care Unit (ICU). As an exemplar prevalent\ncondition, we also characterized delirious and non-delirious patients and their\nenvironment. We used wearable sensors, light and sound sensors, and a\nhigh-resolution camera to collected data on patients and their environment. We\nanalyzed collected data using deep learning and statistical analysis. Our\nsystem performed face detection, face recognition, facial action unit\ndetection, head pose detection, facial expression recognition, posture\nrecognition, actigraphy analysis, sound pressure and light level detection, and\nvisitation frequency detection. We were able to detect patient's face (Mean\naverage precision (mAP)=0.94), recognize patient's face (mAP=0.80), and their\npostures (F1=0.94). We also found that all facial expressions, 11 activity\nfeatures, visitation frequency during the day, visitation frequency during the\nnight, light levels, and sound pressure levels during the night were\nsignificantly different between delirious and non-delirious patients\n(p-value<0.05). In summary, we showed that granular and autonomous monitoring\nof critically ill patients and their environment is feasible and can be used\nfor characterizing critical care conditions and related environment factors.",
        "url": "http://arxiv.org/pdf/1804.10201v2.pdf"
    },
    {
        "title": "Unsupervised Disentangled Representation Learning with Analogical Relations",
        "abstract": "Learning the disentangled representation of interpretable generative factors\nof data is one of the foundations to allow artificial intelligence to think\nlike people. In this paper, we propose the analogical training strategy for the\nunsupervised disentangled representation learning in generative models. The\nanalogy is one of the typical cognitive processes, and our proposed strategy is\nbased on the observation that sample pairs in which one is different from the\nother in one specific generative factor show the same analogical relation.\nThus, the generator is trained to generate sample pairs from which a designed\nclassifier can identify the underlying analogical relation. In addition, we\npropose a disentanglement metric called the subspace score, which is inspired\nby subspace learning methods and does not require supervised information.\nExperiments show that our proposed training strategy allows the generative\nmodels to find the disentangled factors, and that our methods can give\ncompetitive performances as compared with the state-of-the-art methods.",
        "url": "http://arxiv.org/pdf/1804.09502v1.pdf"
    },
    {
        "title": "RGB-D-based Human Motion Recognition with Deep Learning: A Survey",
        "abstract": "Human motion recognition is one of the most important branches of\nhuman-centered research activities. In recent years, motion recognition based\non RGB-D data has attracted much attention. Along with the development in\nartificial intelligence, deep learning techniques have gained remarkable\nsuccess in computer vision. In particular, convolutional neural networks (CNN)\nhave achieved great success for image-based tasks, and recurrent neural\nnetworks (RNN) are renowned for sequence-based problems. Specifically, deep\nlearning methods based on the CNN and RNN architectures have been adopted for\nmotion recognition using RGB-D data. In this paper, a detailed overview of\nrecent advances in RGB-D-based motion recognition is presented. The reviewed\nmethods are broadly categorized into four groups, depending on the modality\nadopted for recognition: RGB-based, depth-based, skeleton-based and\nRGB+D-based. As a survey focused on the application of deep learning to\nRGB-D-based motion recognition, we explicitly discuss the advantages and\nlimitations of existing techniques. Particularly, we highlighted the methods of\nencoding spatial-temporal-structural information inherent in video sequence,\nand discuss potential directions for future research.",
        "url": "http://arxiv.org/pdf/1711.08362v2.pdf"
    },
    {
        "title": "Smart Surveillance as an Edge Network Service: from Harr-Cascade, SVM to a Lightweight CNN",
        "abstract": "Edge computing efficiently extends the realm of information technology beyond\nthe boundary defined by cloud computing paradigm. Performing computation near\nthe source and destination, edge computing is promising to address the\nchallenges in many delay-sensitive applications, like real-time human\nsurveillance. Leveraging the ubiquitously connected cameras and smart mobile\ndevices, it enables video analytics at the edge. In recent years, many smart\nvideo surveillance approaches are proposed for object detection and tracking by\nusing Artificial Intelligence (AI) and Machine Learning (ML) algorithms. This\nwork explores the feasibility of two popular human-objects detection schemes,\nHarr-Cascade and HOG feature extraction and SVM classifier, at the edge and\nintroduces a lightweight Convolutional Neural Network (L-CNN) leveraging the\ndepthwise separable convolution for less computation, for human detection.\nSingle Board computers (SBC) are used as edge devices for tests and algorithms\nare validated using real-world campus surveillance video streams and open data\nsets. The experimental results are promising that the final algorithm is able\nto track humans with a decent accuracy at a resource consumption affordable by\nedge devices in real-time manner.",
        "url": "http://arxiv.org/pdf/1805.00331v2.pdf"
    },
    {
        "title": "AI Safety and Reproducibility: Establishing Robust Foundations for the Neuropsychology of Human Values",
        "abstract": "We propose the creation of a systematic effort to identify and replicate key\nfindings in neuropsychology and allied fields related to understanding human\nvalues. Our aim is to ensure that research underpinning the value alignment\nproblem of artificial intelligence has been sufficiently validated to play a\nrole in the design of AI systems.",
        "url": "http://arxiv.org/pdf/1712.04307v3.pdf"
    },
    {
        "title": "Faster Shift-Reduce Constituent Parsing with a Non-Binary, Bottom-Up Strategy",
        "abstract": "An increasingly wide range of artificial intelligence applications rely on syntactic information to process and extract meaning from natural language text or speech, with constituent trees being one of the most widely used syntactic formalisms. To produce these phrase-structure representations from sentences in natural language, shift-reduce constituent parsers have become one of the most efficient approaches. Increasing their accuracy and speed is still one of the main objectives pursued by the research community so that artificial intelligence applications that make use of parsing outputs, such as machine translation or voice assistant services, can improve their performance. With this goal in mind, we propose in this article a novel non-binary shift-reduce algorithm for constituent parsing. Our parser follows a classical bottom-up strategy but, unlike others, it straightforwardly creates non-binary branchings with just one Reduce transition, instead of requiring prior binarization or a sequence of binary transitions, allowing its direct application to any language without the need of further resources such as percolation tables. As a result, it uses fewer transitions per sentence than existing transition-based constituent parsers, becoming the fastest such system and, as a consequence, speeding up downstream applications. Using static oracle training and greedy search, the accuracy of this novel approach is on par with state-of-the-art transition-based constituent parsers and outperforms all top-down and bottom-up greedy shift-reduce systems on the Wall Street Journal section from the English Penn Treebank and the Penn Chinese Treebank. Additionally, we develop a dynamic oracle for training the proposed transition-based algorithm, achieving further improvements in both benchmarks and obtaining the best accuracy to date on the Penn Chinese Treebank among greedy shift-reduce parsers.",
        "url": "https://arxiv.org/pdf/1804.07961v3.pdf"
    },
    {
        "title": "A Popperian Falsification of Artificial Intelligence -- Lighthill Defended",
        "abstract": "The area of computation called artificial intelligence (AI) is falsified by describing a previous 1972 falsification of AI by British mathematical physicist James Lighthill. How Lighthill's arguments continue to apply to current AI is explained. It is argued that AI should use the Popperian scientific method in which it is the duty of scientists to attempt to falsify theories and if theories are falsified to replace or modify them. The paper describes the Popperian method and discusses Paul Nurse's application of the method to cell biology that also involves questions of mechanism and behavior. It is shown how Lighthill's falsifying arguments especially combinatorial explosion continue to apply to modern AI. Various skeptical arguments against the assumptions of AI mostly by physicists especially against Hilbert's philosophical programme that defined knowledge and truth as provable formal sentences. John von Neumann's arguments from natural complexity against neural networks and evolutionary algorithms are discussed. Next the game of chess is discussed to show how modern chess experts have reacted to computer chess programs. It is shown that currently chess masters can defeat any chess program using Kasperov's arguments from his 1997 Deep Blue match and aftermath. The game of 'go' and climate models are discussed to show computer applications where combinatorial explosion may not apply. The paper concludes by advocating studying computation as Peter Naur's Dataology.",
        "url": "https://arxiv.org/pdf/1704.08111v3.pdf"
    },
    {
        "title": "Learning Convolutional Text Representations for Visual Question Answering",
        "abstract": "Visual question answering is a recently proposed artificial intelligence task\nthat requires a deep understanding of both images and texts. In deep learning,\nimages are typically modeled through convolutional neural networks, and texts\nare typically modeled through recurrent neural networks. While the requirement\nfor modeling images is similar to traditional computer vision tasks, such as\nobject recognition and image classification, visual question answering raises a\ndifferent need for textual representation as compared to other natural language\nprocessing tasks. In this work, we perform a detailed analysis on natural\nlanguage questions in visual question answering. Based on the analysis, we\npropose to rely on convolutional neural networks for learning textual\nrepresentations. By exploring the various properties of convolutional neural\nnetworks specialized for text data, such as width and depth, we present our\n\"CNN Inception + Gate\" model. We show that our model improves question\nrepresentations and thus the overall accuracy of visual question answering\nmodels. We also show that the text representation requirement in visual\nquestion answering is more complicated and comprehensive than that in\nconventional natural language processing tasks, making it a better task to\nevaluate textual representation methods. Shallow models like fastText, which\ncan obtain comparable results with deep learning models in tasks like text\nclassification, are not suitable in visual question answering.",
        "url": "http://arxiv.org/pdf/1705.06824v2.pdf"
    },
    {
        "title": "Learning to Navigate in Cities Without a Map",
        "abstract": "Navigating through unstructured environments is a basic capability of\nintelligent creatures, and thus is of fundamental interest in the study and\ndevelopment of artificial intelligence. Long-range navigation is a complex\ncognitive task that relies on developing an internal representation of space,\ngrounded by recognisable landmarks and robust visual processing, that can\nsimultaneously support continuous self-localisation (\"I am here\") and a\nrepresentation of the goal (\"I am going there\"). Building upon recent research\nthat applies deep reinforcement learning to maze navigation problems, we\npresent an end-to-end deep reinforcement learning approach that can be applied\non a city scale. Recognising that successful navigation relies on integration\nof general policies with locale-specific knowledge, we propose a dual pathway\narchitecture that allows locale-specific features to be encapsulated, while\nstill enabling transfer to multiple cities. We present an interactive\nnavigation environment that uses Google StreetView for its photographic content\nand worldwide coverage, and demonstrate that our learning method allows agents\nto learn to navigate multiple cities and to traverse to target destinations\nthat may be kilometres away. The project webpage http://streetlearn.cc contains\na video summarising our research and showing the trained agent in diverse city\nenvironments and on the transfer task, the form to request the StreetLearn\ndataset and links to further resources. The StreetLearn environment code is\navailable at https://github.com/deepmind/streetlearn",
        "url": "http://arxiv.org/pdf/1804.00168v3.pdf"
    },
    {
        "title": "An AI-driven Malfunction Detection Concept for NFV Instances in 5G",
        "abstract": "Efficient network management is one of the key challenges of the constantly\ngrowing and increasingly complex wide area networks (WAN). The paradigm shift\ntowards virtualized (NFV) and software defined networks (SDN) in the next\ngeneration of mobile networks (5G), as well as the latest scientific insights\nin the field of Artificial Intelligence (AI) enable the transition from\nmanually managed networks nowadays to fully autonomic and dynamic\nself-organized networks (SON). This helps to meet the KPIs and reduce at the\nsame time operational costs (OPEX). In this paper, an AI driven concept is\npresented for the malfunction detection in NFV applications with the help of\nsemi-supervised learning. For this purpose, a profile of the application under\ntest is created. This profile then is used as a reference to detect abnormal\nbehaviour. For example, if there is a bug in the updated version of the app, it\nis now possible to react autonomously and roll-back the NFV app to a previous\nversion in order to avoid network outages.",
        "url": "http://arxiv.org/pdf/1804.05796v1.pdf"
    },
    {
        "title": "MovieGraphs: Towards Understanding Human-Centric Situations from Videos",
        "abstract": "There is growing interest in artificial intelligence to build socially\nintelligent robots. This requires machines to have the ability to \"read\"\npeople's emotions, motivations, and other factors that affect behavior. Towards\nthis goal, we introduce a novel dataset called MovieGraphs which provides\ndetailed, graph-based annotations of social situations depicted in movie clips.\nEach graph consists of several types of nodes, to capture who is present in the\nclip, their emotional and physical attributes, their relationships (i.e.,\nparent/child), and the interactions between them. Most interactions are\nassociated with topics that provide additional details, and reasons that give\nmotivations for actions. In addition, most interactions and many attributes are\ngrounded in the video with time stamps. We provide a thorough analysis of our\ndataset, showing interesting common-sense correlations between different social\naspects of scenes, as well as across scenes over time. We propose a method for\nquerying videos and text with graphs, and show that: 1) our graphs contain rich\nand sufficient information to summarize and localize each scene; and 2)\nsubgraphs allow us to describe situations at an abstract level and retrieve\nmultiple semantically relevant situations. We also propose methods for\ninteraction understanding via ordering, and reason understanding. MovieGraphs\nis the first benchmark to focus on inferred properties of human-centric\nsituations, and opens up an exciting avenue towards socially-intelligent AI\nagents.",
        "url": "http://arxiv.org/pdf/1712.06761v2.pdf"
    },
    {
        "title": "Regularized Singular Value Decomposition and Application to Recommender System",
        "abstract": "Singular value decomposition (SVD) is the mathematical basis of principal\ncomponent analysis (PCA). Together, SVD and PCA are one of the most widely used\nmathematical formalism/decomposition in machine learning, data mining, pattern\nrecognition, artificial intelligence, computer vision, signal processing, etc.\nIn recent applications, regularization becomes an increasing trend. In this\npaper, we present a regularized SVD (RSVD), present an efficient computational\nalgorithm, and provide several theoretical analysis. We show that although RSVD\nis non-convex, it has a closed-form global optimal solution. Finally, we apply\nRSVD to the application of recommender system and experimental result show that\nRSVD outperforms SVD significantly.",
        "url": "http://arxiv.org/pdf/1804.05090v1.pdf"
    },
    {
        "title": "Online Fall Detection using Recurrent Neural Networks",
        "abstract": "Unintentional falls can cause severe injuries and even death, especially if\nno immediate assistance is given. The aim of Fall Detection Systems (FDSs) is\nto detect an occurring fall. This information can be used to trigger the\nnecessary assistance in case of injury. This can be done by using either\nambient-based sensors, e.g. cameras, or wearable devices. The aim of this work\nis to study the technical aspects of FDSs based on wearable devices and\nartificial intelligence techniques, in particular Deep Learning (DL), to\nimplement an effective algorithm for on-line fall detection. The proposed\nclassifier is based on a Recurrent Neural Network (RNN) model with underlying\nLong Short-Term Memory (LSTM) blocks. The method is tested on the publicly\navailable SisFall dataset, with extended annotation, and compared with the\nresults obtained by the SisFall authors.",
        "url": "http://arxiv.org/pdf/1804.04976v1.pdf"
    },
    {
        "title": "A Topological Approach to Meta-heuristics: Analytical Results on the BFS vs. DFS Algorithm Selection Problem",
        "abstract": "Search is a central problem in artificial intelligence, and breadth-first\nsearch (BFS) and depth-first search (DFS) are the two most fundamental ways to\nsearch. In this paper we derive estimates for average BFS and DFS runtime. The\naverage runtime estimates can be used to allocate resources or judge the\nhardness of a problem. They can also be used for selecting the best graph\nrepresentation, and for selecting the faster algorithm out of BFS and DFS. They\nmay also form the basis for an analysis of more advanced search methods. The\npaper treats both tree search and graph search. For tree search, we employ a\nprobabilistic model of goal distribution; for graph search, the analysis\ndepends on an additional statistic of path redundancy and average branching\nfactor. As an application, we use the results to predict BFS and DFS runtime on\ntwo concrete grammar problems and on the N-puzzle. Experimental verification\nshows that our analytical approximations come close to empirical reality.",
        "url": "http://arxiv.org/pdf/1509.02709v2.pdf"
    },
    {
        "title": "Deep Learning For Computer Vision Tasks: A review",
        "abstract": "Deep learning has recently become one of the most popular sub-fields of\nmachine learning owing to its distributed data representation with multiple\nlevels of abstraction. A diverse range of deep learning algorithms are being\nemployed to solve conventional artificial intelligence problems. This paper\ngives an overview of some of the most widely used deep learning algorithms\napplied in the field of computer vision. It first inspects the various\napproaches of deep learning algorithms, followed by a description of their\napplications in image classification, object identification, image extraction\nand semantic segmentation in the presence of noise. The paper concludes with\nthe discussion of the future scope and challenges for construction and training\nof deep neural networks.",
        "url": "http://arxiv.org/pdf/1804.03928v1.pdf"
    },
    {
        "title": "A Mathematical Framework for Superintelligent Machines",
        "abstract": "We describe a class calculus that is expressive enough to describe and\nimprove its own learning process. It can design and debug programs that satisfy\ngiven input/output constraints, based on its ontology of previously learned\nprograms. It can improve its own model of the world by checking the actual\nresults of the actions of its robotic activators. For instance, it could check\nthe black box of a car crash to determine if it was probably caused by electric\nfailure, a stuck electronic gate, dark ice, or some other condition that it\nmust add to its ontology in order to meet its sub-goal of preventing such\ncrashes in the future. Class algebra basically defines the eval/eval-1 Galois\nconnection between the residuated Boolean algebras of 1. equivalence classes\nand super/sub classes of class algebra type expressions, and 2. a residual\nBoolean algebra of biclique relationships. It distinguishes which formulas are\nequivalent, entailed, or unrelated, based on a simplification algorithm that\nmay be thought of as producing a unique pair of Karnaugh maps that describe the\nrough sets of maximal bicliques of relations. Such maps divide the\nn-dimensional space of up to 2n-1 conjunctions of up to n propositions into\nclopen (i.e. a closed set of regions and their boundaries) causal sets. This\nclass algebra is generalized to type-2 fuzzy class algebra by using relative\nfrequencies as probabilities. It is also generalized to a class calculus\ninvolving assignments that change the states of programs.\n  INDEX TERMS 4-valued Boolean Logic, Artificial Intelligence, causal sets,\nclass algebra, consciousness, intelligent design, IS-A hierarchy, mathematical\nlogic, meta-theory, pointless topological space, residuated lattices, rough\nsets, type-2 fuzzy sets",
        "url": "http://arxiv.org/pdf/1804.03301v1.pdf"
    },
    {
        "title": "Categorizing Variants of Goodhart's Law",
        "abstract": "There are several distinct failure modes for overoptimization of systems on\nthe basis of metrics. This occurs when a metric which can be used to improve a\nsystem is used to an extent that further optimization is ineffective or\nharmful, and is sometimes termed Goodhart's Law. This class of failure is often\npoorly understood, partly because terminology for discussing them is ambiguous,\nand partly because discussion using this ambiguous terminology ignores\ndistinctions between different failure modes of this general type. This paper\nexpands on an earlier discussion by Garrabrant, which notes there are \"(at\nleast) four different mechanisms\" that relate to Goodhart's Law. This paper is\nintended to explore these mechanisms further, and specify more clearly how they\noccur. This discussion should be helpful in better understanding these types of\nfailures in economic regulation, in public policy, in machine learning, and in\nArtificial Intelligence alignment. The importance of Goodhart effects depends\non the amount of power directed towards optimizing the proxy, and so the\nincreased optimization power offered by artificial intelligence makes it\nespecially critical for that field.",
        "url": "http://arxiv.org/pdf/1803.04585v4.pdf"
    },
    {
        "title": "Visual Analytics for Explainable Deep Learning",
        "abstract": "Recently, deep learning has been advancing the state of the art in artificial\nintelligence to a new level, and humans rely on artificial intelligence\ntechniques more than ever. However, even with such unprecedented advancements,\nthe lack of explanation regarding the decisions made by deep learning models\nand absence of control over their internal processes act as major drawbacks in\ncritical decision-making processes, such as precision medicine and law\nenforcement. In response, efforts are being made to make deep learning\ninterpretable and controllable by humans. In this paper, we review visual\nanalytics, information visualization, and machine learning perspectives\nrelevant to this aim, and discuss potential challenges and future research\ndirections.",
        "url": "http://arxiv.org/pdf/1804.02527v1.pdf"
    },
    {
        "title": "FPAN: Fine-grained and Progressive Attention Localization Network for Data Retrieval",
        "abstract": "The Localization of the target object for data retrieval is a key issue in\nthe Intelligent and Connected Transportation Systems (ICTS). However, due to\nlack of intelligence in the traditional transportation system, it can take\ntremendous resources to manually retrieve and locate the queried objects among\na large number of images. In order to solve this issue, we propose an effective\nmethod to query-based object localization that uses artificial intelligence\ntechniques to automatically locate the queried object in the complex\nbackground. The presented method is termed as Fine-grained and Progressive\nAttention Localization Network (FPAN), which uses an image and a queried object\nas input to accurately locate the target object in the image. Specifically, the\nfine-grained attention module is naturally embedded into each layer of the\nconvolution neural network (CNN), thereby gradually suppressing the regions\nthat are irrelevant to the queried object and eventually shrinking attention to\nthe target area. We further employ top-down attentions fusion algorithm\noperated by a learnable cascade up-sampling structure to establish the\nconnection between the attention map and the exact location of the queried\nobject in the original image. Furthermore, the FPAN is trained by multi-task\nlearning with box segmentation loss and cosine loss. At last, we conduct\ncomprehensive experiments on both queried-based digit localization and object\ntracking with synthetic and benchmark datasets, respectively. The experimental\nresults show that our algorithm is far superior to other algorithms in the\nsynthesis datasets and outperforms most existing trackers on the OTB and VOT\ndatasets.",
        "url": "http://arxiv.org/pdf/1804.02056v1.pdf"
    },
    {
        "title": "Not just about size - A Study on the Role of Distributed Word Representations in the Analysis of Scientific Publications",
        "abstract": "The emergence of knowledge graphs in the scholarly communication domain and\nrecent advances in artificial intelligence and natural language processing\nbring us closer to a scenario where intelligent systems can assist scientists\nover a range of knowledge-intensive tasks. In this paper we present\nexperimental results about the generation of word embeddings from scholarly\npublications for the intelligent processing of scientific texts extracted from\nSciGraph. We compare the performance of domain-specific embeddings with\nexisting pre-trained vectors generated from very large and general purpose\ncorpora. Our results suggest that there is a trade-off between corpus\nspecificity and volume. Embeddings from domain-specific scientific corpora\neffectively capture the semantics of the domain. On the other hand, obtaining\ncomparable results through general corpora can also be achieved, but only in\nthe presence of very large corpora of well formed text. Furthermore, We also\nshow that the degree of overlapping between knowledge areas is directly related\nto the performance of embeddings in domain evaluation tasks.",
        "url": "http://arxiv.org/pdf/1804.01772v1.pdf"
    },
    {
        "title": "The structure of evolved representations across different substrates for artificial intelligence",
        "abstract": "Artificial neural networks (ANNs), while exceptionally useful for\nclassification, are vulnerable to misdirection. Small amounts of noise can\nsignificantly affect their ability to correctly complete a task. Instead of\ngeneralizing concepts, ANNs seem to focus on surface statistical regularities\nin a given task. Here we compare how recurrent artificial neural networks, long\nshort-term memory units, and Markov Brains sense and remember their\nenvironments. We show that information in Markov Brains is localized and\nsparsely distributed, while the other neural network substrates \"smear\"\ninformation about the environment across all nodes, which makes them vulnerable\nto noise.",
        "url": "http://arxiv.org/pdf/1804.01660v1.pdf"
    },
    {
        "title": "Review of Deep Learning",
        "abstract": "In recent years, China, the United States and other countries, Google and\nother high-tech companies have increased investment in artificial intelligence.\nDeep learning is one of the current artificial intelligence research's key\nareas. This paper analyzes and summarizes the latest progress and future\nresearch directions of deep learning. Firstly, three basic models of deep\nlearning are outlined, including multilayer perceptrons, convolutional neural\nnetworks, and recurrent neural networks. On this basis, we further analyze the\nemerging new models of convolution neural networks and recurrent neural\nnetworks. This paper then summarizes deep learning's applications in many areas\nof artificial intelligence, including speech processing, computer vision,\nnatural language processing and so on. Finally, this paper discusses the\nexisting problems of deep learning and gives the corresponding possible\nsolutions.",
        "url": "http://arxiv.org/pdf/1804.01653v2.pdf"
    },
    {
        "title": "StarCraft Micromanagement with Reinforcement Learning and Curriculum Transfer Learning",
        "abstract": "Real-time strategy games have been an important field of game artificial\nintelligence in recent years. This paper presents a reinforcement learning and\ncurriculum transfer learning method to control multiple units in StarCraft\nmicromanagement. We define an efficient state representation, which breaks down\nthe complexity caused by the large state space in the game environment. Then a\nparameter sharing multi-agent gradientdescent Sarsa({\\lambda}) (PS-MAGDS)\nalgorithm is proposed to train the units. The learning policy is shared among\nour units to encourage cooperative behaviors. We use a neural network as a\nfunction approximator to estimate the action-value function, and propose a\nreward function to help units balance their move and attack. In addition, a\ntransfer learning method is used to extend our model to more difficult\nscenarios, which accelerates the training process and improves the learning\nperformance. In small scale scenarios, our units successfully learn to combat\nand defeat the built-in AI with 100% win rates. In large scale scenarios,\ncurriculum transfer learning method is used to progressively train a group of\nunits, and shows superior performance over some baseline methods in target\nscenarios. With reinforcement learning and curriculum transfer learning, our\nunits are able to learn appropriate strategies in StarCraft micromanagement\nscenarios.",
        "url": "http://arxiv.org/pdf/1804.00810v1.pdf"
    },
    {
        "title": "Artificial Intelligence and its Role in Near Future",
        "abstract": "AI technology has a long history which is actively and constantly changing\nand growing. It focuses on intelligent agents, which contain devices that\nperceive the environment and based on which takes actions in order to maximize\ngoal success chances. In this paper, we will explain the modern AI basics and\nvarious representative applications of AI. In the context of the modern\ndigitalized world, AI is the property of machines, computer programs, and\nsystems to perform the intellectual and creative functions of a person,\nindependently find ways to solve problems, be able to draw conclusions and make\ndecisions. Most artificial intelligence systems have the ability to learn,\nwhich allows people to improve their performance over time. The recent research\non AI tools, including machine learning, deep learning and predictive analysis\nintended toward increasing the planning, learning, reasoning, thinking and\naction taking ability. Based on which, the proposed research intends towards\nexploring on how the human intelligence differs from the artificial\nintelligence. Moreover, we critically analyze what AI of today is capable of\ndoing, why it still cannot reach human intelligence and what are the open\nchallenges existing in front of AI to reach and outperform human level of\nintelligence. Furthermore, it will explore the future predictions for\nartificial intelligence and based on which potential solution will be\nrecommended to solve it within next decades.",
        "url": "http://arxiv.org/pdf/1804.01396v1.pdf"
    },
    {
        "title": "Toward Intelligent Vehicular Networks: A Machine Learning Framework",
        "abstract": "As wireless networks evolve towards high mobility and providing better support for connected vehicles, a number of new challenges arise due to the resulting high dynamics in vehicular environments and thus motive rethinking of traditional wireless design methodologies. Future intelligent vehicles, which are at the heart of high mobility networks, are increasingly equipped with multiple advanced onboard sensors and keep generating large volumes of data. Machine learning, as an effective approach to artificial intelligence, can provide a rich set of tools to exploit such data for the benefit of the networks. In this article, we first identify the distinctive characteristics of high mobility vehicular networks and motivate the use of machine learning to address the resulting challenges. After a brief introduction of the major concepts of machine learning, we discuss its applications to learn the dynamics of vehicular networks and make informed decisions to optimize network performance. In particular, we discuss in greater detail the application of reinforcement learning in managing network resources as an alternative to the prevalent optimization approach. Finally, some open issues worth further investigation are highlighted.",
        "url": "https://arxiv.org/pdf/1804.00338v3.pdf"
    },
    {
        "title": "SampleAhead: Online Classifier-Sampler Communication for Learning from Synthesized Data",
        "abstract": "State-of-the-art techniques of artificial intelligence, in particular deep\nlearning, are mostly data-driven. However, collecting and manually labeling a\nlarge scale dataset is both difficult and expensive. A promising alternative is\nto introduce synthesized training data, so that the dataset size can be\nsignificantly enlarged with little human labor. But, this raises an important\nproblem in active vision: given an {\\bf infinite} data space, how to\neffectively sample a {\\bf finite} subset to train a visual classifier? This\npaper presents an approach for learning from synthesized data effectively. The\nmotivation is straightforward -- increasing the probability of seeing difficult\ntraining data. We introduce a module named {\\bf SampleAhead} to formulate the\nlearning process into an online communication between a {\\em classifier} and a\n{\\em sampler}, and update them iteratively. In each round, we adjust the\nsampling distribution according to the classification results, and train the\nclassifier using the data sampled from the updated distribution. Experiments\nare performed by introducing synthesized images rendered from ShapeNet models\nto assist PASCAL3D+ classification. Our approach enjoys higher classification\naccuracy, especially in the scenario of a limited number of training samples.\nThis demonstrates its efficiency in exploring the infinite data space.",
        "url": "http://arxiv.org/pdf/1804.00248v2.pdf"
    },
    {
        "title": "How an Electrical Engineer Became an Artificial Intelligence Researcher, a Multiphase Active Contours Analysis",
        "abstract": "This essay examines how what is considered to be artificial intelligence (AI)\nhas changed over time and come to intersect with the expertise of the author.\nInitially, AI developed on a separate trajectory, both topically and\ninstitutionally, from pattern recognition, neural information processing,\ndecision and control systems, and allied topics by focusing on symbolic systems\nwithin computer science departments rather than on continuous systems in\nelectrical engineering departments. The separate evolutions continued\nthroughout the author's lifetime, with some crossover in reinforcement learning\nand graphical models, but were shocked into converging by the virality of deep\nlearning, thus making an electrical engineer into an AI researcher. Now that\nthis convergence has happened, opportunity exists to pursue an agenda that\ncombines learning and reasoning bridged by interpretable machine learning\nmodels.",
        "url": "http://arxiv.org/pdf/1803.11261v1.pdf"
    },
    {
        "title": "Two can play this Game: Visual Dialog with Discriminative Question Generation and Answering",
        "abstract": "Human conversation is a complex mechanism with subtle nuances. It is hence an\nambitious goal to develop artificial intelligence agents that can participate\nfluently in a conversation. While we are still far from achieving this goal,\nrecent progress in visual question answering, image captioning, and visual\nquestion generation shows that dialog systems may be realizable in the not too\ndistant future. To this end, a novel dataset was introduced recently and\nencouraging results were demonstrated, particularly for question answering. In\nthis paper, we demonstrate a simple symmetric discriminative baseline, that can\nbe applied to both predicting an answer as well as predicting a question. We\nshow that this method performs on par with the state of the art, even memory\nnet based methods. In addition, for the first time on the visual dialog\ndataset, we assess the performance of a system asking questions, and\ndemonstrate how visual dialog can be generated from discriminative question\ngeneration and question answering.",
        "url": "http://arxiv.org/pdf/1803.11186v1.pdf"
    },
    {
        "title": "Artificial Intelligence and Robotics",
        "abstract": "The recent successes of AI have captured the wildest imagination of both the\nscientific communities and the general public. Robotics and AI amplify human\npotentials, increase productivity and are moving from simple reasoning towards\nhuman-like cognitive abilities. Current AI technologies are used in a set area\nof applications, ranging from healthcare, manufacturing, transport, energy, to\nfinancial services, banking, advertising, management consulting and government\nagencies. The global AI market is around 260 billion USD in 2016 and it is\nestimated to exceed 3 trillion by 2024. To understand the impact of AI, it is\nimportant to draw lessons from it's past successes and failures and this white\npaper provides a comprehensive explanation of the evolution of AI, its current\nstatus and future directions.",
        "url": "http://arxiv.org/pdf/1803.10813v1.pdf"
    },
    {
        "title": "Unsupervised Predictive Memory in a Goal-Directed Agent",
        "abstract": "Animals execute goal-directed behaviours despite the limited range and scope\nof their sensors. To cope, they explore environments and store memories\nmaintaining estimates of important information that is not presently available.\nRecently, progress has been made with artificial intelligence (AI) agents that\nlearn to perform tasks from sensory input, even at a human level, by merging\nreinforcement learning (RL) algorithms with deep neural networks, and the\nexcitement surrounding these results has led to the pursuit of related ideas as\nexplanations of non-human animal learning. However, we demonstrate that\ncontemporary RL algorithms struggle to solve simple tasks when enough\ninformation is concealed from the sensors of the agent, a property called\n\"partial observability\". An obvious requirement for handling partially observed\ntasks is access to extensive memory, but we show memory is not enough; it is\ncritical that the right information be stored in the right format. We develop a\nmodel, the Memory, RL, and Inference Network (MERLIN), in which memory\nformation is guided by a process of predictive modeling. MERLIN facilitates the\nsolution of tasks in 3D virtual reality environments for which partial\nobservability is severe and memories must be maintained over long durations.\nOur model demonstrates a single learning agent architecture that can solve\ncanonical behavioural tasks in psychology and neurobiology without strong\nsimplifying assumptions about the dimensionality of sensory input or the\nduration of experiences.",
        "url": "http://arxiv.org/pdf/1803.10760v1.pdf"
    },
    {
        "title": "A Distributed Extension of the Turing Machine",
        "abstract": "The Turing Machine has two implicit properties that depend on its underlying\nnotion of computing: the format is fully determinate and computations are\ninformation preserving. Distributed representations lack these properties and\ncannot be fully captured by Turing's standard model. To address this limitation\na distributed extension of the Turing Machine is introduced in this paper. In\nthe extended machine, functions and abstractions are expressed extensionally\nand computations are entropic. The machine is applied to the definition of an\nassociative memory, with its corresponding memory register, recognition and\nretrieval operations. The memory is tested with an experiment for storing and\nrecognizing hand written digits with satisfactory results. The experiment can\nbe seen as a proof of concept that information can be stored and processed\neffectively in a highly distributed fashion using a symbolic but not fully\ndeterminate format. The new machine augments the symbolic mode of computing\nwith consequences on the way Church Thesis is understood. The paper is\nconcluded with a discussion of some implications of the extended machine for\nArtificial Intelligence and Cognition.",
        "url": "http://arxiv.org/pdf/1803.10648v1.pdf"
    },
    {
        "title": "Applications of Artificial Intelligence to Network Security",
        "abstract": "Attacks to networks are becoming more complex and sophisticated every day.\nBeyond the so-called script-kiddies and hacking newbies, there is a myriad of\nprofessional attackers seeking to make serious profits infiltrating in\ncorporate networks. Either hostile governments, big corporations or mafias are\nconstantly increasing their resources and skills in cybercrime in order to spy,\nsteal or cause damage more effectively. traditional approaches to Network\nSecurity seem to start hitting their limits and it is being recognized the need\nfor a smarter approach to threat detections. This paper provides an\nintroduction on the need for evolution of Cyber Security techniques and how\nArtificial Intelligence could be of application to help solving some of the\nproblems. It provides also, a high-level overview of some state of the art AI\nNetwork Security techniques, to finish analysing what is the foreseeable future\nof the application of AI to Network Security.",
        "url": "http://arxiv.org/pdf/1803.09992v1.pdf"
    },
    {
        "title": "Generative Design in Minecraft (GDMC), Settlement Generation Competition",
        "abstract": "This paper introduces the settlement generation competition for Minecraft,\nthe first part of the Generative Design in Minecraft challenge. The settlement\ngeneration competition is about creating Artificial Intelligence (AI) agents\nthat can produce functional, aesthetically appealing and believable settlements\nadapted to a given Minecraft map - ideally at a level that can compete with\nhuman created designs. The aim of the competition is to advance procedural\ncontent generation for games, especially in overcoming the challenges of\nadaptive and holistic PCG. The paper introduces the technical details of the\nchallenge, but mostly focuses on what challenges this competition provides and\nwhy they are scientifically relevant.",
        "url": "http://arxiv.org/pdf/1803.09853v2.pdf"
    },
    {
        "title": "Scalable photonic reinforcement learning by time-division multiplexing of laser chaos",
        "abstract": "Reinforcement learning involves decision making in dynamic and uncertain\nenvironments and constitutes a crucial element of artificial intelligence. In\nour previous work, we experimentally demonstrated that the ultrafast chaotic\noscillatory dynamics of lasers can be used to solve the two-armed bandit\nproblem efficiently, which requires decision making concerning a class of\ndifficult trade-offs called the exploration-exploitation dilemma. However, only\ntwo selections were employed in that research; thus, the scalability of the\nlaser-chaos-based reinforcement learning should be clarified. In this study, we\ndemonstrated a scalable, pipelined principle of resolving the multi-armed\nbandit problem by introducing time-division multiplexing of chaotically\noscillated ultrafast time-series. The experimental demonstrations in which\nbandit problems with up to 64 arms were successfully solved are presented in\nthis report. Detailed analyses are also provided that include performance\ncomparisons among laser chaos signals generated in different physical\nconditions, which coincide with the diffusivity inherent in the time series.\nThis study paves the way for ultrafast reinforcement learning by taking\nadvantage of the ultrahigh bandwidths of light wave and practical enabling\ntechnologies.",
        "url": "http://arxiv.org/pdf/1803.09425v1.pdf"
    },
    {
        "title": "Augmented Artificial Intelligence: a Conceptual Framework",
        "abstract": "All artificial Intelligence (AI) systems make errors. These errors are\nunexpected, and differ often from the typical human mistakes (\"non-human\"\nerrors). The AI errors should be corrected without damage of existing skills\nand, hopefully, avoiding direct human expertise. This paper presents an initial\nsummary report of project taking new and systematic approach to improving the\nintellectual effectiveness of the individual AI by communities of AIs. We\ncombine some ideas of learning in heterogeneous multiagent systems with new and\noriginal mathematical approaches for non-iterative corrections of errors of\nlegacy AI systems. The mathematical foundations of AI non-destructive\ncorrection are presented and a series of new stochastic separation theorems is\nproven. These theorems provide a new instrument for the development, analysis,\nand assessment of machine learning methods and algorithms in high dimension.\nThey demonstrate that in high dimensions and even for exponentially large\nsamples, linear classifiers in their classical Fisher's form are powerful\nenough to separate errors from correct responses with high probability and to\nprovide efficient solution to the non-destructive corrector problem. In\nparticular, we prove some hypotheses formulated in our paper `Stochastic\nSeparation Theorems' (Neural Networks, 94, 255--259, 2017), and answer one\ngeneral problem published by Donoho and Tanner in 2009.",
        "url": "http://arxiv.org/pdf/1802.02172v3.pdf"
    },
    {
        "title": "Face Recognition with Hybrid Efficient Convolution Algorithms on FPGAs",
        "abstract": "Deep Convolutional Neural Networks have become a Swiss knife in solving\ncritical artificial intelligence tasks. However, deploying deep CNN models for\nlatency-critical tasks remains to be challenging because of the complex nature\nof CNNs. Recently, FPGA has become a favorable device to accelerate deep CNNs\nthanks to its high parallel processing capability and energy efficiency. In\nthis work, we explore different fast convolution algorithms including Winograd\nand Fast Fourier Transform (FFT), and find an optimal strategy to apply them\ntogether on different types of convolutions. We also propose an optimization\nscheme to exploit parallelism on novel CNN architectures such as Inception\nmodules in GoogLeNet. We implement a configurable IP-based face recognition\nacceleration system based on FaceNet using High-Level Synthesis. Our\nimplementation on a Xilinx Ultrascale device achieves 3.75x latency speedup\ncompared to a high-end NVIDIA GPU and surpasses previous FPGA results\nsignificantly.",
        "url": "http://arxiv.org/pdf/1803.09004v1.pdf"
    },
    {
        "title": "Computational Power and the Social Impact of Artificial Intelligence",
        "abstract": "Machine learning is a computational process. To that end, it is inextricably\ntied to computational power - the tangible material of chips and semiconductors\nthat the algorithms of machine intelligence operate on. Most obviously,\ncomputational power and computing architectures shape the speed of training and\ninference in machine learning, and therefore influence the rate of progress in\nthe technology. But, these relationships are more nuanced than that: hardware\nshapes the methods used by researchers and engineers in the design and\ndevelopment of machine learning models. Characteristics such as the power\nconsumption of chips also define where and how machine learning can be used in\nthe real world.\n  Despite this, many analyses of the social impact of the current wave of\nprogress in AI have not substantively brought the dimension of hardware into\ntheir accounts. While a common trope in both the popular press and scholarly\nliterature is to highlight the massive increase in computational power that has\nenabled the recent breakthroughs in machine learning, the analysis frequently\ngoes no further than this observation around magnitude. This paper aims to dig\nmore deeply into the relationship between computational power and the\ndevelopment of machine learning. Specifically, it examines how changes in\ncomputing architectures, machine learning methodologies, and supply chains\nmight influence the future of AI. In doing so, it seeks to trace a set of\nspecific relationships between this underlying hardware layer and the broader\nsocial impacts and risks around AI.",
        "url": "http://arxiv.org/pdf/1803.08971v1.pdf"
    },
    {
        "title": "Black-box Testing of First-Order Logic Ontologies Using WordNet",
        "abstract": "Artificial Intelligence aims to provide computer programs with commonsense\nknowledge to reason about our world. This paper offers a new practical approach\ntowards automated commonsense reasoning with first-order logic (FOL)\nontologies. We propose a new black-box testing methodology of FOL SUMO-based\nontologies by exploiting WordNet and its mapping into SUMO. Our proposal\nincludes a method for the (semi-)automatic creation of a very large benchmark\nof competency questions and a procedure for its automated evaluation by using\nautomated theorem provers (ATPs). Applying different quality criteria, our\ntesting proposal enables a successful evaluation of a) the competency of\nseveral translations of SUMO into FOL and b) the performance of various\nautomated ATPs. Finally, we also provide a fine-grained and complete analysis\nof the commonsense reasoning competency of current FOL SUMO-based ontologies.",
        "url": "http://arxiv.org/pdf/1705.10217v3.pdf"
    },
    {
        "title": "Learning State Representations for Query Optimization with Deep Reinforcement Learning",
        "abstract": "Deep reinforcement learning is quickly changing the field of artificial\nintelligence. These models are able to capture a high level understanding of\ntheir environment, enabling them to learn difficult dynamic tasks in a variety\nof domains. In the database field, query optimization remains a difficult\nproblem. Our goal in this work is to explore the capabilities of deep\nreinforcement learning in the context of query optimization. At each state, we\nbuild queries incrementally and encode properties of subqueries through a\nlearned representation. The challenge here lies in the formation of the state\ntransition function, which defines how the current subquery state combines with\nthe next query operation (action) to yield the next state. As a first step in\nthis direction, we focus the state representation problem and the formation of\nthe state transition function. We describe our approach and show preliminary\nresults. We further discuss how we can use the state representation to improve\nquery optimization using reinforcement learning.",
        "url": "http://arxiv.org/pdf/1803.08604v1.pdf"
    },
    {
        "title": "Learning the Localization Function: Machine Learning Approach to Fingerprinting Localization",
        "abstract": "Considered as a data-driven approach, Fingerprinting Localization Solutions\n(FPSs) enjoy huge popularity due to their good performance and minimal\nenvironment information requirement. This papers addresses applications of\nartificial intelligence to solve two problems in Received Signal Strength\nIndicator (RSSI) based FPS, first the cumbersome training database construction\nand second the extrapolation of fingerprinting algorithm for similar buildings\nwith slight environmental changes. After a concise overview of deep learning\ndesign techniques, two main techniques widely used in deep learning are\nexploited for the above mentioned issues namely data augmentation and transfer\nlearning. We train a multi-layer neural network that learns the mapping from\nthe observations to the locations. A data augmentation method is proposed to\nincrease the training database size based on the structure of RSSI measurements\nand hence reducing effectively the amount of training data. Then it is shown\nexperimentally how a model trained for a particular building can be transferred\nto a similar one by fine tuning with significantly smaller training numbers.\nThe paper implicitly discusses the new guidelines to consider about deep\nlearning designs when they are employed in a new application context.",
        "url": "http://arxiv.org/pdf/1803.08153v1.pdf"
    },
    {
        "title": "Transaction Fraud Detection Using GRU-centered Sandwich-structured Model",
        "abstract": "Rapid growth of modern technologies such as internet and mobile computing are\nbringing dramatically increased e-commerce payments, as well as the explosion\nin transaction fraud. Meanwhile, fraudsters are continually refining their\ntricks, making rule-based fraud detection systems difficult to handle the\never-changing fraud patterns. Many data mining and artificial intelligence\nmethods have been proposed for identifying small anomalies in large transaction\ndata sets, increasing detecting efficiency to some extent. Nevertheless, there\nis always a contradiction that most methods are irrelevant to transaction\nsequence, yet sequence-related methods usually cannot learn information at\nsingle-transaction level well. In this paper, a new \"within->between->within\"\nsandwich-structured sequence learning architecture has been proposed by\nstacking an ensemble method, a deep sequential learning method and another\ntop-layer ensemble classifier in proper order. Moreover, attention mechanism\nhas also been introduced in to further improve performance. Models in this\nstructure have been manifested to be very efficient in scenarios like fraud\ndetection, where the information sequence is made up of vectors with complex\ninterconnected features.",
        "url": "http://arxiv.org/pdf/1711.01434v3.pdf"
    },
    {
        "title": "Artificial Intelligence Enabled Software Defined Networking: A Comprehensive Overview",
        "abstract": "Software defined networking (SDN) represents a promising networking\narchitecture that combines central management and network programmability. SDN\nseparates the control plane from the data plane and moves the network\nmanagement to a central point, called the controller, that can be programmed\nand used as the brain of the network. Recently, the research community has\nshowed an increased tendency to benefit from the recent advancements in the\nartificial intelligence (AI) field to provide learning abilities and better\ndecision making in SDN. In this study, we provide a detailed overview of the\nrecent efforts to include AI in SDN. Our study showed that the research efforts\nfocused on three main sub-fields of AI namely: machine learning,\nmeta-heuristics and fuzzy inference systems. Accordingly, in this work we\ninvestigate their different application areas and potential use, as well as the\nimprovements achieved by including AI-based techniques in the SDN paradigm.",
        "url": "http://arxiv.org/pdf/1803.06818v3.pdf"
    },
    {
        "title": "Viewpoint: Artificial Intelligence and Labour",
        "abstract": "The welfare of modern societies has been intrinsically linked to wage labour.\nWith some exceptions, the modern human has to sell her labour-power to be able\nreproduce biologically and socially. Thus, a lingering fear of technological\nunemployment features predominately as a theme among Artificial Intelligence\nresearchers. In this short paper we show that, if past trends are anything to\ngo by, this fear is irrational. On the contrary, we argue that the main problem\nhumanity will be facing is the normalisation of extremely long working hours.",
        "url": "http://arxiv.org/pdf/1803.06563v1.pdf"
    },
    {
        "title": "A Dataset and Architecture for Visual Reasoning with a Working Memory",
        "abstract": "A vexing problem in artificial intelligence is reasoning about events that\noccur in complex, changing visual stimuli such as in video analysis or game\nplay. Inspired by a rich tradition of visual reasoning and memory in cognitive\npsychology and neuroscience, we developed an artificial, configurable visual\nquestion and answer dataset (COG) to parallel experiments in humans and\nanimals. COG is much simpler than the general problem of video analysis, yet it\naddresses many of the problems relating to visual and logical reasoning and\nmemory -- problems that remain challenging for modern deep learning\narchitectures. We additionally propose a deep learning architecture that\nperforms competitively on other diagnostic VQA datasets (i.e. CLEVR) as well as\neasy settings of the COG dataset. However, several settings of COG result in\ndatasets that are progressively more challenging to learn. After training, the\nnetwork can zero-shot generalize to many new tasks. Preliminary analyses of the\nnetwork architectures trained on COG demonstrate that the network accomplishes\nthe task in a manner interpretable to humans.",
        "url": "http://arxiv.org/pdf/1803.06092v2.pdf"
    },
    {
        "title": "Can Autism be Catered with Artificial Intelligence-Assisted Intervention Technology? A Literature Review",
        "abstract": "This article presents an extensive literature review of technology based\nintervention methodologies for individuals facing Autism Spectrum Disorder\n(ASD). Reviewed methodologies include: contemporary Computer Aided Systems\n(CAS), Computer Vision Assisted Technologies (CVAT) and Virtual Reality (VR) or\nArtificial Intelligence (AI)-Assisted interventions. The research over the past\ndecade has provided enough demonstrations that individuals with ASD have a\nstrong interest in technology based interventions, which are useful in both,\nclinical settings as well as at home and classrooms. Despite showing great\npromise, research in developing an advanced technology based intervention that\nis clinically quantitative for ASD is minimal. Moreover, the clinicians are\ngenerally not convinced about the potential of the technology based\ninterventions due to non-empirical nature of published results. A major reason\nbehind this lack of acceptability is that a vast majority of studies on\ndistinct intervention methodologies do not follow any specific standard or\nresearch design. We conclude from our findings that there remains a gap between\nthe research community of computer science, psychology and neuroscience to\ndevelop an AI assisted intervention technology for individuals suffering from\nASD. Following the development of a standardized AI based intervention\ntechnology, a database needs to be developed, to devise effective AI\nalgorithms.",
        "url": "http://arxiv.org/pdf/1803.05181v5.pdf"
    },
    {
        "title": "An Introduction to Deep Visual Explanation",
        "abstract": "The practical impact of deep learning on complex supervised learning problems\nhas been significant, so much so that almost every Artificial Intelligence\nproblem, or at least a portion thereof, has been somehow recast as a deep\nlearning problem. The applications appeal is significant, but this appeal is\nincreasingly challenged by what some call the challenge of explainability, or\nmore generally the more traditional challenge of debuggability: if the outcomes\nof a deep learning process produce unexpected results (e.g., less than expected\nperformance of a classifier), then there is little available in the way of\ntheories or tools to help investigate the potential causes of such unexpected\nbehavior, especially when this behavior could impact people's lives. We\ndescribe a preliminary framework to help address this issue, which we call\n\"deep visual explanation\" (DVE). \"Deep,\" because it is the development and\nperformance of deep neural network models that we want to understand. \"Visual,\"\nbecause we believe that the most rapid insight into a complex multi-dimensional\nmodel is provided by appropriate visualization techniques, and \"Explanation,\"\nbecause in the spectrum from instrumentation by inserting print statements to\nthe abductive inference of explanatory hypotheses, we believe that the key to\nunderstanding deep learning relies on the identification and exposure of\nhypotheses about the performance behavior of a learned deep model. In the\nexposition of our preliminary framework, we use relatively straightforward\nimage classification examples and a variety of choices on initial configuration\nof a deep model building scenario. By careful but not complicated\ninstrumentation, we expose classification outcomes of deep models using\nvisualization, and also show initial results for one potential application of\ninterpretability.",
        "url": "http://arxiv.org/pdf/1711.09482v2.pdf"
    },
    {
        "title": "Toolflows for Mapping Convolutional Neural Networks on FPGAs: A Survey and Future Directions",
        "abstract": "In the past decade, Convolutional Neural Networks (CNNs) have demonstrated\nstate-of-the-art performance in various Artificial Intelligence tasks. To\naccelerate the experimentation and development of CNNs, several software\nframeworks have been released, primarily targeting power-hungry CPUs and GPUs.\nIn this context, reconfigurable hardware in the form of FPGAs constitutes a\npotential alternative platform that can be integrated in the existing deep\nlearning ecosystem to provide a tunable balance between performance, power\nconsumption and programmability. In this paper, a survey of the existing\nCNN-to-FPGA toolflows is presented, comprising a comparative study of their key\ncharacteristics which include the supported applications, architectural\nchoices, design space exploration methods and achieved performance. Moreover,\nmajor challenges and objectives introduced by the latest trends in CNN\nalgorithmic research are identified and presented. Finally, a uniform\nevaluation methodology is proposed, aiming at the comprehensive, complete and\nin-depth evaluation of CNN-to-FPGA toolflows.",
        "url": "http://arxiv.org/pdf/1803.05900v1.pdf"
    },
    {
        "title": "Using Convolutional Neural Networks for Determining Reticulocyte Percentage in Cats",
        "abstract": "Recent advances in artificial intelligence (AI), specifically in computer\nvision (CV) and deep learning (DL), have created opportunities for novel\nsystems in many fields. In the last few years, deep learning applications have\ndemonstrated impressive results not only in fields such as autonomous driving\nand robotics, but also in the field of medicine, where they have, in some\ncases, even exceeded human-level performance. However, despite the huge\npotential, adoption of deep learning-based methods is still slow in many areas,\nespecially in veterinary medicine, where we haven't been able to find any\nresearch papers using modern convolutional neural networks (CNNs) in medical\nimage processing. We believe that using deep learning-based medical imaging can\nenable more accurate, faster and less expensive diagnoses in veterinary\nmedicine. In order to do so, however, these methods have to be accessible to\neveryone in this field, not just to computer scientists. To show the potential\nof this technology, we present results on a real-world task in veterinary\nmedicine that is usually done manually: feline reticulocyte percentage. Using\nan open source Keras implementation of the Single-Shot MultiBox Detector (SSD)\nmodel architecture and training it on only 800 labeled images, we achieve an\naccuracy of 98.7% at predicting the correct number of aggregate reticulocytes\nin microscope images of cat blood smears. The main motivation behind this paper\nis to show not only that deep learning can approach or even exceed human-level\nperformance on a task like this, but also that anyone in the field can\nimplement it, even without a background in computer science.",
        "url": "http://arxiv.org/pdf/1803.04873v2.pdf"
    },
    {
        "title": "The 2017 AIBIRDS Competition",
        "abstract": "This paper presents an overview of the sixth AIBIRDS competition, held at the\n26th International Joint Conference on Artificial Intelligence. This\ncompetition tasked participants with developing an intelligent agent which can\nplay the physics-based puzzle game Angry Birds. This game uses a sophisticated\nphysics engine that requires agents to reason and predict the outcome of\nactions with only limited environmental information. Agents entered into this\ncompetition were required to solve a wide assortment of previously unseen\nlevels within a set time limit. The physical reasoning and planning required to\nsolve these levels are very similar to those of many real-world problems. This\nyear's competition featured some of the best agents developed so far and even\nincluded several new AI techniques such as deep reinforcement learning. Within\nthis paper we describe the framework, rules, submitted agents and results for\nthis competition. We also provide some background information on related work\nand other video game AI competitions, as well as discussing some potential\nideas for future AIBIRDS competitions and agent improvements.",
        "url": "http://arxiv.org/pdf/1803.05156v1.pdf"
    },
    {
        "title": "Fractal AI: A fragile theory of intelligence",
        "abstract": "Fractal AI is a theory for general artificial intelligence. It allows deriving new mathematical tools that constitute the foundations for a new kind of stochastic calculus, by modelling information using cellular automaton-like structures instead of smooth functions. In the repository included we are presenting a new Agent, derived from the first principles of the theory, which is capable of solving Atari games several orders of magnitude more efficiently than other similar techniques, like Monte Carlo Tree Search. The code provided shows how it is now possible to beat some of the current State of The Art benchmarks on Atari games, without previous learning and using less than 1000 samples to calculate each one of the actions when standard MCTS uses 3 Million samples. Among other things, Fractal AI makes it possible to generate a huge database of top performing examples with a very little amount of computation required, transforming Reinforcement Learning into a supervised problem. The algorithm presented is capable of solving the exploration vs exploitation dilemma on both the discrete and continuous cases, while maintaining control over any aspect of the behaviour of the Agent. From a general approach, new techniques presented here have direct applications to other areas such as Non-equilibrium thermodynamics, chemistry, quantum physics, economics, information theory, and non-linear control theory.",
        "url": "https://arxiv.org/pdf/1803.05049v5.pdf"
    },
    {
        "title": "Machine Learning Harnesses Molecular Dynamics to Discover New $\u03bc$ Opioid Chemotypes",
        "abstract": "Computational chemists typically assay drug candidates by virtually screening\ncompounds against crystal structures of a protein despite the fact that some\ntargets, like the $\\mu$ Opioid Receptor and other members of the GPCR family,\ntraverse many non-crystallographic states. We discover new conformational\nstates of $\\mu OR$ with molecular dynamics simulation and then machine learn\nligand-structure relationships to predict opioid ligand function. These\nartificial intelligence models identified a novel $\\mu$ opioid chemotype.",
        "url": "http://arxiv.org/pdf/1803.04479v1.pdf"
    },
    {
        "title": "Separation of time scales and direct computation of weights in deep neural networks",
        "abstract": "Artificial intelligence is revolutionizing our lives at an ever increasing\npace. At the heart of this revolution is the recent advancements in deep neural\nnetworks (DNN), learning to perform sophisticated, high-level tasks. However,\ntraining DNNs requires massive amounts of data and is very computationally\nintensive. Gaining analytical understanding of the solutions found by DNNs can\nhelp us devise more efficient training algorithms, replacing the commonly used\nmthod of stochastic gradient descent (SGD). We analyze the dynamics of SGD and\nshow that, indeed, direct computation of the solutions is possible in many\ncases. We show that a high performing setup used in DNNs introduces a\nseparation of time-scales in the training dynamics, allowing SGD to train\nlayers from the lowest (closest to input) to the highest. We then show that for\neach layer, the distribution of solutions found by SGD can be estimated using a\nclass-based principal component analysis (PCA) of the layer's input. This\nfinding allows us to forgo SGD entirely and directly derive the DNN parameters\nusing this class-based PCA, which can be well estimated using significantly\nless data than SGD. We implement these results on image datasets MNIST, CIFAR10\nand CIFAR100 and find that, in fact, layers derived using our class-based PCA\nperform comparable or superior to neural networks of the same size and\narchitecture trained using SGD. We also confirm that the class-based PCA often\nconverges using a fraction of the data required for SGD. Thus, using our method\ntraining time can be reduced both by requiring less training data than SGD, and\nby eliminating layers in the costly backpropagation step of the training.",
        "url": "http://arxiv.org/pdf/1703.04757v3.pdf"
    },
    {
        "title": "Institutional Metaphors for Designing Large-Scale Distributed AI versus AI Techniques for Running Institutions",
        "abstract": "Artificial Intelligence (AI) started out with an ambition to reproduce the human mind, but, as the sheer scale of that ambition became manifest, it quickly retreated into either studying specialized intelligent behaviours, or proposing over-arching architectural concepts for interfacing specialized intelligent behaviour components, conceived of as agents in a kind of organization. This agent-based modeling paradigm, in turn, proves to have interesting applications in understanding, simulating, and predicting the behaviour of social and legal structures on an aggregate level. For these reasons, this chapter examines a number of relevant cross-cutting concerns, conceptualizations, modeling problems and design challenges in large-scale distributed Artificial Intelligence, as well as in institutional systems, and identifies potential grounds for novel advances.",
        "url": "https://arxiv.org/pdf/1803.03407v2.pdf"
    },
    {
        "title": "The Challenge of Crafting Intelligible Intelligence",
        "abstract": "Since Artificial Intelligence (AI) software uses techniques like deep\nlookahead search and stochastic optimization of huge neural networks to fit\nmammoth datasets, it often results in complex behavior that is difficult for\npeople to understand. Yet organizations are deploying AI algorithms in many\nmission-critical settings. To trust their behavior, we must make AI\nintelligible, either by using inherently interpretable models or by developing\nnew methods for explaining and controlling otherwise overwhelmingly complex\ndecisions using local approximation, vocabulary alignment, and interactive\nexplanation. This paper argues that intelligibility is essential, surveys\nrecent work on building such systems, and highlights key directions for\nresearch.",
        "url": "http://arxiv.org/pdf/1803.04263v3.pdf"
    },
    {
        "title": "Value Alignment, Fair Play, and the Rights of Service Robots",
        "abstract": "Ethics and safety research in artificial intelligence is increasingly framed\nin terms of \"alignment\" with human values and interests. I argue that Turing's\ncall for \"fair play for machines\" is an early and often overlooked contribution\nto the alignment literature. Turing's appeal to fair play suggests a need to\ncorrect human behavior to accommodate our machines, a surprising inversion of\nhow value alignment is treated today. Reflections on \"fair play\" motivate a\nnovel interpretation of Turing's notorious \"imitation game\" as a condition not\nof intelligence but instead of value alignment: a machine demonstrates a\nminimal degree of alignment (with the norms of conversation, for instance) when\nit can go undetected when interrogated by a human. I carefully distinguish this\ninterpretation from the Moral Turing Test, which is not motivated by a\nprinciple of fair play, but instead depends on imitation of human moral\nbehavior. Finally, I consider how the framework of fair play can be used to\nsituate the debate over robot rights within the alignment literature. I argue\nthat extending rights to service robots operating in public spaces is \"fair\" in\nprecisely the sense that it encourages an alignment of interests between humans\nand machines.",
        "url": "http://arxiv.org/pdf/1803.02852v1.pdf"
    },
    {
        "title": "Deep Learning in Pharmacogenomics: From Gene Regulation to Patient Stratification",
        "abstract": "This Perspective provides examples of current and future applications of deep\nlearning in pharmacogenomics, including: (1) identification of novel regulatory\nvariants located in noncoding domains and their function as applied to\npharmacoepigenomics; (2) patient stratification from medical records; and (3)\nprediction of drugs, targets, and their interactions. Deep learning\nencapsulates a family of machine learning algorithms that over the last decade\nhas transformed many important subfields of artificial intelligence (AI) and\nhas demonstrated breakthrough performance improvements on a wide range of tasks\nin biomedicine. We anticipate that in the future deep learning will be widely\nused to predict personalized drug response and optimize medication selection\nand dosing, using knowledge extracted from large and complex molecular,\nepidemiological, clinical, and demographic datasets.",
        "url": "http://arxiv.org/pdf/1801.08570v2.pdf"
    },
    {
        "title": "A metric for sets of trajectories that is practical and mathematically consistent",
        "abstract": "Metrics on the space of sets of trajectories are important for scientists in the field of computer vision, machine learning, robotics, and general artificial intelligence. However, existing notions of closeness between sets of trajectories are either mathematically inconsistent or of limited practical use. In this paper, we outline the limitations in the current mathematically-consistent metrics, which are based on OSPA (Schuhmacher et al. 2008); and the inconsistencies in the heuristic notions of closeness used in practice, whose main ideas are common to the CLEAR MOT measures (Keni and Rainer 2008) widely used in computer vision. In two steps, we then propose a new intuitive metric between sets of trajectories and address these limitations. First, we explain a solution that leads to a metric that is hard to compute. Then we modify this formulation to obtain a metric that is easy to compute while keeping the useful properties of the previous metric. Our notion of closeness is the first demonstrating the following three features: the metric 1) can be quickly computed, 2) incorporates confusion of trajectories' identity in an optimal way, and 3) is a metric in the mathematical sense.",
        "url": "https://arxiv.org/pdf/1601.03094v3.pdf"
    },
    {
        "title": "The ORCA Hub: Explainable Offshore Robotics through Intelligent Interfaces",
        "abstract": "We present the UK Robotics and Artificial Intelligence Hub for Offshore\nRobotics for Certification of Assets (ORCA Hub), a 3.5 year EPSRC funded,\nmulti-site project. The ORCA Hub vision is to use teams of robots and\nautonomous intelligent systems (AIS) to work on offshore energy platforms to\nenable cheaper, safer and more efficient working practices. The ORCA Hub will\nresearch, integrate, validate and deploy remote AIS solutions that can operate\nwith existing and future offshore energy assets and sensors, interacting safely\nin autonomous or semi-autonomous modes in complex and cluttered environments,\nco-operating with remote operators. The goal is that through the use of such\nrobotic systems offshore, the need for personnel will decrease. To enable this\nto happen, the remote operator will need a high level of situation awareness\nand key to this is the transparency of what the autonomous systems are doing\nand why. This increased transparency will facilitate a trusting relationship,\nwhich is particularly key in high-stakes, hazardous situations.",
        "url": "http://arxiv.org/pdf/1803.02100v1.pdf"
    },
    {
        "title": "Probabilistic design of a molybdenum-base alloy using a neural network",
        "abstract": "An artificial intelligence tool is exploited to discover and characterize a\nnew molybdenum-base alloy that is the most likely to simultaneously satisfy\ntargets of cost, phase stability, precipitate content, yield stress, and\nhardness. Experimental testing demonstrates that the proposed alloy fulfils the\ncomputational predictions, and furthermore the physical properties exceed those\nof other commercially available Mo-base alloys for forging-die applications.",
        "url": "http://arxiv.org/pdf/1803.00879v1.pdf"
    },
    {
        "title": "Artificial Intelligence as Structural Estimation: Economic Interpretations of Deep Blue, Bonanza, and AlphaGo",
        "abstract": "Artificial intelligence (AI) has achieved superhuman performance in a growing\nnumber of tasks, but understanding and explaining AI remain challenging. This\npaper clarifies the connections between machine-learning algorithms to develop\nAIs and the econometrics of dynamic structural models through the case studies\nof three famous game AIs. Chess-playing Deep Blue is a calibrated value\nfunction, whereas shogi-playing Bonanza is an estimated value function via\nRust's (1987) nested fixed-point method. AlphaGo's \"supervised-learning policy\nnetwork\" is a deep neural network implementation of Hotz and Miller's (1993)\nconditional choice probability estimation; its \"reinforcement-learning value\nnetwork\" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional\nchoice simulation method. Relaxing these AIs' implicit econometric assumptions\nwould improve their structural interpretability.",
        "url": "http://arxiv.org/pdf/1710.10967v3.pdf"
    },
    {
        "title": "Escoin: Efficient Sparse Convolutional Neural Network Inference on GPUs",
        "abstract": "Deep neural networks have achieved remarkable accuracy in many artificial\nintelligence applications, e.g. computer vision, at the cost of a large number\nof parameters and high computational complexity. Weight pruning can compress\nDNN models by removing redundant parameters in the networks, but it brings\nsparsity in the weight matrix, and therefore makes the computation inefficient\non GPUs. Although pruning can remove more than 80% of the weights, it actually\nhurts inference performance (speed) when running models on GPUs.\n  Two major problems cause this unsatisfactory performance on GPUs. First,\nlowering convolution onto matrix multiplication reduces data reuse\nopportunities and wastes memory bandwidth. Second, the sparsity brought by\npruning makes the computation irregular, which leads to inefficiency when\nrunning on massively parallel GPUs. To overcome these two limitations, we\npropose Escort, an efficient sparse convolutional neural networks on GPUs.\nInstead of using the lowering method, we choose to compute the sparse\nconvolutions directly. We then orchestrate the parallelism and locality for the\ndirect sparse convolution kernel, and apply customized optimization techniques\nto further improve performance. Evaluation on NVIDIA GPUs show that Escort can\nimprove sparse convolution speed by 2.63x and 3.07x, and inference speed by\n1.43x and 1.69x, compared to CUBLAS and CUSPARSE respectively.",
        "url": "http://arxiv.org/pdf/1802.10280v2.pdf"
    },
    {
        "title": "Improved Explainability of Capsule Networks: Relevance Path by Agreement",
        "abstract": "Recent advancements in signal processing and machine learning domains have\nresulted in an extensive surge of interest in deep learning models due to their\nunprecedented performance and high accuracy for different and challenging\nproblems of significant engineering importance. However, when such deep\nlearning architectures are utilized for making critical decisions such as the\nones that involve human lives (e.g., in medical applications), it is of\nparamount importance to understand, trust, and in one word \"explain\" the\nrational behind deep models' decisions. Currently, deep learning models are\ntypically considered as black-box systems, which do not provide any clue on\ntheir internal processing actions. Although some recent efforts have been\ninitiated to explain behavior and decisions of deep networks, explainable\nartificial intelligence (XAI) domain is still in its infancy. In this regard,\nwe consider capsule networks (referred to as CapsNets), which are novel deep\nstructures; recently proposed as an alternative counterpart to convolutional\nneural networks (CNNs), and posed to change the future of machine intelligence.\nIn this paper, we investigate and analyze structures and behaviors of the\nCapsNets and illustrate potential explainability properties of such networks.\nFurthermore, we show possibility of transforming deep learning architectures in\nto transparent networks via incorporation of capsules in different layers\ninstead of convolution layers of the CNNs.",
        "url": "http://arxiv.org/pdf/1802.10204v1.pdf"
    },
    {
        "title": "Bioinformatics and Medicine in the Era of Deep Learning",
        "abstract": "Many of the current scientific advances in the life sciences have their\norigin in the intensive use of data for knowledge discovery. In no area this is\nso clear as in bioinformatics, led by technological breakthroughs in data\nacquisition technologies. It has been argued that bioinformatics could quickly\nbecome the field of research generating the largest data repositories, beating\nother data-intensive areas such as high-energy physics or astroinformatics.\nOver the last decade, deep learning has become a disruptive advance in machine\nlearning, giving new live to the long-standing connectionist paradigm in\nartificial intelligence. Deep learning methods are ideally suited to\nlarge-scale data and, therefore, they should be ideally suited to knowledge\ndiscovery in bioinformatics and biomedicine at large. In this brief paper, we\nreview key aspects of the application of deep learning in bioinformatics and\nmedicine, drawing from the themes covered by the contributions to an ESANN 2018\nspecial session devoted to this topic.",
        "url": "http://arxiv.org/pdf/1802.09791v1.pdf"
    },
    {
        "title": "Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey",
        "abstract": "Deep learning is at the heart of the current rise of machine learning and\nartificial intelligence. In the field of Computer Vision, it has become the\nworkhorse for applications ranging from self-driving cars to surveillance and\nsecurity. Whereas deep neural networks have demonstrated phenomenal success\n(often beyond human capabilities) in solving complex problems, recent studies\nshow that they are vulnerable to adversarial attacks in the form of subtle\nperturbations to inputs that lead a model to predict incorrect outputs. For\nimages, such perturbations are often too small to be perceptible, yet they\ncompletely fool the deep learning models. Adversarial attacks pose a serious\nthreat to the success of deep learning in practice. This fact has lead to a\nlarge influx of contributions in this direction. This article presents the\nfirst comprehensive survey on adversarial attacks on deep learning in Computer\nVision. We review the works that design adversarial attacks, analyze the\nexistence of such attacks and propose defenses against them. To emphasize that\nadversarial attacks are possible in practical conditions, we separately review\nthe contributions that evaluate adversarial attacks in the real-world\nscenarios. Finally, we draw on the literature to provide a broader outlook of\nthe research direction.",
        "url": "http://arxiv.org/pdf/1801.00553v3.pdf"
    },
    {
        "title": "Generating retinal flow maps from structural optical coherence tomography with artificial intelligence",
        "abstract": "Despite significant advances in artificial intelligence (AI) for computer\nvision, its application in medical imaging has been limited by the burden and\nlimits of expert-generated labels. We used images from optical coherence\ntomography angiography (OCTA), a relatively new imaging modality that measures\nperfusion of the retinal vasculature, to train an AI algorithm to generate\nvasculature maps from standard structural optical coherence tomography (OCT)\nimages of the same retinae, both exceeding the ability and bypassing the need\nfor expert labeling. Deep learning was able to infer perfusion of\nmicrovasculature from structural OCT images with similar fidelity to OCTA and\nsignificantly better than expert clinicians (P < 0.00001). OCTA suffers from\nneed of specialized hardware, laborious acquisition protocols, and motion\nartifacts; whereas our model works directly from standard OCT which are\nubiquitous and quick to obtain, and allows unlocking of large volumes of\npreviously collected standard OCT data both in existing clinical trials and\nclinical practice. This finding demonstrates a novel application of AI to\nmedical imaging, whereby subtle regularities between different modalities are\nused to image the same body part and AI is used to generate detailed and\naccurate inferences of tissue function from structure imaging.",
        "url": "http://arxiv.org/pdf/1802.08925v1.pdf"
    },
    {
        "title": "Introduction to the SP theory of intelligence",
        "abstract": "This article provides a brief introduction to the \"Theory of Intelligence\"\nand its realisation in the \"SP Computer Model\". The overall goal of the SP\nprogramme of research, in accordance with long-established principles in\nscience, has been the simplification and integration of observations and\nconcepts across artificial intelligence, mainstream computing, mathematics, and\nhuman learning, perception, and cognition. In broad terms, the SP system is a\nbrain-like system that takes in \"New\" information through its senses and stores\nsome or all of it as \"Old\" information. A central idea in the system is the\npowerful concept of \"SP-multiple-alignment\", borrowed and adapted from\nbioinformatics. This the key to the system's versatility in aspects of\nintelligence, in the representation of diverse kinds of knowledge, and in the\nseamless integration of diverse aspects of intelligence and diverse kinds of\nknowledge, in any combination. There are many potential benefits and\napplications of the SP system. It is envisaged that the system will be\ndeveloped as the \"SP Machine\", which will initially be a software virtual\nmachine, hosted on a high-performance computer, a vehicle for further research\nand a step towards the development of an industrial-strength SP Machine.",
        "url": "http://arxiv.org/pdf/1802.09924v1.pdf"
    },
    {
        "title": "Pattern-Based Approach to the Workflow Satisfiability Problem with User-Independent Constraints",
        "abstract": "The fixed parameter tractable (FPT) approach is a powerful tool in tackling computationally hard problems. In this paper, we link FPT results to classic artificial intelligence (AI) techniques to show how they complement each other. Specifically, we consider the workflow satisfiability problem (WSP) which asks whether there exists an assignment of authorised users to the steps in a workflow specification, subject to certain constraints on the assignment. It was shown by Cohen et al. (JAIR 2014) that WSP restricted to the class of user-independent constraints (UI), covering many practical cases, admits FPT algorithms, i.e. can be solved in time exponential only in the number of steps $k$ and polynomial in the number of users $n$. Since usually $k << n$ in WSP, such FPT algorithms are of great practical interest. We present a new interpretation of the FPT nature of the WSP with UI constraints giving a decomposition of the problem into two levels. Exploiting this two-level split, we develop a new FPT algorithm that is by many orders of magnitude faster than the previous state-of-the-art WSP algorithm and also has only polynomial-space complexity. We also introduce new pseudo-Boolean (PB) and Constraint Satisfaction (CSP) formulations of the WSP with UI constraints which efficiently exploit this new decomposition of the problem and raise the novel issue of how to use general-purpose solvers to tackle FPT problems in a fashion that meets FPT efficiency expectations. In our computational study, we investigate, for the first time, the phase transition (PT) properties of the WSP, under a model for generation of random instances. We show how PT studies can be extended, in a novel fashion, to support empirical evaluation of scaling of FPT algorithms.",
        "url": "https://arxiv.org/pdf/1604.05636v4.pdf"
    },
    {
        "title": "Artificial Intelligence and Legal Liability",
        "abstract": "A recent issue of a popular computing journal asked which laws would apply if\na self-driving car killed a pedestrian. This paper considers the question of\nlegal liability for artificially intelligent computer systems. It discusses\nwhether criminal liability could ever apply; to whom it might apply; and, under\ncivil law, whether an AI program is a product that is subject to product design\nlegislation or a service to which the tort of negligence applies. The issue of\nsales warranties is also considered. A discussion of some of the practical\nlimitations that AI systems are subject to is also included.",
        "url": "http://arxiv.org/pdf/1802.07782v1.pdf"
    },
    {
        "title": "Algorithmic Collusion in Cournot Duopoly Market: Evidence from Experimental Economics",
        "abstract": "Algorithmic collusion is an emerging concept in current artificial\nintelligence age. Whether algorithmic collusion is a creditable threat remains\nas an argument. In this paper, we propose an algorithm which can extort its\nhuman rival to collude in a Cournot duopoly competing market. In experiments,\nwe show that, the algorithm can successfully extorted its human rival and gets\nhigher profit in long run, meanwhile the human rival will fully collude with\nthe algorithm. As a result, the social welfare declines rapidly and stably.\nBoth in theory and in experiment, our work confirms that, algorithmic collusion\ncan be a creditable threat. In application, we hope, the frameworks, the\nalgorithm design as well as the experiment environment illustrated in this\nwork, can be an incubator or a test bed for researchers and policymakers to\nhandle the emerging algorithmic collusion.",
        "url": "http://arxiv.org/pdf/1802.08061v1.pdf"
    },
    {
        "title": "Predicting Natural Hazards with Neuronal Networks",
        "abstract": "Gravitational mass flows, such as avalanches, debris flows and rockfalls are\ncommon events in alpine regions with high impact on transport routes. Within\nthe last few decades, hazard zone maps have been developed to systematically\napproach this threat. These maps mark vulnerable zones in habitable areas to\nallow effective planning of hazard mitigation measures and development of\nsettlements. Hazard zone maps have shown to be an effective tool to reduce\nfatalities during extreme events. They are created in a complex process, based\non experience, empirical models, physical simulations and historical data. The\ngeneration of such maps is therefore expensive and limited to crucially\nimportant regions, e.g. permanently inhabited areas.\n  In this work we interpret the task of hazard zone mapping as a classification\nproblem. Every point in a specific area has to be classified according to its\nvulnerability. On a regional scale this leads to a segmentation problem, where\nthe total area has to be divided in the respective hazard zones. The recent\ndevelopments in artificial intelligence, namely convolutional neuronal\nnetworks, have led to major improvement in a very similar task, image\nclassification and semantic segmentation, i.e. computer vision. We use a\nconvolutional neuronal network to identify terrain formations with the\npotential for catastrophic snow avalanches and label points in their reach as\nvulnerable. Repeating this procedure for all points allows us to generate an\nartificial hazard zone map. We demonstrate that the approach is feasible and\npromising based on the hazard zone map of the Tirolean Oberland. However, more\ntraining data and further improvement of the method is required before such\ntechniques can be applied reliably.",
        "url": "http://arxiv.org/pdf/1802.07257v1.pdf"
    },
    {
        "title": "Cooperating with Machines",
        "abstract": "Since Alan Turing envisioned Artificial Intelligence (AI) [1], a major\ndriving force behind technical progress has been competition with human\ncognition. Historical milestones have been frequently associated with computers\nmatching or outperforming humans in difficult cognitive tasks (e.g. face\nrecognition [2], personality classification [3], driving cars [4], or playing\nvideo games [5]), or defeating humans in strategic zero-sum encounters (e.g.\nChess [6], Checkers [7], Jeopardy! [8], Poker [9], or Go [10]). In contrast,\nless attention has been given to developing autonomous machines that establish\nmutually cooperative relationships with people who may not share the machine's\npreferences. A main challenge has been that human cooperation does not require\nsheer computational power, but rather relies on intuition [11], cultural norms\n[12], emotions and signals [13, 14, 15, 16], and pre-evolved dispositions\ntoward cooperation [17], common-sense mechanisms that are difficult to encode\nin machines for arbitrary contexts. Here, we combine a state-of-the-art\nmachine-learning algorithm with novel mechanisms for generating and acting on\nsignals to produce a new learning algorithm that cooperates with people and\nother machines at levels that rival human cooperation in a variety of\ntwo-player repeated stochastic games. This is the first general-purpose\nalgorithm that is capable, given a description of a previously unseen game\nenvironment, of learning to cooperate with people within short timescales in\nscenarios previously unanticipated by algorithm designers. This is achieved\nwithout complex opponent modeling or higher-order theories of mind, thus\nshowing that flexible, fast, and general human-machine cooperation is\ncomputationally achievable using a non-trivial, but ultimately simple, set of\nalgorithmic mechanisms.",
        "url": "http://arxiv.org/pdf/1703.06207v5.pdf"
    },
    {
        "title": "The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation",
        "abstract": "This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.",
        "url": "https://arxiv.org/pdf/1802.07228v2.pdf"
    },
    {
        "title": "Combining Textual Content and Structure to Improve Dialog Similarity",
        "abstract": "Chatbots, taking advantage of the success of the messaging apps and recent\nadvances in Artificial Intelligence, have become very popular, from helping\nbusiness to improve customer services to chatting to users for the sake of\nconversation and engagement (celebrity or personal bots). However, developing\nand improving a chatbot requires understanding their data generated by its\nusers. Dialog data has a different nature of a simple question and answering\ninteraction, in which context and temporal properties (turn order) creates a\ndifferent understanding of such data. In this paper, we propose a novelty\nmetric to compute dialogs' similarity based not only on the text content but\nalso on the information related to the dialog structure. Our experimental\nresults performed over the Switchboard dataset show that using evidence from\nboth textual content and the dialog structure leads to more accurate results\nthan using each measure in isolation.",
        "url": "http://arxiv.org/pdf/1802.07117v1.pdf"
    },
    {
        "title": "TAP-DLND 1.0 : A Corpus for Document Level Novelty Detection",
        "abstract": "Detecting novelty of an entire document is an Artificial Intelligence (AI)\nfrontier problem that has widespread NLP applications, such as extractive\ndocument summarization, tracking development of news events, predicting impact\nof scholarly articles, etc. Important though the problem is, we are unaware of\nany benchmark document level data that correctly addresses the evaluation of\nautomatic novelty detection techniques in a classification framework. To bridge\nthis gap, we present here a resource for benchmarking the techniques for\ndocument level novelty detection. We create the resource via event-specific\ncrawling of news documents across several domains in a periodic manner. We\nrelease the annotated corpus with necessary statistics and show its use with a\ndeveloped system for the problem in concern.",
        "url": "http://arxiv.org/pdf/1802.06950v1.pdf"
    },
    {
        "title": "Osteoarthritis Disease Detection System using Self Organizing Maps Method based on Ossa Manus X-Ray",
        "abstract": "Osteoarthritis is a disease found in the world, including in Indonesia. The\npurpose of this study was to detect the disease Osteoarthritis using Self\nOrganizing mapping (SOM), and to know the procedure of artificial intelligence\non the methods of Self Organizing Mapping (SOM). In this system, there are\nseveral stages to preserve to detect disease Osteoarthritis using Self\nOrganizing maps is the result of photographic images rontgen Ossa Manus normal\nand sick with the resolution (150 x 200 pixels) do the repair phase contrast,\nthe Gray scale, thresholding process, Histogram of process , and do the last\nprocess, where the process of doing training (Training) and testing on images\nthat have kept the shape data (.text). the conclusion is the result of testing\nby using a data image, where 42 of data have 12 Normal image data and image\ndata 30 sick. On the results of the process of training data there are 8 X-ray\nimage revealed normal right and 19 data x-ray image of pain expressed is\ncorrect. Then the accuracy on the process of training was 96.42%, and in the\nprocess of testing normal true image 4 obtained revealed Normal, 9 data pain\nstated true pain and 1 data imagery hurts stated incorrectly, then the accuracy\ngained from the results of testing are 92,8%.",
        "url": "http://arxiv.org/pdf/1802.06624v1.pdf"
    },
    {
        "title": "Artificial intelligence and pediatrics: A synthetic mini review",
        "abstract": "The use of artificial intelligence intelligencein medicine can be traced back\nto 1968 when Paycha published his paper Le diagnostic a l'aide d'intelligences\nartificielle, presentation de la premiere machine diagnostri. Few years later\nShortliffe et al. presented an expert system named Mycin which was able to\nidentify bacteria causing severe blood infections and to recommend antibiotics.\nDespite the fact that Mycin outperformed members of the Stanford medical school\nin the reliability of diagnosis it was never used in practice due to a legal\nissue who do you sue if it gives a wrong diagnosis?. However only in 2016 when\nthe artificial intelligence software built into the IBM Watson AI platform\ncorrectly diagnosed and proposed an effective treatment for a 60-year-old\nwomans rare form of leukemia the AI use in medicine become really popular.On of\nfirst papers presenting the use of AI in paediatrics was published in 1984. The\npaper introduced a computer-assisted medical decision making system called\nSHELP.",
        "url": "http://arxiv.org/pdf/1802.06068v1.pdf"
    },
    {
        "title": "Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner",
        "abstract": "During their first years of life, infants learn the language(s) of their\nenvironment at an amazing speed despite large cross cultural variations in\namount and complexity of the available language input. Understanding this\nsimple fact still escapes current cognitive and linguistic theories. Recently,\nspectacular progress in the engineering science, notably, machine learning and\nwearable technology, offer the promise of revolutionizing the study of\ncognitive development. Machine learning offers powerful learning algorithms\nthat can achieve human-like performance on many linguistic tasks. Wearable\nsensors can capture vast amounts of data, which enable the reconstruction of\nthe sensory experience of infants in their natural environment. The project of\n'reverse engineering' language development, i.e., of building an effective\nsystem that mimics infant's achievements appears therefore to be within reach.\nHere, we analyze the conditions under which such a project can contribute to\nour scientific understanding of early language development. We argue that\ninstead of defining a sub-problem or simplifying the data, computational models\nshould address the full complexity of the learning situation, and take as input\nthe raw sensory signals available to infants. This implies that (1) accessible\nbut privacy-preserving repositories of home data be setup and widely shared,\nand (2) models be evaluated at different linguistic levels through a benchmark\nof psycholinguist tests that can be passed by machines and humans alike, (3)\nlinguistically and psychologically plausible learning architectures be scaled\nup to real data using probabilistic/optimization principles from machine\nlearning. We discuss the feasibility of this approach and present preliminary\nresults.",
        "url": "http://arxiv.org/pdf/1607.08723v4.pdf"
    },
    {
        "title": "Morphologic for knowledge dynamics: revision, fusion, abduction",
        "abstract": "Several tasks in artificial intelligence require to be able to find models\nabout knowledge dynamics. They include belief revision, fusion and belief\nmerging, and abduction. In this paper we exploit the algebraic framework of\nmathematical morphology in the context of propositional logic, and define\noperations such as dilation or erosion of a set of formulas. We derive concrete\noperators, based on a semantic approach, that have an intuitive interpretation\nand that are formally well behaved, to perform revision, fusion and abduction.\nComputation and tractability are addressed, and simple examples illustrate the\ntypical results that can be obtained.",
        "url": "http://arxiv.org/pdf/1802.05142v1.pdf"
    },
    {
        "title": "Scaling up the Automatic Statistician: Scalable Structure Discovery using Gaussian Processes",
        "abstract": "Automating statistical modelling is a challenging problem in artificial\nintelligence. The Automatic Statistician takes a first step in this direction,\nby employing a kernel search algorithm with Gaussian Processes (GP) to provide\ninterpretable statistical models for regression problems. However this does not\nscale due to its $O(N^3)$ running time for the model selection. We propose\nScalable Kernel Composition (SKC), a scalable kernel search algorithm that\nextends the Automatic Statistician to bigger data sets. In doing so, we derive\na cheap upper bound on the GP marginal likelihood that sandwiches the marginal\nlikelihood with the variational lower bound . We show that the upper bound is\nsignificantly tighter than the lower bound and thus useful for model selection.",
        "url": "http://arxiv.org/pdf/1706.02524v2.pdf"
    },
    {
        "title": "Disjoint Multi-task Learning between Heterogeneous Human-centric Tasks",
        "abstract": "Human behavior understanding is arguably one of the most important mid-level\ncomponents in artificial intelligence. In order to efficiently make use of\ndata, multi-task learning has been studied in diverse computer vision tasks\nincluding human behavior understanding. However, multi-task learning relies on\ntask specific datasets and constructing such datasets can be cumbersome. It\nrequires huge amounts of data, labeling efforts, statistical consideration etc.\nIn this paper, we leverage existing single-task datasets for human action\nclassification and captioning data for efficient human behavior learning. Since\nthe data in each dataset has respective heterogeneous annotations, traditional\nmulti-task learning is not effective in this scenario. To this end, we propose\na novel alternating directional optimization method to efficiently learn from\nthe heterogeneous data. We demonstrate the effectiveness of our model and show\nperformance improvements on both classification and sentence retrieval tasks in\ncomparison to the models trained on each of the single-task datasets.",
        "url": "http://arxiv.org/pdf/1802.04962v1.pdf"
    },
    {
        "title": "Learning via social awareness: Improving a deep generative sketching model with facial feedback",
        "abstract": "In the quest towards general artificial intelligence (AI), researchers have\nexplored developing loss functions that act as intrinsic motivators in the\nabsence of external rewards. This paper argues that such research has\noverlooked an important and useful intrinsic motivator: social interaction. We\nposit that making an AI agent aware of implicit social feedback from humans can\nallow for faster learning of more generalizable and useful representations, and\ncould potentially impact AI safety. We collect social feedback in the form of\nfacial expression reactions to samples from Sketch RNN, an LSTM-based\nvariational autoencoder (VAE) designed to produce sketch drawings. We use a\nLatent Constraints GAN (LC-GAN) to learn from the facial feedback of a small\ngroup of viewers, by optimizing the model to produce sketches that it predicts\nwill lead to more positive facial expressions. We show in multiple independent\nevaluations that the model trained with facial feedback produced sketches that\nare more highly rated, and induce significantly more positive facial\nexpressions. Thus, we establish that implicit social feedback can improve the\noutput of a deep learning model.",
        "url": "http://arxiv.org/pdf/1802.04877v2.pdf"
    },
    {
        "title": "Learning to Search with MCTSnets",
        "abstract": "Planning problems are among the most important and well-studied problems in\nartificial intelligence. They are most typically solved by tree search\nalgorithms that simulate ahead into the future, evaluate future states, and\nback-up those evaluations to the root of a search tree. Among these algorithms,\nMonte-Carlo tree search (MCTS) is one of the most general, powerful and widely\nused. A typical implementation of MCTS uses cleverly designed rules, optimized\nto the particular characteristics of the domain. These rules control where the\nsimulation traverses, what to evaluate in the states that are reached, and how\nto back-up those evaluations. In this paper we instead learn where, what and\nhow to search. Our architecture, which we call an MCTSnet, incorporates\nsimulation-based search inside a neural network, by expanding, evaluating and\nbacking-up a vector embedding. The parameters of the network are trained\nend-to-end using gradient-based optimisation. When applied to small searches in\nthe well known planning problem Sokoban, the learned search algorithm\nsignificantly outperformed MCTS baselines.",
        "url": "http://arxiv.org/pdf/1802.04697v2.pdf"
    },
    {
        "title": "Barista - a Graphical Tool for Designing and Training Deep Neural Networks",
        "abstract": "In recent years, the importance of deep learning has significantly increased\nin pattern recognition, computer vision, and artificial intelligence research,\nas well as in industry. However, despite the existence of multiple deep\nlearning frameworks, there is a lack of comprehensible and easy-to-use\nhigh-level tools for the design, training, and testing of deep neural networks\n(DNNs). In this paper, we introduce Barista, an open-source graphical\nhigh-level interface for the Caffe deep learning framework. While Caffe is one\nof the most popular frameworks for training DNNs, editing prototext files in\norder to specify the net architecture and hyper parameters can become a\ncumbersome and error-prone task. Instead, Barista offers a fully graphical user\ninterface with a graph-based net topology editor and provides an end-to-end\ntraining facility for DNNs, which allows researchers to focus on solving their\nproblems without having to write code, edit text files, or manually parse\nlogged data.",
        "url": "http://arxiv.org/pdf/1802.04626v1.pdf"
    },
    {
        "title": "Brain Tumor Segmentation Based on Refined Fully Convolutional Neural Networks with A Hierarchical Dice Loss",
        "abstract": "As a basic task in computer vision, semantic segmentation can provide\nfundamental information for object detection and instance segmentation to help\nthe artificial intelligence better understand real world. Since the proposal of\nfully convolutional neural network (FCNN), it has been widely used in semantic\nsegmentation because of its high accuracy of pixel-wise classification as well\nas high precision of localization. In this paper, we apply several famous FCNN\nto brain tumor segmentation, making comparisons and adjusting network\narchitectures to achieve better performance measured by metrics such as\nprecision, recall, mean of intersection of union (mIoU) and dice score\ncoefficient (DSC). The adjustments to the classic FCNN include adding more\nconnections between convolutional layers, enlarging decoders after up sample\nlayers and changing the way shallower layers' information is reused. Besides\nthe structure modification, we also propose a new classifier with a\nhierarchical dice loss. Inspired by the containing relationship between\nclasses, the loss function converts multiple classification to multiple binary\nclassification in order to counteract the negative effect caused by imbalance\ndata set. Massive experiments have been done on the training set and testing\nset in order to assess our refined fully convolutional neural networks and new\ntypes of loss function. Competitive figures prove they are more effective than\ntheir predecessors.",
        "url": "http://arxiv.org/pdf/1712.09093v3.pdf"
    },
    {
        "title": "Blockchain and Artificial Intelligence",
        "abstract": "It is undeniable that artificial intelligence (AI) and blockchain concepts\nare spreading at a phenomenal rate. Both technologies have distinct degree of\ntechnological complexity and multi-dimensional business implications. However,\na common misunderstanding about blockchain concept, in particular, is that\nblockchain is decentralized and is not controlled by anyone. But the underlying\ndevelopment of a blockchain system is still attributed to a cluster of core\ndevelopers. Take smart contract as an example, it is essentially a collection\nof codes (or functions) and data (or states) that are programmed and deployed\non a blockchain (say, Ethereum) by different human programmers. It is thus,\nunfortunately, less likely to be free of loopholes and flaws. In this article,\nthrough a brief overview about how artificial intelligence could be used to\ndeliver bug-free smart contract so as to achieve the goal of blockchain 2.0, we\nto emphasize that the blockchain implementation can be assisted or enhanced via\nvarious AI techniques. The alliance of AI and blockchain is expected to create\nnumerous possibilities.",
        "url": "http://arxiv.org/pdf/1802.04451v2.pdf"
    },
    {
        "title": "Augmenting End-to-End Dialog Systems with Commonsense Knowledge",
        "abstract": "Building dialog agents that can converse naturally with humans is a\nchallenging yet intriguing problem of artificial intelligence. In open-domain\nhuman-computer conversation, where the conversational agent is expected to\nrespond to human responses in an interesting and engaging way, commonsense\nknowledge has to be integrated into the model effectively. In this paper, we\ninvestigate the impact of providing commonsense knowledge about the concepts\ncovered in the dialog. Our model represents the first attempt to integrating a\nlarge commonsense knowledge base into end-to-end conversational models. In the\nretrieval-based scenario, we propose the Tri-LSTM model to jointly take into\naccount message and commonsense for selecting an appropriate response. Our\nexperiments suggest that the knowledge-augmented models are superior to their\nknowledge-free counterparts in automatic evaluation.",
        "url": "http://arxiv.org/pdf/1709.05453v3.pdf"
    },
    {
        "title": "Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog",
        "abstract": "Goal-oriented dialog has been given attention due to its numerous\napplications in artificial intelligence. Goal-oriented dialogue tasks occur\nwhen a questioner asks an action-oriented question and an answerer responds\nwith the intent of letting the questioner know a correct action to take. To ask\nthe adequate question, deep learning and reinforcement learning have been\nrecently applied. However, these approaches struggle to find a competent\nrecurrent neural questioner, owing to the complexity of learning a series of\nsentences. Motivated by theory of mind, we propose \"Answerer in Questioner's\nMind\" (AQM), a novel information theoretic algorithm for goal-oriented dialog.\nWith AQM, a questioner asks and infers based on an approximated probabilistic\nmodel of the answerer. The questioner figures out the answerer's intention via\nselecting a plausible question by explicitly calculating the information gain\nof the candidate intentions and possible answers to each question. We test our\nframework on two goal-oriented visual dialog tasks: \"MNIST Counting Dialog\" and\n\"GuessWhat?!\". In our experiments, AQM outperforms comparative algorithms by a\nlarge margin.",
        "url": "http://arxiv.org/pdf/1802.03881v3.pdf"
    },
    {
        "title": "Sample Efficient Deep Reinforcement Learning for Dialogue Systems with Large Action Spaces",
        "abstract": "In spoken dialogue systems, we aim to deploy artificial intelligence to build\nautomated dialogue agents that can converse with humans. A part of this effort\nis the policy optimisation task, which attempts to find a policy describing how\nto respond to humans, in the form of a function taking the current state of the\ndialogue and returning the response of the system. In this paper, we\ninvestigate deep reinforcement learning approaches to solve this problem.\nParticular attention is given to actor-critic methods, off-policy reinforcement\nlearning with experience replay, and various methods aimed at reducing the bias\nand variance of estimators. When combined, these methods result in the\npreviously proposed ACER algorithm that gave competitive results in gaming\nenvironments. These environments however are fully observable and have a\nrelatively small action set so in this paper we examine the application of ACER\nto dialogue policy optimisation. We show that this method beats the current\nstate-of-the-art in deep learning approaches for spoken dialogue systems. This\nnot only leads to a more sample efficient algorithm that can train faster, but\nalso allows us to apply the algorithm in more difficult environments than\nbefore. We thus experiment with learning in a very large action space, which\nhas two orders of magnitude more actions than previously considered. We find\nthat ACER trains significantly faster than the current state-of-the-art.",
        "url": "http://arxiv.org/pdf/1802.03753v1.pdf"
    },
    {
        "title": "Deep learning in radiology: an overview of the concepts and a survey of the state of the art",
        "abstract": "Deep learning is a branch of artificial intelligence where networks of simple\ninterconnected units are used to extract patterns from data in order to solve\ncomplex problems. Deep learning algorithms have shown groundbreaking\nperformance in a variety of sophisticated tasks, especially those related to\nimages. They have often matched or exceeded human performance. Since the\nmedical field of radiology mostly relies on extracting useful information from\nimages, it is a very natural application area for deep learning, and research\nin this area has rapidly grown in recent years. In this article, we review the\nclinical reality of radiology and discuss the opportunities for application of\ndeep learning algorithms. We also introduce basic concepts of deep learning\nincluding convolutional neural networks. Then, we present a survey of the\nresearch in deep learning applied to radiology. We organize the studies by the\ntypes of specific tasks that they attempt to solve and review the broad range\nof utilized deep learning algorithms. Finally, we briefly discuss opportunities\nand challenges for incorporating deep learning in the radiology practice of the\nfuture.",
        "url": "http://arxiv.org/pdf/1802.08717v1.pdf"
    },
    {
        "title": "Narrow Artificial Intelligence with Machine Learning for Real-Time Estimation of a Mobile Agents Location Using Hidden Markov Models",
        "abstract": "We propose to use a supervised machine learning technique to track the\nlocation of a mobile agent in real time. Hidden Markov Models are used to build\nartificial intelligence that estimates the unknown position of a mobile target\nmoving in a defined environment. This narrow artificial intelligence performs\ntwo distinct tasks. First, it provides real-time estimation of the mobile\nagent's position using the forward algorithm. Second, it uses the Baum-Welch\nalgorithm as a statistical learning tool to gain knowledge of the mobile\ntarget. Finally, an experimental environment is proposed, namely a video game\nthat we use to test our artificial intelligence. We present statistical and\ngraphical results to illustrate the efficiency of our method.",
        "url": "http://arxiv.org/pdf/1802.03417v1.pdf"
    },
    {
        "title": "Autonomous Agents Modelling Other Agents: A Comprehensive Survey and Open Problems",
        "abstract": "Much research in artificial intelligence is concerned with the development of\nautonomous agents that can interact effectively with other agents. An important\naspect of such agents is the ability to reason about the behaviours of other\nagents, by constructing models which make predictions about various properties\nof interest (such as actions, goals, beliefs) of the modelled agents. A variety\nof modelling approaches now exist which vary widely in their methodology and\nunderlying assumptions, catering to the needs of the different sub-communities\nwithin which they were developed and reflecting the different practical uses\nfor which they are intended. The purpose of the present article is to provide a\ncomprehensive survey of the salient modelling methods which can be found in the\nliterature. The article concludes with a discussion of open problems which may\nform the basis for fruitful future research.",
        "url": "http://arxiv.org/pdf/1709.08071v2.pdf"
    },
    {
        "title": "Web-Based Implementation of Travelling Salesperson Problem Using Genetic Algorithm",
        "abstract": "The world is connected through the Internet. As the abundance of Internet\nusers connected into the Web and the popularity of cloud computing research,\nthe need of Artificial Intelligence (AI) is demanding. In this research,\nGenetic Algorithm (GA) as AI optimization method through natural selection and\ngenetic evolution is utilized. There are many applications of GA such as web\nmining, load balancing, routing, and scheduling or web service selection.\nHence, it is a challenging task to discover whether the code mainly server side\nand web based language technology affects the performance of GA. Travelling\nSalesperson Problem (TSP) as Non Polynomial-hard (NP-hard) problem is provided\nto be a problem domain to be solved by GA. While many scientists prefer Python\nin GA implementation, another popular high-level interpreter programming\nlanguage such as PHP (PHP Hypertext Preprocessor) and Ruby were benchmarked.\nLine of codes, file sizes, and performances based on GA implementation and\nruntime were found varies among these programming languages. Based on the\nresult, the use of Ruby in GA implementation is recommended.",
        "url": "http://arxiv.org/pdf/1802.03155v1.pdf"
    },
    {
        "title": "PoTrojan: powerful neural-level trojan designs in deep learning models",
        "abstract": "With the popularity of deep learning (DL), artificial intelligence (AI) has been applied in many areas of human life. Neural network or artificial neural network (NN), the main technique behind DL, has been extensively studied to facilitate computer vision and natural language recognition. However, the more we rely on information technology, the more vulnerable we are. That is, malicious NNs could bring huge threat in the so-called coming AI era. In this paper, for the first time in the literature, we propose a novel approach to design and insert powerful neural-level trojans or PoTrojan in pre-trained NN models. Most of the time, PoTrojans remain inactive, not affecting the normal functions of their host NN models. PoTrojans could only be triggered in very rare conditions. Once activated, however, the PoTrojans could cause the host NN models to malfunction, either falsely predicting or classifying, which is a significant threat to human society of the AI era. We would explain the principles of PoTrojans and the easiness of designing and inserting them in pre-trained deep learning models. PoTrojans doesn't modify the existing architecture or parameters of the pre-trained models, without re-training. Hence, the proposed method is very efficient.",
        "url": "https://arxiv.org/pdf/1802.03043v2.pdf"
    },
    {
        "title": "Active learning machine learns to create new quantum experiments",
        "abstract": "How useful can machine learning be in a quantum laboratory? Here we raise the\nquestion of the potential of intelligent machines in the context of scientific\nresearch. A major motivation for the present work is the unknown reachability\nof various entanglement classes in quantum experiments. We investigate this\nquestion by using the projective simulation model, a physics-oriented approach\nto artificial intelligence. In our approach, the projective simulation system\nis challenged to design complex photonic quantum experiments that produce\nhigh-dimensional entangled multiphoton states, which are of high interest in\nmodern quantum experiments. The artificial intelligence system learns to create\na variety of entangled states, and improves the efficiency of their\nrealization. In the process, the system autonomously (re)discovers experimental\ntechniques which are only now becoming standard in modern quantum optical\nexperiments - a trait which was not explicitly demanded from the system but\nemerged through the process of learning. Such features highlight the\npossibility that machines could have a significantly more creative role in\nfuture research.",
        "url": "http://arxiv.org/pdf/1706.00868v3.pdf"
    },
    {
        "title": "Recent Advances in Neural Program Synthesis",
        "abstract": "In recent years, deep learning has made tremendous progress in a number of\nfields that were previously out of reach for artificial intelligence. The\nsuccesses in these problems has led researchers to consider the possibilities\nfor intelligent systems to tackle a problem that humans have only recently\nthemselves considered: program synthesis. This challenge is unlike others such\nas object recognition and speech translation, since its abstract nature and\ndemand for rigor make it difficult even for human minds to attempt. While it is\nstill far from being solved or even competitive with most existing methods,\nneural program synthesis is a rapidly growing discipline which holds great\npromise if completely realized. In this paper, we start with exploring the\nproblem statement and challenges of program synthesis. Then, we examine the\nfascinating evolution of program induction models, along with how they have\nsucceeded, failed and been reimagined since. Finally, we conclude with a\ncontrastive look at program synthesis and future research recommendations for\nthe field.",
        "url": "http://arxiv.org/pdf/1802.02353v1.pdf"
    },
    {
        "title": "Visual Interpretability for Deep Learning: a Survey",
        "abstract": "This paper reviews recent studies in understanding neural-network\nrepresentations and learning neural networks with interpretable/disentangled\nmiddle-layer representations. Although deep neural networks have exhibited\nsuperior performance in various tasks, the interpretability is always the\nAchilles' heel of deep neural networks. At present, deep neural networks obtain\nhigh discrimination power at the cost of low interpretability of their\nblack-box representations. We believe that high model interpretability may help\npeople to break several bottlenecks of deep learning, e.g., learning from very\nfew annotations, learning via human-computer communications at the semantic\nlevel, and semantically debugging network representations. We focus on\nconvolutional neural networks (CNNs), and we revisit the visualization of CNN\nrepresentations, methods of diagnosing representations of pre-trained CNNs,\napproaches for disentangling pre-trained CNN representations, learning of CNNs\nwith disentangled representations, and middle-to-end learning based on model\ninterpretability. Finally, we discuss prospective trends in explainable\nartificial intelligence.",
        "url": "http://arxiv.org/pdf/1802.00614v2.pdf"
    },
    {
        "title": "Deceptive Games",
        "abstract": "Deceptive games are games where the reward structure or other aspects of the\ngame are designed to lead the agent away from a globally optimal policy. While\nmany games are already deceptive to some extent, we designed a series of games\nin the Video Game Description Language (VGDL) implementing specific types of\ndeception, classified by the cognitive biases they exploit. VGDL games can be\nrun in the General Video Game Artificial Intelligence (GVGAI) Framework, making\nit possible to test a variety of existing AI agents that have been submitted to\nthe GVGAI Competition on these deceptive games. Our results show that all\ntested agents are vulnerable to several kinds of deception, but that different\nagents have different weaknesses. This suggests that we can use deception to\nunderstand the capabilities of a game-playing algorithm, and game-playing\nalgorithms to characterize the deception displayed by a game.",
        "url": "http://arxiv.org/pdf/1802.00048v2.pdf"
    },
    {
        "title": "DeepType: Multilingual Entity Linking by Neural Type System Evolution",
        "abstract": "The wealth of structured (e.g. Wikidata) and unstructured data about the\nworld available today presents an incredible opportunity for tomorrow's\nArtificial Intelligence. So far, integration of these two different modalities\nis a difficult process, involving many decisions concerning how best to\nrepresent the information so that it will be captured or useful, and\nhand-labeling large amounts of data. DeepType overcomes this challenge by\nexplicitly integrating symbolic information into the reasoning process of a\nneural network with a type system. First we construct a type system, and\nsecond, we use it to constrain the outputs of a neural network to respect the\nsymbolic structure. We achieve this by reformulating the design problem into a\nmixed integer problem: create a type system and subsequently train a neural\nnetwork with it. In this reformulation discrete variables select which\nparent-child relations from an ontology are types within the type system, while\ncontinuous variables control a classifier fit to the type system. The original\nproblem cannot be solved exactly, so we propose a 2-step algorithm: 1)\nheuristic search or stochastic optimization over discrete variables that define\na type system informed by an Oracle and a Learnability heuristic, 2) gradient\ndescent to fit classifier parameters. We apply DeepType to the problem of\nEntity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC\nKBP 2010) and find that it outperforms all existing solutions by a wide margin,\nincluding approaches that rely on a human-designed type system or recent deep\nlearning-based entity embeddings, while explicitly using symbolic information\nlets it integrate new entities without retraining.",
        "url": "http://arxiv.org/pdf/1802.01021v1.pdf"
    },
    {
        "title": "Multi-attention Recurrent Network for Human Communication Comprehension",
        "abstract": "Human face-to-face communication is a complex multimodal signal. We use words\n(language modality), gestures (vision modality) and changes in tone (acoustic\nmodality) to convey our intentions. Humans easily process and understand\nface-to-face communication, however, comprehending this form of communication\nremains a significant challenge for Artificial Intelligence (AI). AI must\nunderstand each modality and the interactions between them that shape human\ncommunication. In this paper, we present a novel neural architecture for\nunderstanding human communication called the Multi-attention Recurrent Network\n(MARN). The main strength of our model comes from discovering interactions\nbetween modalities through time using a neural component called the\nMulti-attention Block (MAB) and storing them in the hybrid memory of a\nrecurrent component called the Long-short Term Hybrid Memory (LSTHM). We\nperform extensive comparisons on six publicly available datasets for multimodal\nsentiment analysis, speaker trait recognition and emotion recognition. MARN\nshows state-of-the-art performance on all the datasets.",
        "url": "http://arxiv.org/pdf/1802.00923v1.pdf"
    },
    {
        "title": "Stochastic Primal-Dual Proximal ExtraGradient Descent for Compositely Regularized Optimization",
        "abstract": "We consider a wide range of regularized stochastic minimization problems with\ntwo regularization terms, one of which is composed with a linear function. This\noptimization model abstracts a number of important applications in artificial\nintelligence and machine learning, such as fused Lasso, fused logistic\nregression, and a class of graph-guided regularized minimization. The\ncomputational challenges of this model are in two folds. On one hand, the\nclosed-form solution of the proximal mapping associated with the composed\nregularization term or the expected objective function is not available. On the\nother hand, the calculation of the full gradient of the expectation in the\nobjective is very expensive when the number of input data samples is\nconsiderably large. To address these issues, we propose a stochastic variant of\nextra-gradient type methods, namely \\textsf{Stochastic Primal-Dual Proximal\nExtraGradient descent (SPDPEG)}, and analyze its convergence property for both\nconvex and strongly convex objectives. For general convex objectives, the\nuniformly average iterates generated by \\textsf{SPDPEG} converge in expectation\nwith $O(1/\\sqrt{t})$ rate. While for strongly convex objectives, the uniformly\nand non-uniformly average iterates generated by \\textsf{SPDPEG} converge with\n$O(\\log(t)/t)$ and $O(1/t)$ rates, respectively. The order of the rate of the\nproposed algorithm is known to match the best convergence rate for first-order\nstochastic algorithms. Experiments on fused logistic regression and\ngraph-guided regularized logistic regression problems show that the proposed\nalgorithm performs very efficiently and consistently outperforms other\ncompeting algorithms.",
        "url": "http://arxiv.org/pdf/1708.05978v4.pdf"
    },
    {
        "title": "Machine learning and evolutionary techniques in interplanetary trajectory design",
        "abstract": "After providing a brief historical overview on the synergies between\nartificial intelligence research, in the areas of evolutionary computations and\nmachine learning, and the optimal design of interplanetary trajectories, we\npropose and study the use of deep artificial neural networks to represent,\non-board, the optimal guidance profile of an interplanetary mission. The\nresults, limited to the chosen test case of an Earth-Mars orbital transfer,\nextend the findings made previously for landing scenarios and quadcopter\ndynamics, opening a new research area in interplanetary trajectory planning.",
        "url": "http://arxiv.org/pdf/1802.00180v2.pdf"
    },
    {
        "title": "Modelling contextuality by probabilistic programs with hypergraph semantics",
        "abstract": "Models of a phenomenon are often developed by examining it under different\nexperimental conditions, or measurement contexts. The resultant probabilistic\nmodels assume that the underlying random variables, which define a measurable\nset of outcomes, can be defined independent of the measurement context. The\nphenomenon is deemed contextual when this assumption fails. Contextuality is an\nimportant issue in quantum physics. However, there has been growing speculation\nthat it manifests outside the quantum realm with human cognition being a\nparticularly prominent area of investigation. This article contributes the\nfoundations of a probabilistic programming language that allows convenient\nexploration of contextuality in wide range of applications relevant to\ncognitive science and artificial intelligence. Specific syntax is proposed to\nallow the specification of \"measurement contexts\". Each such context delivers a\npartial model of the phenomenon based on the associated experimental condition\ndescribed by the measurement context. The probabilistic program is translated\ninto a hypergraph in a modular way. Recent theoretical results from the field\nof quantum physics show that contextuality can be equated with the possibility\nof constructing a probabilistic model on the resulting hypergraph. The use of\nhypergraphs opens the door for a theoretically succinct and efficient\ncomputational semantics sensitive to modelling both contextual and\nnon-contextual phenomena. Finally, this article raises awareness of\ncontextuality beyond quantum physics and to contribute formal methods to detect\nits presence by means of hypergraph semantics.",
        "url": "http://arxiv.org/pdf/1802.00690v1.pdf"
    },
    {
        "title": "How linguistic descriptions of data can help to the teaching-learning process in higher education, case of study: artificial intelligence",
        "abstract": "Artificial Intelligence is a central topic in the computer science\ncurriculum. From the year 2011 a project-based learning methodology based on\ncomputer games has been designed and implemented into the intelligence\nartificial course at the University of the Bio-Bio. The project aims to develop\nsoftware-controlled agents (bots) which are programmed by using heuristic\nalgorithms seen during the course. This methodology allows us to obtain good\nlearning results, however several challenges have been founded during its\nimplementation.\n  In this paper we show how linguistic descriptions of data can help to provide\nstudents and teachers with technical and personalized feedback about the\nlearned algorithms. Algorithm behavior profile and a new Turing test for\ncomputer games bots based on linguistic modelling of complex phenomena are also\nproposed in order to deal with such challenges.\n  In order to show and explore the possibilities of this new technology, a web\nplatform has been designed and implemented by one of authors and its\nincorporation in the process of assessment allows us to improve the teaching\nlearning process.",
        "url": "http://arxiv.org/pdf/1711.09744v3.pdf"
    },
    {
        "title": "Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation",
        "abstract": "This paper surveys the current state of the art in Natural Language\nGeneration (NLG), defined as the task of generating text or speech from\nnon-linguistic input. A survey of NLG is timely in view of the changes that the\nfield has undergone over the past decade or so, especially in relation to new\n(usually data-driven) methods, as well as new applications of NLG technology.\nThis survey therefore aims to (a) give an up-to-date synthesis of research on\nthe core tasks in NLG and the architectures adopted in which such tasks are\norganised; (b) highlight a number of relatively recent research topics that\nhave arisen partly as a result of growing synergies between NLG and other areas\nof artificial intelligence; (c) draw attention to the challenges in NLG\nevaluation, relating them to similar challenges faced in other areas of Natural\nLanguage Processing, with an emphasis on different evaluation methods and the\nrelationships between them.",
        "url": "http://arxiv.org/pdf/1703.09902v4.pdf"
    },
    {
        "title": "Deep Reinforcement Learning using Capsules in Advanced Game Environments",
        "abstract": "Reinforcement Learning (RL) is a research area that has blossomed\ntremendously in recent years and has shown remarkable potential for artificial\nintelligence based opponents in computer games. This success is primarily due\nto vast capabilities of Convolutional Neural Networks (ConvNet), enabling\nalgorithms to extract useful information from noisy environments. Capsule\nNetwork (CapsNet) is a recent introduction to the Deep Learning algorithm group\nand has only barely begun to be explored. The network is an architecture for\nimage classification, with superior performance for classification of the MNIST\ndataset. CapsNets have not been explored beyond image classification.\n  This thesis introduces the use of CapsNet for Q-Learning based game\nalgorithms. To successfully apply CapsNet in advanced game play, three main\ncontributions follow. First, the introduction of four new game environments as\nframeworks for RL research with increasing complexity, namely Flash RL, Deep\nLine Wars, Deep RTS, and Deep Maze. These environments fill the gap between\nrelatively simple and more complex game environments available for RL research\nand are in the thesis used to test and explore the CapsNet behavior.\n  Second, the thesis introduces a generative modeling approach to produce\nartificial training data for use in Deep Learning models including CapsNets. We\nempirically show that conditional generative modeling can successfully generate\ngame data of sufficient quality to train a Deep Q-Network well.\n  Third, we show that CapsNet is a reliable architecture for Deep Q-Learning\nbased algorithms for game AI. A capsule is a group of neurons that determine\nthe presence of objects in the data and is in the literature shown to increase\nthe robustness of training and predictions while lowering the amount training\ndata needed. It should, therefore, be ideally suited for game plays.",
        "url": "http://arxiv.org/pdf/1801.09597v1.pdf"
    },
    {
        "title": "Game of Sketches: Deep Recurrent Models of Pictionary-style Word Guessing",
        "abstract": "The ability of intelligent agents to play games in human-like fashion is\npopularly considered a benchmark of progress in Artificial Intelligence.\nSimilarly, performance on multi-disciplinary tasks such as Visual Question\nAnswering (VQA) is considered a marker for gauging progress in Computer Vision.\nIn our work, we bring games and VQA together. Specifically, we introduce the\nfirst computational model aimed at Pictionary, the popular word-guessing social\ngame. We first introduce Sketch-QA, an elementary version of Visual Question\nAnswering task. Styled after Pictionary, Sketch-QA uses incrementally\naccumulated sketch stroke sequences as visual data. Notably, Sketch-QA involves\nasking a fixed question (\"What object is being drawn?\") and gathering\nopen-ended guess-words from human guessers. We analyze the resulting dataset\nand present many interesting findings therein. To mimic Pictionary-style\nguessing, we subsequently propose a deep neural model which generates\nguess-words in response to temporally evolving human-drawn sketches. Our model\neven makes human-like mistakes while guessing, thus amplifying the human\nmimicry factor. We evaluate our model on the large-scale guess-word dataset\ngenerated via Sketch-QA task and compare with various baselines. We also\nconduct a Visual Turing Test to obtain human impressions of the guess-words\ngenerated by humans and our model. Experimental results demonstrate the promise\nof our approach for Pictionary and similarly themed games.",
        "url": "http://arxiv.org/pdf/1801.09356v1.pdf"
    },
    {
        "title": "Symbol Emergence in Cognitive Developmental Systems: a Survey",
        "abstract": "Humans use signs, e.g., sentences in a spoken language, for communication and\nthought. Hence, symbol systems like language are crucial for our communication\nwith other agents and adaptation to our real-world environment. The symbol\nsystems we use in our human society adaptively and dynamically change over\ntime. In the context of artificial intelligence (AI) and cognitive systems, the\nsymbol grounding problem has been regarded as one of the central problems\nrelated to {\\it symbols}. However, the symbol grounding problem was originally\nposed to connect symbolic AI and sensorimotor information and did not consider\nmany interdisciplinary phenomena in human communication and dynamic symbol\nsystems in our society, which semiotics considered. In this paper, we focus on\nthe symbol emergence problem, addressing not only cognitive dynamics but also\nthe dynamics of symbol systems in society, rather than the symbol grounding\nproblem. We first introduce the notion of a symbol in semiotics from the\nhumanities, to leave the very narrow idea of symbols in symbolic AI.\nFurthermore, over the years, it became more and more clear that symbol\nemergence has to be regarded as a multifaceted problem. Therefore, secondly, we\nreview the history of the symbol emergence problem in different fields,\nincluding both biological and artificial systems, showing their mutual\nrelations. We summarize the discussion and provide an integrative viewpoint and\ncomprehensive overview of symbol emergence in cognitive systems. Additionally,\nwe describe the challenges facing the creation of cognitive systems that can be\npart of symbol emergence systems.",
        "url": "http://arxiv.org/pdf/1801.08829v2.pdf"
    },
    {
        "title": "Etymo: A New Discovery Engine for AI Research",
        "abstract": "We present Etymo (https://etymo.io), a discovery engine to facilitate\nartificial intelligence (AI) research and development. It aims to help readers\nnavigate a large number of AI-related papers published every week by using a\nnovel form of search that finds relevant papers and displays related papers in\na graphical interface. Etymo constructs and maintains an adaptive\nsimilarity-based network of research papers as an all-purpose knowledge graph\nfor ranking, recommendation, and visualisation. The network is constantly\nevolving and can learn from user feedback to adjust itself.",
        "url": "http://arxiv.org/pdf/1801.08573v1.pdf"
    },
    {
        "title": "Probabilistic Planning by Probabilistic Programming",
        "abstract": "Automated planning is a major topic of research in artificial intelligence,\nand enjoys a long and distinguished history. The classical paradigm assumes a\ndistinguished initial state, comprised of a set of facts, and is defined over a\nset of actions which change that state in one way or another. Planning in many\nreal-world settings, however, is much more involved: an agent's knowledge is\nalmost never simply a set of facts that are true, and actions that the agent\nintends to execute never operate the way they are supposed to. Thus,\nprobabilistic planning attempts to incorporate stochastic models directly into\nthe planning process. In this article, we briefly report on probabilistic\nplanning through the lens of probabilistic programming: a programming paradigm\nthat aims to ease the specification of structured probability distributions. In\nparticular, we provide an overview of the features of two systems, HYPE and\nALLEGRO, which emphasise different strengths of probabilistic programming that\nare particularly useful for complex modelling issues raised in probabilistic\nplanning. Among other things, with these systems, one can instantiate planning\nproblems with growing and shrinking state spaces, discrete and continuous\nprobability distributions, and non-unique prior distributions in a first-order\nsetting.",
        "url": "http://arxiv.org/pdf/1801.08365v1.pdf"
    },
    {
        "title": "Free Energy Minimization Using the 2-D Cluster Variation Method: Initial Code Verification and Validation",
        "abstract": "A new approach for general artificial intelligence (GAI), building on neural network deep learning architectures, can make use of one or more hidden layers that have the ability to continuously reach a free energy minimum even after input stimulus is removed, allowing for a variety of possible behaviors. One reason that this approach has not been developed until now has been the lack of a suitable free energy equation. The Cluster Variation Method (CVM) offers a means for characterizing 2-D local pattern distributions, or configuration variables, and provides a free energy formalism in terms of these configuration variables. The equilibrium distribution of these configuration variables is defined in terms of a single interaction enthalpy parameter, h, for the case of equiprobable distribution of bistate units. For non-equiprobable distributions, the equilibrium distribution can be characterized by providing a fixed value for the fraction of units in the active state (x1), corresponding to the influence of a per-unit activation enthalpy, together with the pairwise interaction enthalpy parameter h. This paper provides verification and validation (V&V) for code that computes the configuration variable and thermodynamic values for 2-D CVM grids characterized by different interaction enthalpy parameters, or h-values. This work provides a foundation for experimenting with a 2-D CVM-based hidden layer that can, as an alternative to responding strictly to inputs, also now independently come to its own free energy minimum and also return to a free energy-minimized state after perturbations, which will enable a range of input-independent behaviors. A further use of this 2-D CVM grid is that by characterizing local patterns in terms of their corresponding h-values (together with their x1 values), we have a means for quantitatively characterizing different kinds of neural topographies.",
        "url": "https://arxiv.org/pdf/1801.08113v2.pdf"
    },
    {
        "title": "Accelerating Deep Learning with Memcomputing",
        "abstract": "Restricted Boltzmann machines (RBMs) and their extensions, called\n'deep-belief networks', are powerful neural networks that have found\napplications in the fields of machine learning and artificial intelligence. The\nstandard way to training these models resorts to an iterative unsupervised\nprocedure based on Gibbs sampling, called 'contrastive divergence' (CD), and\nadditional supervised tuning via back-propagation. However, this procedure has\nbeen shown not to follow any gradient and can lead to suboptimal solutions. In\nthis paper, we show an efficient alternative to CD by means of simulations of\ndigital memcomputing machines (DMMs). We test our approach on pattern\nrecognition using a modified version of the MNIST data set. DMMs sample\neffectively the vast phase space given by the model distribution of the RBM,\nand provide a very good approximation close to the optimum. This efficient\nsearch significantly reduces the number of pretraining iterations necessary to\nachieve a given level of accuracy, as well as a total performance gain over CD.\nIn fact, the acceleration of pretraining achieved by simulating DMMs is\ncomparable to, in number of iterations, the recently reported hardware\napplication of the quantum annealing method on the same network and data set.\nNotably, however, DMMs perform far better than the reported quantum annealing\nresults in terms of quality of the training. We also compare our method to\nadvances in supervised training, like batch-normalization and rectifiers, that\nwork to reduce the advantage of pretraining. We find that the memcomputing\nmethod still maintains a quality advantage ($>1\\%$ in accuracy, and a $20\\%$\nreduction in error rate) over these approaches. Furthermore, our method is\nagnostic about the connectivity of the network. Therefore, it can be extended\nto train full Boltzmann machines, and even deep networks at once.",
        "url": "http://arxiv.org/pdf/1801.00512v3.pdf"
    },
    {
        "title": "Dynamic Optimization of Neural Network Structures Using Probabilistic Modeling",
        "abstract": "Deep neural networks (DNNs) are powerful machine learning models and have\nsucceeded in various artificial intelligence tasks. Although various\narchitectures and modules for the DNNs have been proposed, selecting and\ndesigning the appropriate network structure for a target problem is a\nchallenging task. In this paper, we propose a method to simultaneously optimize\nthe network structure and weight parameters during neural network training. We\nconsider a probability distribution that generates network structures, and\noptimize the parameters of the distribution instead of directly optimizing the\nnetwork structure. The proposed method can apply to the various network\nstructure optimization problems under the same framework. We apply the proposed\nmethod to several structure optimization problems such as selection of layers,\nselection of unit types, and selection of connections using the MNIST,\nCIFAR-10, and CIFAR-100 datasets. The experimental results show that the\nproposed method can find the appropriate and competitive network structures.",
        "url": "http://arxiv.org/pdf/1801.07650v1.pdf"
    },
    {
        "title": "DeepGestalt - Identifying Rare Genetic Syndromes Using Deep Learning",
        "abstract": "Facial analysis technologies have recently measured up to the capabilities of\nexpert clinicians in syndrome identification. To date, these technologies could\nonly identify phenotypes of a few diseases, limiting their role in clinical\nsettings where hundreds of diagnoses must be considered.\n  We developed a facial analysis framework, DeepGestalt, using computer vision\nand deep learning algorithms, that quantifies similarities to hundreds of\ngenetic syndromes based on unconstrained 2D images. DeepGestalt is currently\ntrained with over 26,000 patient cases from a rapidly growing\nphenotype-genotype database, consisting of tens of thousands of validated\nclinical cases, curated through a community-driven platform. DeepGestalt\ncurrently achieves 91% top-10-accuracy in identifying over 215 different\ngenetic syndromes and has outperformed clinical experts in three separate\nexperiments.\n  We suggest that this form of artificial intelligence is ready to support\nmedical genetics in clinical and laboratory practices and will play a key role\nin the future of precision medicine.",
        "url": "http://arxiv.org/pdf/1801.07637v1.pdf"
    },
    {
        "title": "Automated dataset generation for image recognition using the example of taxonomy",
        "abstract": "This master thesis addresses the subject of automatically generating a\ndataset for image recognition, which takes a lot of time when being done\nmanually. As the thesis was written with motivation from the context of the\nbiodiversity workgroup at the City University of Applied Sciences Bremen, the\nclassification of taxonomic entries was chosen as an exemplary use case. In\norder to automate the dataset creation, a prototype was conceptualized and\nimplemented after working out knowledge basics and analyzing requirements for\nit. It makes use of an pre-trained abstract artificial intelligence which is\nable to sort out images that do not contain the desired content. Subsequent to\nthe implementation and the automated dataset creation resulting from it, an\nevaluation was performed. Other, manually collected datasets were compared to\nthe one the prototype produced in means of specifications and accuracy. The\nresults were more than satisfactory and showed that automatically generating a\ndataset for image recognition is not only possible, but also might be a decent\nalternative to spending time and money in doing this task manually. At the very\nend of this work, an idea of how to use the principle of employing abstract\nartificial intelligences for step-by-step classification of deeper taxonomic\nlayers in a productive system is presented and discussed.",
        "url": "http://arxiv.org/pdf/1802.02207v1.pdf"
    },
    {
        "title": "Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations",
        "abstract": "A long-standing problem at the interface of artificial intelligence and\napplied mathematics is to devise an algorithm capable of achieving human level\nor even superhuman proficiency in transforming observed data into predictive\nmathematical models of the physical world. In the current era of abundance of\ndata and advanced machine learning capabilities, the natural question arises:\nHow can we automatically uncover the underlying laws of physics from\nhigh-dimensional data generated from experiments? In this work, we put forth a\ndeep learning approach for discovering nonlinear partial differential equations\nfrom scattered and potentially noisy observations in space and time.\nSpecifically, we approximate the unknown solution as well as the nonlinear\ndynamics by two deep neural networks. The first network acts as a prior on the\nunknown solution and essentially enables us to avoid numerical differentiations\nwhich are inherently ill-conditioned and unstable. The second network\nrepresents the nonlinear dynamics and helps us distill the mechanisms that\ngovern the evolution of a given spatiotemporal data-set. We test the\neffectiveness of our approach for several benchmark problems spanning a number\nof scientific domains and demonstrate how the proposed framework can help us\naccurately learn the underlying dynamics and forecast future states of the\nsystem. In particular, we study the Burgers', Korteweg-de Vries (KdV),\nKuramoto-Sivashinsky, nonlinear Schr\\\"{o}dinger, and Navier-Stokes equations.",
        "url": "http://arxiv.org/pdf/1801.06637v1.pdf"
    },
    {
        "title": "Innateness, AlphaZero, and Artificial Intelligence",
        "abstract": "The concept of innateness is rarely discussed in the context of artificial\nintelligence. When it is discussed, or hinted at, it is often the context of\ntrying to reduce the amount of innate machinery in a given system. In this\npaper, I consider as a test case a recent series of papers by Silver et al\n(Silver et al., 2017a) on AlphaGo and its successors that have been presented\nas an argument that a \"even in the most challenging of domains: it is possible\nto train to superhuman level, without human examples or guidance\", \"starting\ntabula rasa.\"\n  I argue that these claims are overstated, for multiple reasons. I close by\narguing that artificial intelligence needs greater attention to innateness, and\nI point to some proposals about what that innateness might look like.",
        "url": "http://arxiv.org/pdf/1801.05667v1.pdf"
    },
    {
        "title": "Artificial Intelligence (AI) Methods in Optical Networks: A Comprehensive Survey",
        "abstract": "Artificial intelligence (AI) is an extensive scientific discipline which\nenables computer systems to solve problems by emulating complex biological\nprocesses such as learning, reasoning and self-correction. This paper presents\na comprehensive review of the application of AI techniques for improving\nperformance of optical communication systems and networks. The use of AI-based\ntechniques is first studied in applications related to optical transmission,\nranging from the characterization and operation of network components to\nperformance monitoring, mitigation of nonlinearities, and quality of\ntransmission estimation. Then, applications related to optical network control\nand management are also reviewed, including topics like optical network\nplanning and operation in both transport and access networks. Finally, the\npaper also presents a summary of opportunities and challenges in optical\nnetworking where AI is expected to play a key role in the near future.",
        "url": "http://arxiv.org/pdf/1801.01704v2.pdf"
    },
    {
        "title": "Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification",
        "abstract": "Deep neural networks (DNNs) have transformed several artificial intelligence research areas including computer vision, speech recognition, and natural language processing. However, recent studies demonstrated that DNNs are vulnerable to adversarial manipulations at testing time. Specifically, suppose we have a testing example, whose label can be correctly predicted by a DNN classifier. An attacker can add a small carefully crafted noise to the testing example such that the DNN classifier predicts an incorrect label, where the crafted testing example is called adversarial example. Such attacks are called evasion attacks. Evasion attacks are one of the biggest challenges for deploying DNNs in safety and security critical applications such as self-driving cars. In this work, we develop new methods to defend against evasion attacks. Our key observation is that adversarial examples are close to the classification boundary. Therefore, we propose region-based classification to be robust to adversarial examples. For a benign/adversarial testing example, we ensemble information in a hypercube centered at the example to predict its label. In contrast, traditional classifiers are point-based classification, i.e., given a testing example, the classifier predicts its label based on the testing example alone. Our evaluation results on MNIST and CIFAR-10 datasets demonstrate that our region-based classification can significantly mitigate evasion attacks without sacrificing classification accuracy on benign examples. Specifically, our region-based classification achieves the same classification accuracy on testing benign examples as point-based classification, but our region-based classification is significantly more robust than point-based classification to various evasion attacks.",
        "url": "https://arxiv.org/pdf/1709.05583v4.pdf"
    },
    {
        "title": "Distributed Constraint Optimization Problems and Applications: A Survey",
        "abstract": "The field of Multi-Agent System (MAS) is an active area of research within\nArtificial Intelligence, with an increasingly important impact in industrial\nand other real-world applications. Within a MAS, autonomous agents interact to\npursue personal interests and/or to achieve common objectives. Distributed\nConstraint Optimization Problems (DCOPs) have emerged as one of the prominent\nagent architectures to govern the agents' autonomous behavior, where both\nalgorithms and communication models are driven by the structure of the specific\nproblem. During the last decade, several extensions to the DCOP model have\nenabled them to support MAS in complex, real-time, and uncertain environments.\nThis survey aims at providing an overview of the DCOP model, giving a\nclassification of its multiple extensions and addressing both resolution\nmethods and applications that find a natural mapping within each class of\nDCOPs. The proposed classification suggests several future perspectives for\nDCOP extensions, and identifies challenges in the design of efficient\nresolution algorithms, possibly through the adaptation of strategies from\ndifferent areas.",
        "url": "http://arxiv.org/pdf/1602.06347v4.pdf"
    },
    {
        "title": "Blessing of dimensionality: mathematical foundations of the statistical physics of data",
        "abstract": "The concentration of measure phenomena were discovered as the mathematical\nbackground of statistical mechanics at the end of the XIX - beginning of the XX\ncentury and were then explored in mathematics of the XX-XXI centuries. At the\nbeginning of the XXI century, it became clear that the proper utilisation of\nthese phenomena in machine learning might transform the curse of dimensionality\ninto the blessing of dimensionality.\n  This paper summarises recently discovered phenomena of measure concentration\nwhich drastically simplify some machine learning problems in high dimension,\nand allow us to correct legacy artificial intelligence systems. The classical\nconcentration of measure theorems state that i.i.d. random points are\nconcentrated in a thin layer near a surface (a sphere or equators of a sphere,\nan average or median level set of energy or another Lipschitz function, etc.).\n  The new stochastic separation theorems describe the thin structure of these\nthin layers: the random points are not only concentrated in a thin layer but\nare all linearly separable from the rest of the set, even for exponentially\nlarge random sets. The linear functionals for separation of points can be\nselected in the form of the linear Fisher's discriminant.\n  All artificial intelligence systems make errors. Non-destructive correction\nrequires separation of the situations (samples) with errors from the samples\ncorresponding to correct behaviour by a simple and robust classifier. The\nstochastic separation theorems provide us by such classifiers and a\nnon-iterative (one-shot) procedure for learning.",
        "url": "http://arxiv.org/pdf/1801.03421v1.pdf"
    },
    {
        "title": "EBIC: an evolutionary-based parallel biclustering algorithm for pattern discover",
        "abstract": "In this paper a novel biclustering algorithm based on artificial intelligence\n(AI) is introduced. The method called EBIC aims to detect biologically\nmeaningful, order-preserving patterns in complex data. The proposed algorithm\nis probably the first one capable of discovering with accuracy exceeding 50%\nmultiple complex patterns in real gene expression datasets. It is also one of\nthe very few biclustering methods designed for parallel environments with\nmultiple graphics processing units (GPUs). We demonstrate that EBIC outperforms\nstate-of-the-art biclustering methods, in terms of recovery and relevance, on\nboth synthetic and genetic datasets. EBIC also yields results over 12 times\nfaster than the most accurate reference algorithms. The proposed algorithm is\nanticipated to be added to the repertoire of unsupervised machine learning\nalgorithms for the analysis of datasets, including those from large-scale\ngenomic studies.",
        "url": "http://arxiv.org/pdf/1801.03039v2.pdf"
    },
    {
        "title": "Multilayered Model of Speech",
        "abstract": "Human speech is the most important part of General Artificial Intelligence and subject of much research. The hypothesis proposed in this article provides explanation of difficulties that modern science tackles in the field of human brain simulation. The hypothesis is based on the author's conviction that the brain of any given person has different ability to process and store information. Therefore, the approaches that are currently used to create General Artificial Intelligence have to be altered.",
        "url": "https://arxiv.org/pdf/1801.04170v2.pdf"
    },
    {
        "title": "Trading the Twitter Sentiment with Reinforcement Learning",
        "abstract": "This paper is to explore the possibility to use alternative data and\nartificial intelligence techniques to trade stocks. The efficacy of the daily\nTwitter sentiment on predicting the stock return is examined using machine\nlearning methods. Reinforcement learning(Q-learning) is applied to generate the\noptimal trading policy based on the sentiment signal. The predicting power of\nthe sentiment signal is more significant if the stock price is driven by the\nexpectation of the company growth and when the company has a major event that\ndraws the public attention. The optimal trading strategy based on reinforcement\nlearning outperforms the trading strategy based on the machine learning\nprediction.",
        "url": "http://arxiv.org/pdf/1801.02243v1.pdf"
    },
    {
        "title": "Approximate FPGA-based LSTMs under Computation Time Constraints",
        "abstract": "Recurrent Neural Networks and in particular Long Short-Term Memory (LSTM)\nnetworks have demonstrated state-of-the-art accuracy in several emerging\nArtificial Intelligence tasks. However, the models are becoming increasingly\ndemanding in terms of computational and memory load. Emerging latency-sensitive\napplications including mobile robots and autonomous vehicles often operate\nunder stringent computation time constraints. In this paper, we address the\nchallenge of deploying computationally demanding LSTMs at a constrained time\nbudget by introducing an approximate computing scheme that combines iterative\nlow-rank compression and pruning, along with a novel FPGA-based LSTM\narchitecture. Combined in an end-to-end framework, the approximation method's\nparameters are optimised and the architecture is configured to address the\nproblem of high-performance LSTM execution in time-constrained applications.\nQuantitative evaluation on a real-life image captioning application indicates\nthat the proposed methods required up to 6.5x less time to achieve the same\napplication-level accuracy compared to a baseline method, while achieving an\naverage of 25x higher accuracy under the same computation time constraints.",
        "url": "http://arxiv.org/pdf/1801.02190v1.pdf"
    },
    {
        "title": "Applications of Deep Learning and Reinforcement Learning to Biological Data",
        "abstract": "Rapid advances of hardware-based technologies during the past decades have\nopened up new possibilities for Life scientists to gather multimodal data in\nvarious application domains (e.g., Omics, Bioimaging, Medical Imaging, and\n[Brain/Body]-Machine Interfaces), thus generating novel opportunities for\ndevelopment of dedicated data intensive machine learning techniques. Overall,\nrecent research in Deep learning (DL), Reinforcement learning (RL), and their\ncombination (Deep RL) promise to revolutionize Artificial Intelligence. The\ngrowth in computational power accompanied by faster and increased data storage\nand declining computing costs have already allowed scientists in various fields\nto apply these techniques on datasets that were previously intractable for\ntheir size and complexity. This review article provides a comprehensive survey\non the application of DL, RL, and Deep RL techniques in mining Biological data.\nIn addition, we compare performances of DL techniques when applied to different\ndatasets across various application domains. Finally, we outline open issues in\nthis challenging research area and discuss future development perspectives.",
        "url": "http://arxiv.org/pdf/1711.03985v2.pdf"
    },
    {
        "title": "SenseNet: 3D Objects Database and Tactile Simulator",
        "abstract": "The majority of artificial intelligence research, as it relates from which to\nbiological senses has been focused on vision. The recent explosion of machine\nlearning and in particular, dee p learning, can be partially attributed to the\nrelease of high quality data sets for algorithm s from which to model the world\non. Thus, most of these datasets are comprised of images. We believe that\nfocusing on sensorimotor systems and tactile feedback will create algorithms\nthat better mimic human intelligence. Here we present SenseNet: a collection of\ntactile simulators and a large scale dataset of 3D objects for manipulation.\nSenseNet was created for the purpose of researching and training Artificial\nIntelligences (AIs) to interact with the environment via sensorimotor neural\nsystems and tactile feedback. We aim to accelerate that same explosion in image\nprocessing, but for the domain of tactile feedback and sensorimotor research.\nWe hope that SenseNet can offer researchers in both the machine learning and\ncomputational neuroscience communities brand new opportunities and avenues to\nexplore.",
        "url": "http://arxiv.org/pdf/1801.00361v1.pdf"
    },
    {
        "title": "How will the Internet of Things enable Augmented Personalized Health?",
        "abstract": "Internet-of-Things (IoT) is profoundly redefining the way we create, consume,\nand share information. Health aficionados and citizens are increasingly using\nIoT technologies to track their sleep, food intake, activity, vital body\nsignals, and other physiological observations. This is complemented by IoT\nsystems that continuously collect health-related data from the environment and\ninside the living quarters. Together, these have created an opportunity for a\nnew generation of healthcare solutions. However, interpreting data to\nunderstand an individual's health is challenging. It is usually necessary to\nlook at that individual's clinical record and behavioral information, as well\nas social and environmental information affecting that individual. Interpreting\nhow well a patient is doing also requires looking at his adherence to\nrespective health objectives, application of relevant clinical knowledge and\nthe desired outcomes.\n  We resort to the vision of Augmented Personalized Healthcare (APH) to exploit\nthe extensive variety of relevant data and medical knowledge using Artificial\nIntelligence (AI) techniques to extend and enhance human health to presents\nvarious stages of augmented health management strategies: self-monitoring,\nself-appraisal, self-management, intervention, and disease progress tracking\nand prediction. kHealth technology, a specific incarnation of APH, and its\napplication to Asthma and other diseases are used to provide illustrations and\ndiscuss alternatives for technology-assisted health management. Several\nprominent efforts involving IoT and patient-generated health data (PGHD) with\nrespect converting multimodal data into actionable information (big data to\nsmart data) are also identified. Roles of three components in an evidence-based\nsemantic perception approach- Contextualization, Abstraction, and\nPersonalization are discussed.",
        "url": "http://arxiv.org/pdf/1801.00356v1.pdf"
    },
    {
        "title": "Towards an unanimous international regulatory body for responsible use of Artificial Intelligence [UIRB-AI]",
        "abstract": "Artificial Intelligence (AI), is once again in the phase of drastic\nadvancements. Unarguably, the technology itself can revolutionize the way we\nlive our everyday life. But the exponential growth of technology poses a\ndaunting task for policy researchers and law makers in making amendments to the\nexisting norms. In addition, not everyone in the society is studying the\npotential socio-economic intricacies and cultural drifts that AI can bring\nabout. It is prudence to reflect from our historical past to propel the\ndevelopment of technology in the right direction. To benefit the society of the\npresent and future, I scientifically explore the societal impact of AI. While\nthere are many public and private partnerships working on similar aspects, here\nI describe the necessity for an Unanimous International Regulatory Body for all\napplications of AI (UIRB-AI). I also discuss the benefits and drawbacks of such\nan organization. To combat any drawbacks in the formation of an UIRB-AI, both\nidealistic and pragmatic perspectives are discussed alternatively. The paper\nfurther advances the discussion by proposing novel policies on how such\norganization should be structured and how it can bring about a win-win\nsituation for everyone in the society.",
        "url": "http://arxiv.org/pdf/1712.07752v3.pdf"
    },
    {
        "title": "What do we need to build explainable AI systems for the medical domain?",
        "abstract": "Artificial intelligence (AI) generally and machine learning (ML) specifically\ndemonstrate impressive practical success in many different application domains,\ne.g. in autonomous driving, speech recognition, or recommender systems. Deep\nlearning approaches, trained on extremely large data sets or using\nreinforcement learning methods have even exceeded human performance in visual\ntasks, particularly on playing games such as Atari, or mastering the game of\nGo. Even in the medical domain there are remarkable results. The central\nproblem of such models is that they are regarded as black-box models and even\nif we understand the underlying mathematical principles, they lack an explicit\ndeclarative knowledge representation, hence have difficulty in generating the\nunderlying explanatory structures. This calls for systems enabling to make\ndecisions transparent, understandable and explainable. A huge motivation for\nour approach are rising legal and privacy aspects. The new European General\nData Protection Regulation entering into force on May 25th 2018, will make\nblack-box approaches difficult to use in business. This does not imply a ban on\nautomatic learning approaches or an obligation to explain everything all the\ntime, however, there must be a possibility to make the results re-traceable on\ndemand. In this paper we outline some of our research topics in the context of\nthe relatively new area of explainable-AI with a focus on the application in\nmedicine, which is a very special domain. This is due to the fact that medical\nprofessionals are working mostly with distributed heterogeneous and complex\nsources of data. In this paper we concentrate on three sources: images, *omics\ndata and text. We argue that research in explainable-AI would generally help to\nfacilitate the implementation of AI/ML in the medical domain, and specifically\nhelp to facilitate transparency and trust.",
        "url": "http://arxiv.org/pdf/1712.09923v1.pdf"
    },
    {
        "title": "An empirical evaluation for the intrusion detection features based on machine learning and feature selection methods",
        "abstract": "Despite the great developments in information technology, particularly the\nInternet, computer networks, global information exchange, and its positive\nimpact in all areas of daily life, it has also contributed to the development\nof penetration and intrusion which forms a high risk to the security of\ninformation organizations, government agencies, and causes large economic\nlosses. There are many techniques designed for protection such as firewall and\nintrusion detection systems (IDS). IDS is a set of software and/or hardware\ntechniques used to detect hacker's activities in computer systems. Two types of\nanomalies are used in IDS to detect intrusive activities different from normal\nuser behavior. Misuse relies on the knowledge base that contains all known\nattack techniques and intrusion is discovered through research in this\nknowledge base. Artificial intelligence techniques have been introduced to\nimprove the performance of these systems. The importance of IDS is to identify\nunauthorized access attempting to compromise confidentiality, integrity or\navailability of the computer network. This paper investigates the Intrusion\nDetection (ID) problem using three machine learning algorithms namely, BayesNet\nalgorithm, Multi-Layer Perceptron (MLP), and Support Vector Machine (SVM). The\nalgorithms are applied on a real, Management Information Based (MIB) dataset\nthat is collected from real life environment. To enhance the detection process\naccuracy, a set of feature selection approaches is used; Infogain (IG), ReleifF\n(RF), and Genetic Search (GS). Our experiments show that the three feature\nselection methods have enhanced the classification performance. GS with\nbayesNet, MLP and SVM give high accuracy rates, more specifically the BayesNet\nwith the GS accuracy rate is 99.9%.",
        "url": "http://arxiv.org/pdf/1712.09623v1.pdf"
    },
    {
        "title": "The Power of Arc Consistency for CSPs Defined by Partially-Ordered Forbidden Patterns",
        "abstract": "Characterising tractable fragments of the constraint satisfaction problem\n(CSP) is an important challenge in theoretical computer science and artificial\nintelligence. Forbidding patterns (generic sub-instances) provides a means of\ndefining CSP fragments which are neither exclusively language-based nor\nexclusively structure-based. It is known that the class of binary CSP instances\nin which the broken-triangle pattern (BTP) does not occur, a class which\nincludes all tree-structured instances, are decided by arc consistency (AC), a\nubiquitous reduction operation in constraint solvers. We provide a\ncharacterisation of simple partially-ordered forbidden patterns which have this\nAC-solvability property. It turns out that BTP is just one of five such\nAC-solvable patterns. The four other patterns allow us to exhibit new tractable\nclasses.",
        "url": "http://arxiv.org/pdf/1604.07981v4.pdf"
    },
    {
        "title": "Null Dynamical State Models of Human Cognitive Dysfunction",
        "abstract": "The hard problem in artificial intelligence asks how the shuffling of\nsyntactical symbols in a program can lead to systems which experience semantics\nand qualia. We address this question in three stages. First, we introduce a new\nclass of human semantic symbols which appears when unexpected and drastic\nenvironmental change causes humans to become surprised, confused, uncertain,\nand in extreme cases, unresponsive, passive and dysfunctional. For this class\nof symbols, pre-learned programs become inoperative so these syntactical\nprograms cannot be the source of experienced qualia. Second, we model the\ndysfunctional human response to a radically changed environment as being the\nnatural response of any learning machine facing novel inputs from well outside\nits previous training set. In this situation, learning machines are unable to\nextract information from their input and will typically enter a dynamical state\ncharacterized by null outputs and a lack of response. This state immediately\npredicts and explains the characteristics of the semantic experiences of humans\nin similar circumstances. In the third stage, we consider learning machines\ntrained to implement multiple functions in simple sequential programs using\nenvironmental data to specify subroutine names, control flow instructions,\nmemory calls, and so on. Drastic change in any of these environmental inputs\ncan again lead to inoperative programs. By examining changes specific to people\nor locations we can model human cognitive symbols featuring these dependencies,\nsuch as attachment and grief. Our approach links known dynamical machines\nstates with human qualia and thus offers new insight into the hard problem of\nartificial intelligence.",
        "url": "http://arxiv.org/pdf/1712.09014v1.pdf"
    },
    {
        "title": "Taking Visual Motion Prediction To New Heightfields",
        "abstract": "While the basic laws of Newtonian mechanics are well understood, explaining a physical scenario still requires manually modeling the problem with suitable equations and estimating the associated parameters. In order to be able to leverage the approximation capabilities of artificial intelligence techniques in such physics related contexts, researchers have handcrafted the relevant states, and then used neural networks to learn the state transitions using simulation runs as training data. Unfortunately, such approaches are unsuited for modeling complex real-world scenarios, where manually authoring relevant state spaces tend to be tedious and challenging. In this work, we investigate if neural networks can implicitly learn physical states of real-world mechanical processes only based on visual data while internally modeling non-homogeneous environment and in the process enable long-term physical extrapolation. We develop a recurrent neural network architecture for this task and also characterize resultant uncertainties in the form of evolving variance estimates. We evaluate our setup to extrapolate motion of rolling ball(s) on bowls of varying shape and orientation, and on arbitrary heightfields using only images as input. We report significant improvements over existing image-based methods both in terms of accuracy of predictions and complexity of scenarios; and report competitive performance with approaches that, unlike us, assume access to internal physical states.",
        "url": "https://arxiv.org/pdf/1712.09448v2.pdf"
    },
    {
        "title": "An Ensemble Model with Ranking for Social Dialogue",
        "abstract": "Open-domain social dialogue is one of the long-standing goals of Artificial\nIntelligence. This year, the Amazon Alexa Prize challenge was announced for the\nfirst time, where real customers get to rate systems developed by leading\nuniversities worldwide. The aim of the challenge is to converse \"coherently and\nengagingly with humans on popular topics for 20 minutes\". We describe our Alexa\nPrize system (called 'Alana') consisting of an ensemble of bots, combining\nrule-based and machine learning systems, and using a contextual ranking\nmechanism to choose a system response. The ranker was trained on real user\nfeedback received during the competition, where we address the problem of how\nto train on the noisy and sparse feedback obtained during the competition.",
        "url": "http://arxiv.org/pdf/1712.07558v1.pdf"
    },
    {
        "title": "Partial Labeled Gastric Tumor Segmentation via patch-based Reiterative Learning",
        "abstract": "Gastric cancer is the second leading cause of cancer-related deaths\nworldwide, and the major hurdle in biomedical image analysis is the\ndetermination of the cancer extent. This assignment has high clinical relevance\nand would generally require vast microscopic assessment by pathologists. Recent\nadvances in deep learning have produced inspiring results on biomedical image\nsegmentation, while its outcome is reliant on comprehensive annotation. This\nrequires plenty of labor costs, for the ground truth must be annotated\nmeticulously by pathologists. In this paper, a reiterative learning framework\nwas presented to train our network on partial annotated biomedical images, and\nsuperior performance was achieved without any pre-trained or further manual\nannotation. We eliminate the boundary error of patch-based model through our\noverlapped region forecast algorithm. Through these advisable methods, a mean\nintersection over union coefficient (IOU) of 0.883 and mean accuracy of 91.09%\non the partial labeled dataset was achieved, which made us win the 2017 China\nBig Data & Artificial Intelligence Innovation and Entrepreneurship\nCompetitions.",
        "url": "http://arxiv.org/pdf/1712.07488v1.pdf"
    },
    {
        "title": "Revisiting the Master-Slave Architecture in Multi-Agent Deep Reinforcement Learning",
        "abstract": "Many tasks in artificial intelligence require the collaboration of multiple\nagents. We exam deep reinforcement learning for multi-agent domains. Recent\nresearch efforts often take the form of two seemingly conflicting perspectives,\nthe decentralized perspective, where each agent is supposed to have its own\ncontroller; and the centralized perspective, where one assumes there is a\nlarger model controlling all agents. In this regard, we revisit the idea of the\nmaster-slave architecture by incorporating both perspectives within one\nframework. Such a hierarchical structure naturally leverages advantages from\none another. The idea of combining both perspectives is intuitive and can be\nwell motivated from many real world systems, however, out of a variety of\npossible realizations, we highlights three key ingredients, i.e. composed\naction representation, learnable communication and independent reasoning. With\nnetwork designs to facilitate these explicitly, our proposal consistently\noutperforms latest competing methods both in synthetic experiments and when\napplied to challenging StarCraft micromanagement tasks.",
        "url": "http://arxiv.org/pdf/1712.07305v1.pdf"
    },
    {
        "title": "Cognitive Database: A Step towards Endowing Relational Databases with Artificial Intelligence Capabilities",
        "abstract": "We propose Cognitive Databases, an approach for transparently enabling\nArtificial Intelligence (AI) capabilities in relational databases. A novel\naspect of our design is to first view the structured data source as meaningful\nunstructured text, and then use the text to build an unsupervised neural\nnetwork model using a Natural Language Processing (NLP) technique called word\nembedding. This model captures the hidden inter-/intra-column relationships\nbetween database tokens of different types. For each database token, the model\nincludes a vector that encodes contextual semantic relationships. We seamlessly\nintegrate the word embedding model into existing SQL query infrastructure and\nuse it to enable a new class of SQL-based analytics queries called cognitive\nintelligence (CI) queries. CI queries use the model vectors to enable complex\nqueries such as semantic matching, inductive reasoning queries such as\nanalogies, predictive queries using entities not present in a database, and,\nmore generally, using knowledge from external sources. We demonstrate unique\ncapabilities of Cognitive Databases using an Apache Spark based prototype to\nexecute inductive reasoning CI queries over a multi-modal database containing\ntext and images. We believe our first-of-a-kind system exemplifies using AI\nfunctionality to endow relational databases with capabilities that were\npreviously very hard to realize in practice.",
        "url": "http://arxiv.org/pdf/1712.07199v1.pdf"
    },
    {
        "title": "Scale-invariant temporal history (SITH): optimal slicing of the past in an uncertain world",
        "abstract": "In both the human brain and any general artificial intelligence (AI), a\nrepresentation of the past is necessary to predict the future. However, perfect\nstorage of all experiences is not feasible. One approach utilized in many\napplications, including reward prediction in reinforcement learning, is to\nretain recently active features of experience in a buffer. Despite its prior\nsuccesses, we show that the fixed length buffer renders Deep Q-learning\nNetworks (DQNs) fragile to changes in the scale over which information can be\nlearned. To enable learning when the relevant temporal scales in the\nenvironment are not known *a priori*, recent advances in psychology and\nneuroscience suggest that the brain maintains a compressed representation of\nthe past. Here we introduce a neurally-plausible, scale-free memory\nrepresentation we call Scale-Invariant Temporal History (SITH) for use with\nartificial agents. This representation covers an exponentially large period of\ntime by sacrificing temporal accuracy for events further in the past. We\ndemonstrate the utility of this representation by comparing the performance of\nagents given SITH, buffer, and exponential decay representations in learning to\nplay video games at different levels of complexity. In these environments, SITH\nexhibits better learning performance by storing information for longer\ntimescales than a fixed-size buffer, and representing this information more\nclearly than a set of exponentially decayed features. Finally, we discuss how\nthe application of SITH, along with other human-inspired models of cognition,\ncould improve reinforcement and machine learning algorithms in general.",
        "url": "http://arxiv.org/pdf/1712.07165v3.pdf"
    },
    {
        "title": "Towards the Augmented Pathologist: Challenges of Explainable-AI in Digital Pathology",
        "abstract": "Digital pathology is not only one of the most promising fields of diagnostic\nmedicine, but at the same time a hot topic for fundamental research. Digital\npathology is not just the transfer of histopathological slides into digital\nrepresentations. The combination of different data sources (images, patient\nrecords, and *omics data) together with current advances in artificial\nintelligence/machine learning enable to make novel information accessible and\nquantifiable to a human expert, which is not yet available and not exploited in\ncurrent medical settings. The grand goal is to reach a level of usable\nintelligence to understand the data in the context of an application task,\nthereby making machine decisions transparent, interpretable and explainable.\nThe foundation of such an \"augmented pathologist\" needs an integrated approach:\nWhile machine learning algorithms require many thousands of training examples,\na human expert is often confronted with only a few data points. Interestingly,\nhumans can learn from such few examples and are able to instantly interpret\ncomplex patterns. Consequently, the grand goal is to combine the possibilities\nof artificial intelligence with human intelligence and to find a well-suited\nbalance between them to enable what neither of them could do on their own. This\ncan raise the quality of education, diagnosis, prognosis and prediction of\ncancer and other diseases. In this paper we describe some (incomplete) research\nissues which we believe should be addressed in an integrated and concerted\neffort for paving the way towards the augmented pathologist.",
        "url": "http://arxiv.org/pdf/1712.06657v1.pdf"
    },
    {
        "title": "Deep Learning for Distant Speech Recognition",
        "abstract": "Deep learning is an emerging technology that is considered one of the most\npromising directions for reaching higher levels of artificial intelligence.\nAmong the other achievements, building computers that understand speech\nrepresents a crucial leap towards intelligent machines. Despite the great\nefforts of the past decades, however, a natural and robust human-machine speech\ninteraction still appears to be out of reach, especially when users interact\nwith a distant microphone in noisy and reverberant environments. The latter\ndisturbances severely hamper the intelligibility of a speech signal, making\nDistant Speech Recognition (DSR) one of the major open challenges in the field.\n  This thesis addresses the latter scenario and proposes some novel techniques,\narchitectures, and algorithms to improve the robustness of distant-talking\nacoustic models. We first elaborate on methodologies for realistic data\ncontamination, with a particular emphasis on DNN training with simulated data.\nWe then investigate on approaches for better exploiting speech contexts,\nproposing some original methodologies for both feed-forward and recurrent\nneural networks. Lastly, inspired by the idea that cooperation across different\nDNNs could be the key for counteracting the harmful effects of noise and\nreverberation, we propose a novel deep learning paradigm called network of deep\nneural networks. The analysis of the original concepts were based on extensive\nexperimental validations conducted on both real and simulated data, considering\ndifferent corpora, microphone configurations, environments, noisy conditions,\nand ASR tasks.",
        "url": "http://arxiv.org/pdf/1712.06086v1.pdf"
    },
    {
        "title": "An Artificial Neural Network Architecture Based on Context Transformations in Cortical Minicolumns",
        "abstract": "Cortical minicolumns are considered a model of cortical organization. Their\nfunction is still a source of research and not reflected properly in modern\narchitecture of nets in algorithms of Artificial Intelligence. We assume its\nfunction and describe it in this article. Furthermore, we show how this\nproposal allows to construct a new architecture, that is not based on\nconvolutional neural networks, test it on MNIST data and receive close to\nConvolutional Neural Network accuracy. We also show that the proposed\narchitecture possesses an ability to train on a small quantity of samples. To\nachieve these results, we enable the minicolumns to remember context\ntransformations.",
        "url": "http://arxiv.org/pdf/1712.05954v1.pdf"
    },
    {
        "title": "A Berkeley View of Systems Challenges for AI",
        "abstract": "With the increasing commoditization of computer vision, speech recognition\nand machine translation systems and the widespread deployment of learning-based\nback-end technologies such as digital advertising and intelligent\ninfrastructures, AI (Artificial Intelligence) has moved from research labs to\nproduction. These changes have been made possible by unprecedented levels of\ndata and computation, by methodological advances in machine learning, by\ninnovations in systems software and architectures, and by the broad\naccessibility of these technologies.\n  The next generation of AI systems promises to accelerate these developments\nand increasingly impact our lives via frequent interactions and making (often\nmission-critical) decisions on our behalf, often in highly personalized\ncontexts. Realizing this promise, however, raises daunting challenges. In\nparticular, we need AI systems that make timely and safe decisions in\nunpredictable environments, that are robust against sophisticated adversaries,\nand that can process ever increasing amounts of data across organizations and\nindividuals without compromising confidentiality. These challenges will be\nexacerbated by the end of the Moore's Law, which will constrain the amount of\ndata these technologies can store and process. In this paper, we propose\nseveral open research directions in systems, architectures, and security that\ncan address these challenges and help unlock AI's potential to improve lives\nand society.",
        "url": "http://arxiv.org/pdf/1712.05855v1.pdf"
    },
    {
        "title": "Three IQs of AI Systems and their Testing Methods",
        "abstract": "The rapid development of artificial intelligence has brought the artificial\nintelligence threat theory as well as the problem about how to evaluate the\nintelligence level of intelligent products. Both need to find a quantitative\nmethod to evaluate the intelligence level of intelligence systems, including\nhuman intelligence. Based on the standard intelligence system and the extended\nVon Neumann architecture, this paper proposes General IQ, Service IQ and Value\nIQ evaluation methods for intelligence systems, depending on different\nevaluation purposes. Among them, the General IQ of intelligence systems is to\nanswer the question of whether the artificial intelligence can surpass the\nhuman intelligence, which is reflected in putting the intelligence systems on\nan equal status and conducting the unified evaluation. The Service IQ and Value\nIQ of intelligence systems are used to answer the question of how the\nintelligent products can better serve the human, reflecting the intelligence\nand required cost of each intelligence system as a product in the process of\nserving human.",
        "url": "http://arxiv.org/pdf/1712.06440v1.pdf"
    },
    {
        "title": "A review of Gaussian Markov models for conditional independence",
        "abstract": "Markov models lie at the interface between statistical independence in a probability distribution and graph separation properties. We review model selection and estimation in directed and undirected Markov models with Gaussian parametrization, emphasizing the main similarities and differences. These two model classes are similar but not equivalent, although they share a common intersection. We present the existing results from a historical perspective, taking into account the amount of literature existing from both the artificial intelligence and statistics research communities, where these models were originated. We cover classical topics such as maximum likelihood estimation and model selection via hypothesis testing, but also more modern approaches like regularization and Bayesian methods. We also discuss how the Markov models reviewed fit in the rich hierarchy of other, higher level Markov model classes. Finally, we close the paper overviewing relaxations of the Gaussian assumption and pointing out the main areas of application where these Markov models are nowadays used.",
        "url": "https://arxiv.org/pdf/1606.07282v5.pdf"
    },
    {
        "title": "Generating and Estimating Nonverbal Alphabets for Situated and Multimodal Communications",
        "abstract": "In this paper, we discuss the formalized approach for generating and\nestimating symbols (and alphabets), which can be communicated by the wide range\nof non-verbal means based on specific user requirements (medium, priorities,\ntype of information that needs to be conveyed). The short characterization of\nbasic terms and parameters of such symbols (and alphabets) with approaches to\ngenerate them are given. Then the framework, experimental setup, and some\nmachine learning methods to estimate usefulness and effectiveness of the\nnonverbal alphabets and systems are presented. The previous results demonstrate\nthat usage of multimodal data sources (like wearable accelerometer, heart\nmonitor, muscle movements sensors, braincomputer interface) along with machine\nlearning approaches can provide the deeper understanding of the usefulness and\neffectiveness of such alphabets and systems for nonverbal and situated\ncommunication. The symbols (and alphabets) generated and estimated by such\nmethods may be useful in various applications: from synthetic languages and\nconstructed scripts to multimodal nonverbal and situated interaction between\npeople and artificial intelligence systems through Human-Computer Interfaces,\nsuch as mouse gestures, touchpads, body gestures, eyetracking cameras,\nwearables, and brain-computing interfaces, especially in applications for\nelderly care and people with disabilities.",
        "url": "http://arxiv.org/pdf/1712.04314v1.pdf"
    },
    {
        "title": "Co-attending Free-form Regions and Detections with Multi-modal Multiplicative Feature Embedding for Visual Question Answering",
        "abstract": "Recently, the Visual Question Answering (VQA) task has gained increasing\nattention in artificial intelligence. Existing VQA methods mainly adopt the\nvisual attention mechanism to associate the input question with corresponding\nimage regions for effective question answering. The free-form region based and\nthe detection-based visual attention mechanisms are mostly investigated, with\nthe former ones attending free-form image regions and the latter ones attending\npre-specified detection-box regions. We argue that the two attention mechanisms\nare able to provide complementary information and should be effectively\nintegrated to better solve the VQA problem. In this paper, we propose a novel\ndeep neural network for VQA that integrates both attention mechanisms. Our\nproposed framework effectively fuses features from free-form image regions,\ndetection boxes, and question representations via a multi-modal multiplicative\nfeature embedding scheme to jointly attend question-related free-form image\nregions and detection boxes for more accurate question answering. The proposed\nmethod is extensively evaluated on two publicly available datasets, COCO-QA and\nVQA, and outperforms state-of-the-art approaches. Source code is available at\nhttps://github.com/lupantech/dual-mfa-vqa.",
        "url": "http://arxiv.org/pdf/1711.06794v2.pdf"
    },
    {
        "title": "Continual Learning with Deep Generative Replay",
        "abstract": "Attempts to train a comprehensive artificial intelligence capable of solving\nmultiple tasks have been impeded by a chronic problem called catastrophic\nforgetting. Although simply replaying all previous data alleviates the problem,\nit requires large memory and even worse, often infeasible in real world\napplications where the access to past data is limited. Inspired by the\ngenerative nature of hippocampus as a short-term memory system in primate\nbrain, we propose the Deep Generative Replay, a novel framework with a\ncooperative dual model architecture consisting of a deep generative model\n(\"generator\") and a task solving model (\"solver\"). With only these two models,\ntraining data for previous tasks can easily be sampled and interleaved with\nthose for a new task. We test our methods in several sequential learning\nsettings involving image classification tasks.",
        "url": "http://arxiv.org/pdf/1705.08690v3.pdf"
    },
    {
        "title": "Detecting Qualia in Natural and Artificial Agents",
        "abstract": "The Hard Problem of consciousness has been dismissed as an illusion. By\nshowing that computers are capable of experiencing, we show that they are at\nleast rudimentarily conscious with potential to eventually reach\nsuperconsciousness. The main contribution of the paper is a test for confirming\ncertain subjective experiences in a tested agent. We follow with analysis of\nbenefits and problems with conscious machines and implications of such\ncapability on future of computing, machine rights and artificial intelligence\nsafety.",
        "url": "http://arxiv.org/pdf/1712.04020v1.pdf"
    },
    {
        "title": "A Heuristic Search Algorithm Using the Stability of Learning Algorithms in Certain Scenarios as the Fitness Function: An Artificial General Intelligence Engineering Approach",
        "abstract": "This paper presents a non-manual design engineering method based on heuristic\nsearch algorithm to search for candidate agents in the solution space which\nformed by artificial intelligence agents modeled on the base of\nbionics.Compared with the artificial design method represented by meta-learning\nand the bionics method represented by the neural architecture chip,this method\nis more feasible for realizing artificial general intelligence,and it has a\nmuch better interaction with cognitive neuroscience;at the same time,the\nengineering method is based on the theoretical hypothesis that the final\nlearning algorithm is stable in certain scenarios,and has generalization\nability in various scenarios.The paper discusses the theory preliminarily and\nproposes the possible correlation between the theory and the fixed-point\ntheorem in the field of mathematics.Limited by the author's knowledge\nlevel,this correlation is proposed only as a kind of conjecture.",
        "url": "http://arxiv.org/pdf/1712.03043v3.pdf"
    },
    {
        "title": "Cogniculture: Towards a Better Human-Machine Co-evolution",
        "abstract": "Research in Artificial Intelligence is breaking technology barriers every\nday. New algorithms and high performance computing are making things possible\nwhich we could only have imagined earlier. Though the enhancements in AI are\nmaking life easier for human beings day by day, there is constant fear that AI\nbased systems will pose a threat to humanity. People in AI community have\ndiverse set of opinions regarding the pros and cons of AI mimicking human\nbehavior. Instead of worrying about AI advancements, we propose a novel idea of\ncognitive agents, including both human and machines, living together in a\ncomplex adaptive ecosystem, collaborating on human computation for producing\nessential social goods while promoting sustenance, survival and evolution of\nthe agents' life cycle. We highlight several research challenges and technology\nbarriers in achieving this goal. We propose a governance mechanism around this\necosystem to ensure ethical behaviors of all cognitive agents. Along with a\nnovel set of use-cases of Cogniculture, we discuss the road map ahead for this\njourney.",
        "url": "http://arxiv.org/pdf/1712.03724v1.pdf"
    },
    {
        "title": "Artificial Intelligence and Statistics",
        "abstract": "Artificial intelligence (AI) is intrinsically data-driven. It calls for the\napplication of statistical concepts through human-machine collaboration during\ngeneration of data, development of algorithms, and evaluation of results. This\npaper discusses how such human-machine collaboration can be approached through\nthe statistical concepts of population, question of interest,\nrepresentativeness of training data, and scrutiny of results (PQRS). The PQRS\nworkflow provides a conceptual framework for integrating statistical ideas with\nhuman input into AI products and research. These ideas include experimental\ndesign principles of randomization and local control as well as the principle\nof stability to gain reproducibility and interpretability of algorithms and\ndata results. We discuss the use of these principles in the contexts of\nself-driving cars, automated medical diagnoses, and examples from the authors'\ncollaborative research.",
        "url": "http://arxiv.org/pdf/1712.03779v1.pdf"
    },
    {
        "title": "Network Analysis for Explanation",
        "abstract": "Safety critical systems strongly require the quality aspects of artificial\nintelligence including explainability. In this paper, we analyzed a trained\nnetwork to extract features which mainly contribute the inference. Based on the\nanalysis, we developed a simple solution to generate explanations of the\ninference processes.",
        "url": "http://arxiv.org/pdf/1712.02890v1.pdf"
    },
    {
        "title": "Columnar Database Techniques for Creating AI Features",
        "abstract": "Recent advances with in-memory columnar database techniques have increased\nthe performance of analytical queries on very large databases and data\nwarehouses. At the same time, advances in artificial intelligence (AI)\nalgorithms have increased the ability to analyze data. We use the term AI to\nencompass both Deep Learning (DL or neural network) and Machine Learning (ML\naka Big Data analytics). Our exploration of the AI full stack has led us to a\ncross-stack columnar database innovation that efficiently creates features for\nAI analytics. The innovation is to create Augmented Dictionary Values (ADVs) to\nadd to existing columnar database dictionaries in order to increase the\nefficiency of featurization by minimizing data movement and data duplication.\nWe show how various forms of featurization (feature selection, feature\nextraction, and feature creation) can be efficiently calculated in a columnar\ndatabase. The full stack AI investigation has also led us to propose an\nintegrated columnar database and AI architecture. This architecture has\ninformation flows and feedback loops to improve the whole analytics cycle\nduring multiple iterations of extracting data from the data sources,\nfeaturization, and analysis.",
        "url": "http://arxiv.org/pdf/1712.02882v1.pdf"
    },
    {
        "title": "Symbol Grounding Association in Multimodal Sequences with Missing Elements",
        "abstract": "In this paper, we extend a symbolic association framework for being able to\nhandle missing elements in multimodal sequences. The general scope of the work\nis the symbolic associations of object-word mappings as it happens in language\ndevelopment in infants. In other words, two different representations of the\nsame abstract concepts can associate in both directions. This scenario has been\nlong interested in Artificial Intelligence, Psychology, and Neuroscience. In\nthis work, we extend a recent approach for multimodal sequences (visual and\naudio) to also cope with missing elements in one or both modalities. Our method\nuses two parallel Long Short-Term Memories (LSTMs) with a learning rule based\non EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We\npropose to include an extra step for the combination with the max operation for\nexploiting the common elements between both sequences. The motivation behind is\nthat the combination acts as a condition selector for choosing the best\nrepresentation from both LSTMs. We evaluated the proposed extension in the\nfollowing scenarios: missing elements in one modality (visual or audio) and\nmissing elements in both modalities (visual and sound). The performance of our\nextension reaches better results than the original model and similar results to\nindividual LSTM trained in each modality.",
        "url": "http://arxiv.org/pdf/1511.04401v5.pdf"
    },
    {
        "title": "Learning General Latent-Variable Graphical Models with Predictive Belief Propagation",
        "abstract": "Learning general latent-variable probabilistic graphical models is a key theoretical challenge in machine learning and artificial intelligence. All previous methods, including the EM algorithm and the spectral algorithms, face severe limitations that largely restrict their applicability and affect their performance. In order to overcome these limitations, in this paper we introduce a novel formulation of message-passing inference over junction trees named predictive belief propagation, and propose a new learning and inference algorithm for general latent-variable graphical models based on this formulation. Our proposed algorithm reduces the hard parameter learning problem into a sequence of supervised learning problems, and unifies the learning of different kinds of latent graphical models into a single learning framework, which is local-optima-free and statistically consistent. We then give a proof of the correctness of our algorithm and show in experiments on both synthetic and real datasets that our algorithm significantly outperforms both the EM algorithm and the spectral algorithm while also being orders of magnitude faster to compute.",
        "url": "https://arxiv.org/pdf/1712.02046v2.pdf"
    },
    {
        "title": "Deep Anticipation: Light Weight Intelligent Mobile Sensing in IoT by Recurrent Architecture",
        "abstract": "The rapid growth of IoT era is shaping the future of mobile services.\nAdvanced communication technology enables a heterogeneous connectivity where\nmobile devices broadcast information to everything. Mobile applications such as\nrobotics and vehicles connecting to cloud and surroundings transfer the\nshort-range on-board sensor perception system to long-range mobile-sensing\nperception system. However, the mobile sensing perception brings new challenges\nfor how to efficiently analyze and intelligently interpret the deluge of IoT\ndata in mission- critical services. In this article, we model the challenges as\nlatency, packet loss and measurement noise which severely deteriorate the\nreliability and quality of IoT data. We integrate the artificial intelligence\ninto IoT to tackle these challenges. We propose a novel architecture that\nleverages recurrent neural networks (RNN) and Kalman filtering to anticipate\nmotions and interac- tions between objects. The basic idea is to learn\nenvironment dynamics by recurrent networks. To improve the robustness of IoT\ncommunication, we use the idea of Kalman filtering and deploy a prediction and\ncorrection step. In this way, the architecture learns to develop a biased\nbelief between prediction and measurement in the different situation. We\ndemonstrate our approach with synthetic and real-world datasets with noise that\nmimics the challenges of IoT communications. Our method brings a new level of\nIoT intelligence. It is also lightweight compared to other state-of-the-art\nconvolutional recurrent architecture and is ideally suitable for the\nresource-limited mobile applications.",
        "url": "http://arxiv.org/pdf/1801.01444v2.pdf"
    },
    {
        "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
        "abstract": "The game of chess is the most widely-studied domain in the history of\nartificial intelligence. The strongest programs are based on a combination of\nsophisticated search techniques, domain-specific adaptations, and handcrafted\nevaluation functions that have been refined by human experts over several\ndecades. In contrast, the AlphaGo Zero program recently achieved superhuman\nperformance in the game of Go, by tabula rasa reinforcement learning from games\nof self-play. In this paper, we generalise this approach into a single\nAlphaZero algorithm that can achieve, tabula rasa, superhuman performance in\nmany challenging domains. Starting from random play, and given no domain\nknowledge except the game rules, AlphaZero achieved within 24 hours a\nsuperhuman level of play in the games of chess and shogi (Japanese chess) as\nwell as Go, and convincingly defeated a world-champion program in each case.",
        "url": "http://arxiv.org/pdf/1712.01815v1.pdf"
    },
    {
        "title": "Autonomous development and learning in artificial intelligence and robotics: Scaling up deep learning to human--like learning",
        "abstract": "Autonomous lifelong development and learning is a fundamental capability of\nhumans, differentiating them from current deep learning systems. However, other\nbranches of artificial intelligence have designed crucial ingredients towards\nautonomous learning: curiosity and intrinsic motivation, social learning and\nnatural interaction with peers, and embodiment. These mechanisms guide\nexploration and autonomous choice of goals, and integrating them with deep\nlearning opens stimulating perspectives. Deep learning (DL) approaches made\ngreat advances in artificial intelligence, but are still far away from human\nlearning. As argued convincingly by Lake et al., differences include human\ncapabilities to learn causal models of the world from very little data,\nleveraging compositional representations and priors like intuitive physics and\npsychology. However, there are other fundamental differences between current DL\nsystems and human learning, as well as technical ingredients to fill this gap,\nthat are either superficially, or not adequately, discussed by Lake et al.\nThese fundamental mechanisms relate to autonomous development and learning.\nThey are bound to play a central role in artificial intelligence in the future.\nCurrent DL systems require engineers to manually specify a task-specific\nobjective function for every new task, and learn through off-line processing of\nlarge training databases. On the contrary, humans learn autonomously open-ended\nrepertoires of skills, deciding for themselves which goals to pursue or value,\nand which skills to explore, driven by intrinsic motivation/curiosity and\nsocial learning through natural interaction with peers. Such learning processes\nare incremental, online, and progressive. Human child development involves a\nprogressive increase of complexity in a curriculum of learning where skills are\nexplored, acquired, and built on each other, through particular ordering and\ntiming. Finally, human learning happens in the physical world, and through\nbodily and physical experimentation, under severe constraints on energy, time,\nand computational resources. In the two last decades, the field of\nDevelopmental and Cognitive Robotics (Cangelosi and Schlesinger, 2015, Asada et\nal., 2009), in strong interaction with developmental psychology and\nneuroscience, has achieved significant advances in computational",
        "url": "http://arxiv.org/pdf/1712.01626v1.pdf"
    },
    {
        "title": "Fast Top-k Area Topics Extraction with Knowledge Base",
        "abstract": "What are the most popular research topics in Artificial Intelligence (AI)? We\nformulate the problem as extracting top-$k$ topics that can best represent a\ngiven area with the help of knowledge base. We theoretically prove that the\nproblem is NP-hard and propose an optimization model, FastKATE, to address this\nproblem by combining both explicit and latent representations for each topic.\nWe leverage a large-scale knowledge base (Wikipedia) to generate topic\nembeddings using neural networks and use this kind of representations to help\ncapture the representativeness of topics for given areas. We develop a fast\nheuristic algorithm to efficiently solve the problem with a provable error\nbound. We evaluate the proposed model on three real-world datasets.\nExperimental results demonstrate our model's effectiveness, robustness,\nreal-timeness (return results in $<1$s), and its superiority over several\nalternative methods.",
        "url": "http://arxiv.org/pdf/1710.04822v2.pdf"
    },
    {
        "title": "In folly ripe. In reason rotten. Putting machine theology to rest",
        "abstract": "Computation has changed the world more than any previous expressions of\nknowledge. In its particular algorithmic embodiment, it offers a perspective,\nwithin which the digital computer (one of many possible) exercises a role\nreminiscent of theology. Since it is closed to meaning, algorithmic digital\ncomputation can at most mimic the creative aspects of life. AI, in the\nperspective of time, proved to be less an acronym for artificial intelligence\nand more of automating tasks associated with intelligence. The entire\ndevelopment led to the hypostatized role of the machine: outputting nothing\nelse but reality, including that of the humanity that made the machine happen.\nThe convergence machine called deep learning is only the latest form through\nwhich the deterministic theology of the machine claims more than what extremely\neffective data processing actually is. A new understanding of complexity, as\nwell as the need to distinguish between the reactive nature of the artificial\nand the anticipatory nature of the living are suggested as practical responses\nto the challenges posed by machine theology.",
        "url": "http://arxiv.org/pdf/1712.04306v1.pdf"
    },
    {
        "title": "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering",
        "abstract": "Vision and language understanding has emerged as a subject undergoing intense\nstudy in Artificial Intelligence. Among many tasks in this line of research,\nvisual question answering (VQA) has been one of the most successful ones, where\nthe goal is to learn a model that understands visual content at region-level\ndetails and finds their associations with pairs of questions and answers in the\nnatural language form. Despite the rapid progress in the past few years, most\nexisting work in VQA have focused primarily on images. In this paper, we focus\non extending VQA to the video domain and contribute to the literature in three\nimportant ways. First, we propose three new tasks designed specifically for\nvideo VQA, which require spatio-temporal reasoning from videos to answer\nquestions correctly. Next, we introduce a new large-scale dataset for video VQA\nnamed TGIF-QA that extends existing VQA work with our new tasks. Finally, we\npropose a dual-LSTM based approach with both spatial and temporal attention,\nand show its effectiveness over conventional VQA techniques through empirical\nevaluations.",
        "url": "http://arxiv.org/pdf/1704.04497v3.pdf"
    },
    {
        "title": "Artificial intelligence in peer review: How can evolutionary computation support journal editors?",
        "abstract": "With the volume of manuscripts submitted for publication growing every year,\nthe deficiencies of peer review (e.g. long review times) are becoming more\napparent. Editorial strategies, sets of guidelines designed to speed up the\nprocess and reduce editors workloads, are treated as trade secrets by\npublishing houses and are not shared publicly. To improve the effectiveness of\ntheir strategies, editors in small publishing groups are faced with undertaking\nan iterative trial-and-error approach. We show that Cartesian Genetic\nProgramming, a nature-inspired evolutionary algorithm, can dramatically improve\neditorial strategies. The artificially evolved strategy reduced the duration of\nthe peer review process by 30%, without increasing the pool of reviewers (in\ncomparison to a typical human-developed strategy). Evolutionary computation has\ntypically been used in technological processes or biological ecosystems. Our\nresults demonstrate that genetic programs can improve real-world social systems\nthat are usually much harder to understand and control than physical systems.",
        "url": "http://arxiv.org/pdf/1712.01682v1.pdf"
    },
    {
        "title": "Improvised Comedy as a Turing Test",
        "abstract": "The best improvisational theatre actors can make any scene partner, of any\nskill level or ability, appear talented and proficient in the art form, and\nthus \"make them shine\". To challenge this improvisational paradigm, we built an\nartificial intelligence (AI) trained to perform live shows alongside human\nactors for human audiences. Over the course of 30 performances to a combined\naudience of almost 3000 people, we have refined theatrical games which involve\ncombinations of human and (at times, adversarial) AI actors. We have developed\nspecific scene structures to include audience participants in interesting ways.\nFinally, we developed a complete show structure that submitted the audience to\na Turing test and observed their suspension of disbelief, which we believe is\nkey for human/non-human theatre co-creation.",
        "url": "http://arxiv.org/pdf/1711.08819v2.pdf"
    },
    {
        "title": "The mind as a computational system",
        "abstract": "The present document is an excerpt of an essay that I wrote as part of my\napplication material to graduate school in Computer Science (with a focus on\nArtificial Intelligence), in 1986. I was not invited by any of the schools that\nreceived it, so I became a theoretical physicist instead. The essay's full\ntitle was \"Some Topics in Philosophy and Computer Science\". I am making this\ntext (unchanged from 1985, preserving the typesetting as much as possible)\navailable now in memory of Jerry Fodor, whose writings had influenced me\nsignificantly at the time (even though I did not always agree).",
        "url": "http://arxiv.org/pdf/1712.01093v1.pdf"
    },
    {
        "title": "Pulsar Candidate Identification with Artificial Intelligence Techniques",
        "abstract": "Discovering pulsars is a significant and meaningful research topic in the field of radio astronomy. With the advent of astronomical instruments such as he Five-hundred-meter Aperture Spherical Telescope (FAST) in China, data volumes and data rates are exponentially growing. This fact necessitates a focus on artificial intelligence (AI) technologies that can perform the automatic pulsar candidate identification to mine large astronomical data sets. Automatic pulsar candidate identification can be considered as a task of determining potential candidates for further investigation and eliminating noises of radio frequency interferences or other non-pulsar signals. It is very hard to raise the performance of DCNN-based pulsar identification because the limited training samples restrict network structure to be designed deep enough for learning good features as well as the crucial class imbalance problem due to very limited number of real pulsar samples. To address these problems, we proposed a framework which combines deep convolution generative adversarial network (DCGAN) with support vector machine (SVM) to deal with imbalance class problem and to improve pulsar identification accuracy. DCGAN is used as sample generation and feature learning model, and SVM is adopted as the classifier for predicting candidate's labels in the inference stage. The proposed framework is a novel technique which not only can solve imbalance class problem but also can learn discriminative feature representations of pulsar candidates instead of computing hand-crafted features in preprocessing steps too, which makes it more accurate for automatic pulsar candidate selection. Experiments on two pulsar datasets verify the effectiveness and efficiency of our proposed method.",
        "url": "https://arxiv.org/pdf/1711.10339v2.pdf"
    },
    {
        "title": "Quantum Artificial Life in an IBM Quantum Computer",
        "abstract": "We present the first experimental realization of a quantum artificial life\nalgorithm in a quantum computer. The quantum biomimetic protocol encodes\ntailored quantum behaviors belonging to living systems, namely,\nself-replication, mutation, interaction between individuals, and death, into\nthe cloud quantum computer IBM ibmqx4. In this experiment, entanglement spreads\nthroughout generations of individuals, where genuine quantum information\nfeatures are inherited through genealogical networks. As a pioneering\nproof-of-principle, experimental data fits the ideal model with accuracy.\nThereafter, these and other models of quantum artificial life, for which no\nclassical device may predict its quantum supremacy evolution, can be further\nexplored in novel generations of quantum computers. Quantum biomimetics,\nquantum machine learning, and quantum artificial intelligence will move forward\nhand in hand through more elaborate levels of quantum complexity.",
        "url": "http://arxiv.org/pdf/1711.09442v2.pdf"
    },
    {
        "title": "A Big Data Analysis Framework Using Apache Spark and Deep Learning",
        "abstract": "With the spreading prevalence of Big Data, many advances have recently been\nmade in this field. Frameworks such as Apache Hadoop and Apache Spark have\ngained a lot of traction over the past decades and have become massively\npopular, especially in industries. It is becoming increasingly evident that\neffective big data analysis is key to solving artificial intelligence problems.\nThus, a multi-algorithm library was implemented in the Spark framework, called\nMLlib. While this library supports multiple machine learning algorithms, there\nis still scope to use the Spark setup efficiently for highly time-intensive and\ncomputationally expensive procedures like deep learning. In this paper, we\npropose a novel framework that combines the distributive computational\nabilities of Apache Spark and the advanced machine learning architecture of a\ndeep multi-layer perceptron (MLP), using the popular concept of Cascade\nLearning. We conduct empirical analysis of our framework on two real world\ndatasets. The results are encouraging and corroborate our proposed framework,\nin turn proving that it is an improvement over traditional big data analysis\nmethods that use either Spark or Deep learning as individual elements.",
        "url": "http://arxiv.org/pdf/1711.09279v1.pdf"
    },
    {
        "title": "Cooperative Multi-Agent Planning: A Survey",
        "abstract": "Cooperative multi-agent planning (MAP) is a relatively recent research field\nthat combines technologies, algorithms and techniques developed by the\nArtificial Intelligence Planning and Multi-Agent Systems communities. While\nplanning has been generally treated as a single-agent task, MAP generalizes\nthis concept by considering multiple intelligent agents that work cooperatively\nto develop a course of action that satisfies the goals of the group.\n  This paper reviews the most relevant approaches to MAP, putting the focus on\nthe solvers that took part in the 2015 Competition of Distributed and\nMulti-Agent Planning, and classifies them according to their key features and\nrelative performance.",
        "url": "http://arxiv.org/pdf/1711.09057v1.pdf"
    },
    {
        "title": "Interactive Robot Learning of Gestures, Language and Affordances",
        "abstract": "A growing field in robotics and Artificial Intelligence (AI) research is\nhuman-robot collaboration, whose target is to enable effective teamwork between\nhumans and robots. However, in many situations human teams are still superior\nto human-robot teams, primarily because human teams can easily agree on a\ncommon goal with language, and the individual members observe each other\neffectively, leveraging their shared motor repertoire and sensorimotor\nresources. This paper shows that for cognitive robots it is possible, and\nindeed fruitful, to combine knowledge acquired from interacting with elements\nof the environment (affordance exploration) with the probabilistic observation\nof another agent's actions.\n  We propose a model that unites (i) learning robot affordances and word\ndescriptions with (ii) statistical recognition of human gestures with vision\nsensors. We discuss theoretical motivations, possible implementations, and we\nshow initial results which highlight that, after having acquired knowledge of\nits surrounding environment, a humanoid robot can generalize this knowledge to\nthe case when it observes another agent (human partner) performing the same\nmotor actions previously executed during training.",
        "url": "http://arxiv.org/pdf/1711.09055v1.pdf"
    },
    {
        "title": "fpgaConvNet: A Toolflow for Mapping Diverse Convolutional Neural Networks on Embedded FPGAs",
        "abstract": "In recent years, Convolutional Neural Networks (ConvNets) have become an\nenabling technology for a wide range of novel embedded Artificial Intelligence\nsystems. Across the range of applications, the performance needs vary\nsignificantly, from high-throughput video surveillance to the very low-latency\nrequirements of autonomous cars. In this context, FPGAs can provide a potential\nplatform that can be optimally configured based on the different performance\nneeds. However, the complexity of ConvNet models keeps increasing making their\nmapping to an FPGA device a challenging task. This work presents fpgaConvNet,\nan end-to-end framework for mapping ConvNets on FPGAs. The proposed framework\nemploys an automated design methodology based on the Synchronous Dataflow (SDF)\nparadigm and defines a set of SDF transformations in order to efficiently\nexplore the architectural design space. By selectively optimising for\nthroughput, latency or multiobjective criteria, the presented tool is able to\nefficiently explore the design space and generate hardware designs from\nhigh-level ConvNet specifications, explicitly optimised for the performance\nmetric of interest. Overall, our framework yields designs that improve the\nperformance by up to 6.65x over highly optimised embedded GPU designs for the\nsame power constraints in embedded environments.",
        "url": "http://arxiv.org/pdf/1711.08740v1.pdf"
    },
    {
        "title": "Accountability of AI Under the Law: The Role of Explanation",
        "abstract": "The ubiquity of systems using artificial intelligence or \"AI\" has brought increasing attention to how those systems should be regulated. The choice of how to regulate AI systems will require care. AI systems have the potential to synthesize large amounts of data, allowing for greater levels of personalization and precision than ever before---applications range from clinical decision support to autonomous driving and predictive policing. That said, there exist legitimate concerns about the intentional and unintentional negative consequences of AI systems. There are many ways to hold AI systems accountable. In this work, we focus on one: explanation. Questions about a legal right to explanation from AI systems was recently debated in the EU General Data Protection Regulation, and thus thinking carefully about when and how explanation from AI systems might improve accountability is timely. In this work, we review contexts in which explanation is currently required under the law, and then list the technical considerations that must be considered if we desired AI systems that could provide kinds of explanations that are currently required of humans.",
        "url": "https://arxiv.org/pdf/1711.01134v3.pdf"
    },
    {
        "title": "The Promise and Peril of Human Evaluation for Model Interpretability",
        "abstract": "Transparency, user trust, and human comprehension are popular ethical motivations for interpretable machine learning. In support of these goals, researchers evaluate model explanation performance using humans and real world applications. This alone presents a challenge in many areas of artificial intelligence. In this position paper, we propose a distinction between descriptive and persuasive explanations. We discuss reasoning suggesting that functional interpretability may be correlated with cognitive function and user preferences. If this is indeed the case, evaluation and optimization using functional metrics could perpetuate implicit cognitive bias in explanations that threaten transparency. Finally, we propose two potential research directions to disambiguate cognitive function and explanation models, retaining control over the tradeoff between accuracy and interpretability.",
        "url": "https://arxiv.org/pdf/1711.07414v2.pdf"
    },
    {
        "title": "Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions",
        "abstract": "Artificial Intelligence (AI) has been used extensively in automatic decision\nmaking in a broad variety of scenarios, ranging from credit ratings for loans\nto recommendations of movies. Traditional design guidelines for AI models focus\nessentially on accuracy maximization, but recent work has shown that\neconomically irrational and socially unacceptable scenarios of discrimination\nand unfairness are likely to arise unless these issues are explicitly\naddressed. This undesirable behavior has several possible sources, such as\nbiased datasets used for training that may not be detected in black-box models.\nAfter pointing out connections between such bias of AI and the problem of\ninduction, we focus on Popper's contributions after Hume's, which offer a\nlogical theory of preferences. An AI model can be preferred over others on\npurely rational grounds after one or more attempts at refutation based on\naccuracy and fairness. Inspired by such epistemological principles, this paper\nproposes a structured approach to mitigate discrimination and unfairness caused\nby bias in AI systems. In the proposed computational framework, models are\nselected and enhanced after attempts at refutation. To illustrate our\ndiscussion, we focus on hiring decision scenarios where an AI system filters in\nwhich job applicants should go to the interview phase.",
        "url": "http://arxiv.org/pdf/1711.07111v1.pdf"
    },
    {
        "title": "From Algorithmic Black Boxes to Adaptive White Boxes: Declarative Decision-Theoretic Ethical Programs as Codes of Ethics",
        "abstract": "Ethics of algorithms is an emerging topic in various disciplines such as\nsocial science, law, and philosophy, but also artificial intelligence (AI). The\nvalue alignment problem expresses the challenge of (machine) learning values\nthat are, in some way, aligned with human requirements or values. In this paper\nI argue for looking at how humans have formalized and communicated values, in\nprofessional codes of ethics, and for exploring declarative decision-theoretic\nethical programs (DDTEP) to formalize codes of ethics. This renders machine\nethical reasoning and decision-making, as well as learning, more transparent\nand hopefully more accountable. The paper includes proof-of-concept examples of\nknown toy dilemmas and gatekeeping domains such as archives and libraries.",
        "url": "http://arxiv.org/pdf/1711.06035v1.pdf"
    },
    {
        "title": "Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning",
        "abstract": "Having accurate, detailed, and up-to-date information about the location and\nbehavior of animals in the wild would revolutionize our ability to study and\nconserve ecosystems. We investigate the ability to automatically, accurately,\nand inexpensively collect such data, which could transform many fields of\nbiology, ecology, and zoology into \"big data\" sciences. Motion sensor \"camera\ntraps\" enable collecting wildlife pictures inexpensively, unobtrusively, and\nfrequently. However, extracting information from these pictures remains an\nexpensive, time-consuming, manual task. We demonstrate that such information\ncan be automatically extracted by deep learning, a cutting-edge type of\nartificial intelligence. We train deep convolutional neural networks to\nidentify, count, and describe the behaviors of 48 species in the\n3.2-million-image Snapshot Serengeti dataset. Our deep neural networks\nautomatically identify animals with over 93.8% accuracy, and we expect that\nnumber to improve rapidly in years to come. More importantly, if our system\nclassifies only images it is confident about, our system can automate animal\nidentification for 99.3% of the data while still performing at the same 96.6%\naccuracy as that of crowdsourced teams of human volunteers, saving more than\n8.4 years (at 40 hours per week) of human labeling effort (i.e. over 17,000\nhours) on this 3.2-million-image dataset. Those efficiency gains immediately\nhighlight the importance of using deep neural networks to automate data\nextraction from camera-trap images. Our results suggest that this technology\ncould enable the inexpensive, unobtrusive, high-volume, and even real-time\ncollection of a wealth of information about vast numbers of animals in the\nwild.",
        "url": "http://arxiv.org/pdf/1703.05830v5.pdf"
    },
    {
        "title": "Introduction to intelligent computing unit 1",
        "abstract": "This brief note highlights some basic concepts required toward understanding\nthe evolution of machine learning and deep learning models. The note starts\nwith an overview of artificial intelligence and its relationship to biological\nneuron that ultimately led to the evolution of todays intelligent models.",
        "url": "http://arxiv.org/pdf/1711.06552v1.pdf"
    },
    {
        "title": "TorusE: Knowledge Graph Embedding on a Lie Group",
        "abstract": "Knowledge graphs are useful for many artificial intelligence (AI) tasks.\nHowever, knowledge graphs often have missing facts. To populate the graphs,\nknowledge graph embedding models have been developed. Knowledge graph embedding\nmodels map entities and relations in a knowledge graph to a vector space and\npredict unknown triples by scoring candidate triples. TransE is the first\ntranslation-based method and it is well known because of its simplicity and\nefficiency for knowledge graph completion. It employs the principle that the\ndifferences between entity embeddings represent their relations. The principle\nseems very simple, but it can effectively capture the rules of a knowledge\ngraph. However, TransE has a problem with its regularization. TransE forces\nentity embeddings to be on a sphere in the embedding vector space. This\nregularization warps the embeddings and makes it difficult for them to fulfill\nthe abovementioned principle. The regularization also affects adversely the\naccuracies of the link predictions. On the other hand, regularization is\nimportant because entity embeddings diverge by negative sampling without it.\nThis paper proposes a novel embedding model, TorusE, to solve the\nregularization problem. The principle of TransE can be defined on any Lie\ngroup. A torus, which is one of the compact Lie groups, can be chosen for the\nembedding space to avoid regularization. To the best of our knowledge, TorusE\nis the first model that embeds objects on other than a real or complex vector\nspace, and this paper is the first to formally discuss the problem of\nregularization of TransE. Our approach outperforms other state-of-the-art\napproaches such as TransE, DistMult and ComplEx on a standard link prediction\ntask. We show that TorusE is scalable to large-size knowledge graphs and is\nfaster than the original TransE.",
        "url": "http://arxiv.org/pdf/1711.05435v1.pdf"
    },
    {
        "title": "Knowledge Transfer Between Artificial Intelligence Systems",
        "abstract": "We consider the fundamental question: how a legacy \"student\" Artificial\nIntelligent (AI) system could learn from a legacy \"teacher\" AI system or a\nhuman expert without complete re-training and, most importantly, without\nrequiring significant computational resources. Here \"learning\" is understood as\nan ability of one system to mimic responses of the other and vice-versa. We\ncall such learning an Artificial Intelligence knowledge transfer. We show that\nif internal variables of the \"student\" Artificial Intelligent system have the\nstructure of an $n$-dimensional topological vector space and $n$ is\nsufficiently high then, with probability close to one, the required knowledge\ntransfer can be implemented by simple cascades of linear functionals. In\nparticular, for $n$ sufficiently large, with probability close to one, the\n\"student\" system can successfully and non-iteratively learn $k\\ll n$ new\nexamples from the \"teacher\" (or correct the same number of mistakes) at the\ncost of two additional inner products. The concept is illustrated with an\nexample of knowledge transfer from a pre-trained convolutional neural network\nto a simple linear classifier with HOG features.",
        "url": "http://arxiv.org/pdf/1709.01547v2.pdf"
    },
    {
        "title": "Neural-Symbolic Learning and Reasoning: A Survey and Interpretation",
        "abstract": "The study and understanding of human behaviour is relevant to computer\nscience, artificial intelligence, neural computation, cognitive science,\nphilosophy, psychology, and several other areas. Presupposing cognition as\nbasis of behaviour, among the most prominent tools in the modelling of\nbehaviour are computational-logic systems, connectionist models of cognition,\nand models of uncertainty. Recent studies in cognitive science, artificial\nintelligence, and psychology have produced a number of cognitive models of\nreasoning, learning, and language that are underpinned by computation. In\naddition, efforts in computer science research have led to the development of\ncognitive computational systems integrating machine learning and automated\nreasoning. Such systems have shown promise in a range of applications,\nincluding computational biology, fault diagnosis, training and assessment in\nsimulators, and software verification. This joint survey reviews the personal\nideas and views of several researchers on neural-symbolic learning and\nreasoning. The article is organised in three parts: Firstly, we frame the scope\nand goals of neural-symbolic computation and have a look at the theoretical\nfoundations. We then proceed to describe the realisations of neural-symbolic\ncomputation, systems, and applications. Finally we present the challenges\nfacing the area and avenues for further research.",
        "url": "http://arxiv.org/pdf/1711.03902v1.pdf"
    },
    {
        "title": "Deep Neural Networks to Enable Real-time Multimessenger Astrophysics",
        "abstract": "Gravitational wave astronomy has set in motion a scientific revolution. To\nfurther enhance the science reach of this emergent field, there is a pressing\nneed to increase the depth and speed of the gravitational wave algorithms that\nhave enabled these groundbreaking discoveries. To contribute to this effort, we\nintroduce Deep Filtering, a new highly scalable method for end-to-end\ntime-series signal processing, based on a system of two deep convolutional\nneural networks, which we designed for classification and regression to rapidly\ndetect and estimate parameters of signals in highly noisy time-series data\nstreams. We demonstrate a novel training scheme with gradually increasing noise\nlevels, and a transfer learning procedure between the two networks. We showcase\nthe application of this method for the detection and parameter estimation of\ngravitational waves from binary black hole mergers. Our results indicate that\nDeep Filtering significantly outperforms conventional machine learning\ntechniques, achieves similar performance compared to matched-filtering while\nbeing several orders of magnitude faster thus allowing real-time processing of\nraw big data with minimal resources. More importantly, Deep Filtering extends\nthe range of gravitational wave signals that can be detected with ground-based\ngravitational wave detectors. This framework leverages recent advances in\nartificial intelligence algorithms and emerging hardware architectures, such as\ndeep-learning-optimized GPUs, to facilitate real-time searches of gravitational\nwave sources and their electromagnetic and astro-particle counterparts.",
        "url": "http://arxiv.org/pdf/1701.00008v3.pdf"
    },
    {
        "title": "Distributed Bayesian Piecewise Sparse Linear Models",
        "abstract": "The importance of interpretability of machine learning models has been\nincreasing due to emerging enterprise predictive analytics, threat of data\nprivacy, accountability of artificial intelligence in society, and so on.\nPiecewise linear models have been actively studied to achieve both accuracy and\ninterpretability. They often produce competitive accuracy against\nstate-of-the-art non-linear methods. In addition, their representations (i.e.,\nrule-based segmentation plus sparse linear formula) are often preferred by\ndomain experts. A disadvantage of such models, however, is high computational\ncost for simultaneous determinations of the number of \"pieces\" and cardinality\nof each linear predictor, which has restricted their applicability to\nmiddle-scale data sets. This paper proposes a distributed factorized asymptotic\nBayesian (FAB) inference of learning piece-wise sparse linear models on\ndistributed memory architectures. The distributed FAB inference solves the\nsimultaneous model selection issue without communicating $O(N)$ data where N is\nthe number of training samples and achieves linear scale-out against the number\nof CPU cores. Experimental results demonstrate that the distributed FAB\ninference achieves high prediction accuracy and performance scalability with\nboth synthetic and benchmark data.",
        "url": "http://arxiv.org/pdf/1711.02368v1.pdf"
    },
    {
        "title": "A General Neural Network Hardware Architecture on FPGA",
        "abstract": "Field Programmable Gate Arrays (FPGAs) plays an increasingly important role\nin data sampling and processing industries due to its highly parallel\narchitecture, low power consumption, and flexibility in custom algorithms.\nEspecially, in the artificial intelligence field, for training and implement\nthe neural networks and machine learning algorithms, high energy efficiency\nhardware implement and massively parallel computing capacity are heavily\ndemanded. Therefore, many global companies have applied FPGAs into AI and\nMachine learning fields such as autonomous driving and Automatic Spoken\nLanguage Recognition (Baidu) [1] [2] and Bing search (Microsoft) [3].\nConsidering the FPGAs great potential in these fields, we tend to implement a\ngeneral neural network hardware architecture on XILINX ZU9CG System On Chip\n(SOC) platform [4], which contains abundant hardware resource and powerful\nprocessing capacity. The general neural network architecture on the FPGA SOC\nplatform can perform forward and backward algorithms in deep neural networks\n(DNN) with high performance and easily be adjusted according to the type and\nscale of the neural networks.",
        "url": "http://arxiv.org/pdf/1711.05860v1.pdf"
    },
    {
        "title": "Learning Solving Procedure for Artificial Neural Network",
        "abstract": "It is expected that progress toward true artificial intelligence will be\nachieved through the emergence of a system that integrates representation\nlearning and complex reasoning (LeCun et al. 2015). In response to this\nprediction, research has been conducted on implementing the symbolic reasoning\nof a von Neumann computer in an artificial neural network (Graves et al. 2016;\nGraves et al. 2014; Reed et al. 2015). However, these studies have many\nlimitations in realizing neural-symbolic integration (Jaeger. 2016). Here, we\npresent a new learning paradigm: a learning solving procedure (LSP) that learns\nthe procedure for solving complex problems. This is not accomplished merely by\nlearning input-output data, but by learning algorithms through a solving\nprocedure that obtains the output as a sequence of tasks for a given input\nproblem. The LSP neural network system not only learns simple problems of\naddition and multiplication, but also the algorithms of complicated problems,\nsuch as complex arithmetic expression, sorting, and Hanoi Tower. To realize\nthis, the LSP neural network structure consists of a deep neural network and\nlong short-term memory, which are recursively combined. Through\nexperimentation, we demonstrate the efficiency and scalability of LSP and its\nvalidity as a mechanism of complex reasoning.",
        "url": "http://arxiv.org/pdf/1711.01754v1.pdf"
    },
    {
        "title": "Data Fusion on Motion and Magnetic Sensors embedded on Mobile Devices for the Identification of Activities of Daily Living",
        "abstract": "Several types of sensors have been available in off-the-shelf mobile devices,\nincluding motion, magnetic, vision, acoustic, and location sensors. This paper\nfocuses on the fusion of the data acquired from motion and magnetic sensors,\ni.e., accelerometer, gyroscope and magnetometer sensors, for the recognition of\nActivities of Daily Living (ADL) using pattern recognition techniques. The\nsystem developed in this study includes data acquisition, data processing, data\nfusion, and artificial intelligence methods. Artificial Neural Networks (ANN)\nare included in artificial intelligence methods, which are used in this study\nfor the recognition of ADL. The purpose of this study is the creation of a new\nmethod using ANN for the identification of ADL, comparing three types of ANN,\nin order to achieve results with a reliable accuracy. The best accuracy was\nobtained with Deep Learning, which, after the application of the L2\nregularization and normalization techniques on the sensors data, reports an\naccuracy of 89.51%.",
        "url": "http://arxiv.org/pdf/1711.07328v1.pdf"
    },
    {
        "title": "Projective simulation with generalization",
        "abstract": "The ability to generalize is an important feature of any intelligent agent.\nNot only because it may allow the agent to cope with large amounts of data, but\nalso because in some environments, an agent with no generalization capabilities\ncannot learn. In this work we outline several criteria for generalization, and\npresent a dynamic and autonomous machinery that enables projective simulation\nagents to meaningfully generalize. Projective simulation, a novel, physical\napproach to artificial intelligence, was recently shown to perform well in\nstandard reinforcement learning problems, with applications in advanced\nrobotics as well as quantum experiments. Both the basic projective simulation\nmodel and the presented generalization machinery are based on very simple\nprinciples. This allows us to provide a full analytical analysis of the agent's\nperformance and to illustrate the benefit the agent gains by generalizing.\nSpecifically, we show that already in basic (but extreme) environments,\nlearning without generalization may be impossible, and demonstrate how the\npresented generalization machinery enables the projective simulation agent to\nlearn.",
        "url": "http://arxiv.org/pdf/1504.02247v2.pdf"
    },
    {
        "title": "Detection and Analysis of Human Emotions through Voice and Speech Pattern Processing",
        "abstract": "The ability to modulate vocal sounds and generate speech is one of the\nfeatures which set humans apart from other living beings. The human voice can\nbe characterized by several attributes such as pitch, timbre, loudness, and\nvocal tone. It has often been observed that humans express their emotions by\nvarying different vocal attributes during speech generation. Hence, deduction\nof human emotions through voice and speech analysis has a practical\nplausibility and could potentially be beneficial for improving human\nconversational and persuasion skills. This paper presents an algorithmic\napproach for detection and analysis of human emotions with the help of voice\nand speech processing. The proposed approach has been developed with the\nobjective of incorporation with futuristic artificial intelligence systems for\nimproving human-computer interactions.",
        "url": "http://arxiv.org/pdf/1710.10198v1.pdf"
    },
    {
        "title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning",
        "abstract": "Conceptual spaces are geometric representations of conceptual knowledge, in\nwhich entities correspond to points, natural properties correspond to convex\nregions, and the dimensions of the space correspond to salient features. While\nconceptual spaces enable elegant models of various cognitive phenomena, the\nlack of automated methods for constructing such representations have so far\nlimited their application in artificial intelligence. To address this issue, we\npropose a method which learns a vector-space embedding of entities from\nWikipedia and constrains this embedding such that entities of the same semantic\ntype are located in some lower-dimensional subspace. We experimentally\ndemonstrate the usefulness of these subspaces as (approximate) conceptual space\nrepresentations by showing, among others, that important features can be\nmodelled as directions and that natural properties tend to correspond to convex\nregions.",
        "url": "http://arxiv.org/pdf/1602.05765v2.pdf"
    },
    {
        "title": "How Important is Syntactic Parsing Accuracy? An Empirical Evaluation on Rule-Based Sentiment Analysis",
        "abstract": "Syntactic parsing, the process of obtaining the internal structure of\nsentences in natural languages, is a crucial task for artificial intelligence\napplications that need to extract meaning from natural language text or speech.\nSentiment analysis is one example of application for which parsing has recently\nproven useful.\n  In recent years, there have been significant advances in the accuracy of\nparsing algorithms. In this article, we perform an empirical, task-oriented\nevaluation to determine how parsing accuracy influences the performance of a\nstate-of-the-art rule-based sentiment analysis system that determines the\npolarity of sentences from their parse trees. In particular, we evaluate the\nsystem using four well-known dependency parsers, including both current models\nwith state-of-the-art accuracy and more innacurate models which, however,\nrequire less computational resources.\n  The experiments show that all of the parsers produce similarly good results\nin the sentiment analysis task, without their accuracy having any relevant\ninfluence on the results. Since parsing is currently a task with a relatively\nhigh computational cost that varies strongly between algorithms, this suggests\nthat sentiment analysis researchers and users should prioritize speed over\naccuracy when choosing a parser; and parsing researchers should investigate\nmodels that improve speed further, even at some cost to accuracy.",
        "url": "http://arxiv.org/pdf/1706.02141v3.pdf"
    },
    {
        "title": "Human-in-the-loop Artificial Intelligence",
        "abstract": "Little by little, newspapers are revealing the bright future that Artificial\nIntelligence (AI) is building. Intelligent machines will help everywhere.\nHowever, this bright future has a dark side: a dramatic job market contraction\nbefore its unpredictable transformation. Hence, in a near future, large numbers\nof job seekers will need financial support while catching up with these novel\nunpredictable jobs. This possible job market crisis has an antidote inside. In\nfact, the rise of AI is sustained by the biggest knowledge theft of the recent\nyears. Learning AI machines are extracting knowledge from unaware skilled or\nunskilled workers by analyzing their interactions. By passionately doing their\njobs, these workers are digging their own graves.\n  In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI)\nas a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward\naware and unaware knowledge producers with a different scheme: decisions of AI\nsystems generating revenues will repay the legitimate owners of the knowledge\nused for taking those decisions. As modern Robin Hoods, HIT-AI researchers\nshould fight for a fairer Artificial Intelligence that gives back what it\nsteals.",
        "url": "http://arxiv.org/pdf/1710.08191v1.pdf"
    },
    {
        "title": "Towards Verified Artificial Intelligence",
        "abstract": "Verified artificial intelligence (AI) is the goal of designing AI-based systems that that have strong, ideally provable, assurances of correctness with respect to mathematically-specified requirements. This paper considers Verified AI from a formal methods perspective. We describe five challenges for achieving Verified AI, and five corresponding principles for addressing these challenges.",
        "url": "https://arxiv.org/pdf/1606.08514v4.pdf"
    },
    {
        "title": "What Automated Planning can do for Business Process Management",
        "abstract": "Business Process Management (BPM) is a central element of today\norganizations. Despite over the years its main focus has been the support of\nprocesses in highly controlled domains, nowadays many domains of interest to\nthe BPM community are characterized by ever-changing requirements,\nunpredictable environments and increasing amounts of data that influence the\nexecution of process instances. Under such dynamic conditions, BPM systems must\nincrease their level of automation to provide the reactivity and flexibility\nnecessary for process management. On the other hand, the Artificial\nIntelligence (AI) community has concentrated its efforts on investigating\ndynamic domains that involve active control of computational entities and\nphysical devices (e.g., robots, software agents, etc.). In this context,\nAutomated Planning, which is one of the oldest areas in AI, is conceived as a\nmodel-based approach to synthesize autonomous behaviours in automated way from\na model. In this paper, we discuss how automated planning techniques can be\nleveraged to enable new levels of automation and support for business\nprocessing, and we show some concrete examples of their successful application\nto the different stages of the BPM life cycle.",
        "url": "http://arxiv.org/pdf/1709.10482v2.pdf"
    },
    {
        "title": "Characterization of Gradient Dominance and Regularity Conditions for Neural Networks",
        "abstract": "The past decade has witnessed a successful application of deep learning to\nsolving many challenging problems in machine learning and artificial\nintelligence. However, the loss functions of deep neural networks (especially\nnonlinear networks) are still far from being well understood from a theoretical\naspect. In this paper, we enrich the current understanding of the landscape of\nthe square loss functions for three types of neural networks. Specifically,\nwhen the parameter matrices are square, we provide an explicit characterization\nof the global minimizers for linear networks, linear residual networks, and\nnonlinear networks with one hidden layer. Then, we establish two quadratic\ntypes of landscape properties for the square loss of these neural networks,\ni.e., the gradient dominance condition within the neighborhood of their full\nrank global minimizers, and the regularity condition along certain directions\nand within the neighborhood of their global minimizers. These two landscape\nproperties are desirable for the optimization around the global minimizers of\nthe loss function for these neural networks.",
        "url": "http://arxiv.org/pdf/1710.06910v2.pdf"
    },
    {
        "title": "Unsupervised Sentence Representations as Word Information Series: Revisiting TF--IDF",
        "abstract": "Sentence representation at the semantic level is a challenging task for\nNatural Language Processing and Artificial Intelligence. Despite the advances\nin word embeddings (i.e. word vector representations), capturing sentence\nmeaning is an open question due to complexities of semantic interactions among\nwords. In this paper, we present an embedding method, which is aimed at\nlearning unsupervised sentence representations from unlabeled text. We propose\nan unsupervised method that models a sentence as a weighted series of word\nembeddings. The weights of the word embeddings are fitted by using Shannon's\nword entropies provided by the Term Frequency--Inverse Document Frequency\n(TF--IDF) transform. The hyperparameters of the model can be selected according\nto the properties of data (e.g. sentence length and textual gender).\nHyperparameter selection involves word embedding methods and dimensionalities,\nas well as weighting schemata. Our method offers advantages over existing\nmethods: identifiable modules, short-term training, online inference of\n(unseen) sentence representations, as well as independence from domain,\nexternal knowledge and language resources. Results showed that our model\noutperformed the state of the art in well-known Semantic Textual Similarity\n(STS) benchmarks. Moreover, our model reached state-of-the-art performance when\ncompared to supervised and knowledge-based STS systems.",
        "url": "http://arxiv.org/pdf/1710.06524v2.pdf"
    },
    {
        "title": "Discovery Radiomics via Evolutionary Deep Radiomic Sequencer Discovery for Pathologically-Proven Lung Cancer Detection",
        "abstract": "While lung cancer is the second most diagnosed form of cancer in men and\nwomen, a sufficiently early diagnosis can be pivotal in patient survival rates.\nImaging-based, or radiomics-driven, detection methods have been developed to\naid diagnosticians, but largely rely on hand-crafted features which may not\nfully encapsulate the differences between cancerous and healthy tissue.\nRecently, the concept of discovery radiomics was introduced, where custom\nabstract features are discovered from readily available imaging data. We\npropose a novel evolutionary deep radiomic sequencer discovery approach based\non evolutionary deep intelligence. Motivated by patient privacy concerns and\nthe idea of operational artificial intelligence, the evolutionary deep radiomic\nsequencer discovery approach organically evolves increasingly more efficient\ndeep radiomic sequencers that produce significantly more compact yet similarly\ndescriptive radiomic sequences over multiple generations. As a result, this\nframework improves operational efficiency and enables diagnosis to be run\nlocally at the radiologist's computer while maintaining detection accuracy. We\nevaluated the evolved deep radiomic sequencer (EDRS) discovered via the\nproposed evolutionary deep radiomic sequencer discovery framework against\nstate-of-the-art radiomics-driven and discovery radiomics methods using\nclinical lung CT data with pathologically-proven diagnostic data from the\nLIDC-IDRI dataset. The evolved deep radiomic sequencer shows improved\nsensitivity (93.42%), specificity (82.39%), and diagnostic accuracy (88.78%)\nrelative to previous radiomics approaches.",
        "url": "http://arxiv.org/pdf/1705.03572v2.pdf"
    },
    {
        "title": "A Memristor-Based Optimization Framework for AI Applications",
        "abstract": "Memristors have recently received significant attention as ubiquitous\ndevice-level components for building a novel generation of computing systems.\nThese devices have many promising features, such as non-volatility, low power\nconsumption, high density, and excellent scalability. The ability to control\nand modify biasing voltages at the two terminals of memristors make them\npromising candidates to perform matrix-vector multiplications and solve systems\nof linear equations. In this article, we discuss how networks of memristors\narranged in crossbar arrays can be used for efficiently solving optimization\nand machine learning problems. We introduce a new memristor-based optimization\nframework that combines the computational merit of memristor crossbars with the\nadvantages of an operator splitting method, alternating direction method of\nmultipliers (ADMM). Here, ADMM helps in splitting a complex optimization\nproblem into subproblems that involve the solution of systems of linear\nequations. The capability of this framework is shown by applying it to linear\nprogramming, quadratic programming, and sparse optimization. In addition to\nADMM, implementation of a customized power iteration (PI) method for\neigenvalue/eigenvector computation using memristor crossbars is discussed. The\nmemristor-based PI method can further be applied to principal component\nanalysis (PCA). The use of memristor crossbars yields a significant speed-up in\ncomputation, and thus, we believe, has the potential to advance optimization\nand machine learning research in artificial intelligence (AI).",
        "url": "http://arxiv.org/pdf/1710.08882v1.pdf"
    },
    {
        "title": "On Hashing-Based Approaches to Approximate DNF-Counting",
        "abstract": "Propositional model counting is a fundamental problem in artificial\nintelligence with a wide variety of applications, such as probabilistic\ninference, decision making under uncertainty, and probabilistic databases.\nConsequently, the problem is of theoretical as well as practical interest. When\nthe constraints are expressed as DNF formulas, Monte Carlo-based techniques\nhave been shown to provide a fully polynomial randomized approximation scheme\n(FPRAS). For CNF constraints, hashing-based approximation techniques have been\ndemonstrated to be highly successful. Furthermore, it was shown that\nhashing-based techniques also yield an FPRAS for DNF counting without usage of\nMonte Carlo sampling. Our analysis, however, shows that the proposed\nhashing-based approach to DNF counting provides poor time complexity compared\nto the Monte Carlo-based DNF counting techniques. Given the success of\nhashing-based techniques for CNF constraints, it is natural to ask: Can\nhashing-based techniques provide an efficient FPRAS for DNF counting? In this\npaper, we provide a positive answer to this question. To this end, we introduce\ntwo novel algorithmic techniques: \\emph{Symbolic Hashing} and \\emph{Stochastic\nCell Counting}, along with a new hash family of \\emph{Row-Echelon hash\nfunctions}. These innovations allow us to design a hashing-based FPRAS for DNF\ncounting of similar complexity (up to polylog factors) as that of prior works.\nFurthermore, we expect these techniques to have potential applications beyond\nDNF counting.",
        "url": "http://arxiv.org/pdf/1710.05247v1.pdf"
    },
    {
        "title": "Artificial Neural Networks-Based Machine Learning for Wireless Networks: A Tutorial",
        "abstract": "Next-generation wireless networks must support ultra-reliable, low-latency communication and intelligently manage a massive number of Internet of Things (IoT) devices in real-time, within a highly dynamic environment. This need for stringent communication quality-of-service (QoS) requirements as well as mobile edge and core intelligence can only be realized by integrating fundamental notions of artificial intelligence (AI) and machine learning across the wireless infrastructure and end-user devices. In this context, this paper provides a comprehensive tutorial that introduces the main concepts of machine learning, in general, and artificial neural networks (ANNs), in particular, and their potential applications in wireless communications. For this purpose, we present a comprehensive overview on a number of key types of neural networks that include feed-forward, recurrent, spiking, and deep neural networks. For each type of neural network, we present the basic architecture and training procedure, as well as the associated challenges and opportunities. Then, we provide an in-depth overview on the variety of wireless communication problems that can be addressed using ANNs, ranging from communication using unmanned aerial vehicles to virtual reality and edge caching.For each individual application, we present the main motivation for using ANNs along with the associated challenges while also providing a detailed example for a use case scenario and outlining future works that can be addressed using ANNs. In a nutshell, this article constitutes one of the first holistic tutorials on the development of machine learning techniques tailored to the needs of future wireless networks.",
        "url": "https://arxiv.org/pdf/1710.02913v2.pdf"
    },
    {
        "title": "Intelligence Quotient and Intelligence Grade of Artificial Intelligence",
        "abstract": "Although artificial intelligence is currently one of the most interesting\nareas in scientific research, the potential threats posed by emerging AI\nsystems remain a source of persistent controversy. To address the issue of AI\nthreat, this study proposes a standard intelligence model that unifies AI and\nhuman characteristics in terms of four aspects of knowledge, i.e., input,\noutput, mastery, and creation. Using this model, we observe three challenges,\nnamely, expanding of the von Neumann architecture; testing and ranking the\nintelligence quotient of naturally and artificially intelligent systems,\nincluding humans, Google, Bing, Baidu, and Siri; and finally, the dividing of\nartificially intelligent systems into seven grades from robots to Google Brain.\nBased on this, we conclude that AlphaGo belongs to the third grade.",
        "url": "http://arxiv.org/pdf/1709.10242v2.pdf"
    },
    {
        "title": "Feasibility Study: Moving Non-Homogeneous Teams in Congested Video Game Environments",
        "abstract": "Multi-agent path finding (MAPF) is a well-studied problem in artificial\nintelligence, where one needs to find collision-free paths for agents with\ngiven start and goal locations. In video games, agents of different types often\nform teams. In this paper, we demonstrate the usefulness of MAPF algorithms\nfrom artificial intelligence for moving such non-homogeneous teams in congested\nvideo game environments.",
        "url": "http://arxiv.org/pdf/1710.01447v1.pdf"
    },
    {
        "title": "Simple Cortex: A Model of Cells in the Sensory Nervous System",
        "abstract": "Neuroscience research has produced many theories and computational neural\nmodels of sensory nervous systems. Notwithstanding many different perspectives\ntowards developing intelligent machines, artificial intelligence has ultimately\nbeen influenced by neuroscience. Therefore, this paper provides an introduction\nto biologically inspired machine intelligence by exploring the basic principles\nof sensation and perception as well as the structure and behavior of biological\nsensory nervous systems like the neocortex. Concepts like spike timing,\nsynaptic plasticity, inhibition, neural structure, and neural behavior are\napplied to a new model, Simple Cortex (SC). A software implementation of SC has\nbeen built and demonstrates fast observation, learning, and prediction of\nspatio-temporal sensory-motor patterns and sequences. Finally, this paper\nsuggests future areas of improvement and growth for Simple Cortex and other\nrelated machine intelligence models.",
        "url": "http://arxiv.org/pdf/1710.01347v1.pdf"
    },
    {
        "title": "It Takes Two to Tango: Towards Theory of AI's Mind",
        "abstract": "Theory of Mind is the ability to attribute mental states (beliefs, intents,\nknowledge, perspectives, etc.) to others and recognize that these mental states\nmay differ from one's own. Theory of Mind is critical to effective\ncommunication and to teams demonstrating higher collective performance. To\neffectively leverage the progress in Artificial Intelligence (AI) to make our\nlives more productive, it is important for humans and AI to work well together\nin a team. Traditionally, there has been much emphasis on research to make AI\nmore accurate, and (to a lesser extent) on having it better understand human\nintentions, tendencies, beliefs, and contexts. The latter involves making AI\nmore human-like and having it develop a theory of our minds. In this work, we\nargue that for human-AI teams to be effective, humans must also develop a\ntheory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,\nand quirks. We instantiate these ideas within the domain of Visual Question\nAnswering (VQA). We find that using just a few examples (50), lay people can be\ntrained to better predict responses and oncoming failures of a complex VQA\nmodel. We further evaluate the role existing explanation (or interpretability)\nmodalities play in helping humans build ToAIM. Explainable AI has received\nconsiderable scientific and popular attention in recent times. Surprisingly, we\nfind that having access to the model's internal states - its confidence in its\ntop-k predictions, explicit or implicit attention maps which highlight regions\nin the image (and words in the question) the model is looking at (and listening\nto) while answering a question about an image - do not help people better\npredict its behavior.",
        "url": "http://arxiv.org/pdf/1704.00717v2.pdf"
    },
    {
        "title": "Cooperative Automated Vehicles: a Review of Opportunities and Challenges in Socially Intelligent Vehicles Beyond Networking",
        "abstract": "The connected automated vehicle has been often touted as a technology that will become pervasive in society in the near future. One can view an automated vehicle as having Artificial Intelligence (AI) capabilities, being able to self-drive, sense its surroundings, recognise objects in its vicinity, and perform reasoning and decision-making. Rather than being stand alone, we examine the need for automated vehicles to cooperate and interact within their socio-cyber-physical environments, including the problems cooperation will solve, but also the issues and challenges. We review current work in cooperation for automated vehicles, based on selected examples from the literature. We conclude noting the need for the ability to behave cooperatively as a form of social-AI capability for automated vehicles, beyond sensing the immediate environment and beyond the underlying networking technology.",
        "url": "https://arxiv.org/pdf/1710.00461v2.pdf"
    },
    {
        "title": "Tweeting AI: Perceptions of AI-Tweeters (AIT) vs Expert AI-Tweeters (EAIT)",
        "abstract": "With the recent advancements in Artificial Intelligence (AI), various\norganizations and individuals started debating about the progress of AI as a\nblessing or a curse for the future of the society. This paper conducts an\ninvestigation on how the public perceives the progress of AI by utilizing the\ndata shared on Twitter. Specifically, this paper performs a comparative\nanalysis on the understanding of users from two categories -- general\nAI-Tweeters (AIT) and the expert AI-Tweeters (EAIT) who share posts about AI on\nTwitter. Our analysis revealed that users from both the categories express\ndistinct emotions and interests towards AI. Users from both the categories\nregard AI as positive and are optimistic about the progress of AI but the\nexperts are more negative than the general AI-Tweeters. Characterization of\nusers manifested that `London' is the popular location of users from where they\ntweet about AI. Tweets posted by AIT are highly retweeted than posts made by\nEAIT that reveals greater diffusion of information from AIT.",
        "url": "http://arxiv.org/pdf/1704.08389v2.pdf"
    },
    {
        "title": "Applying Data Augmentation to Handwritten Arabic Numeral Recognition Using Deep Learning Neural Networks",
        "abstract": "Handwritten character recognition has been the center of research and a benchmark problem in the sector of pattern recognition and artificial intelligence, and it continues to be a challenging research topic. Due to its enormous application many works have been done in this field focusing on different languages. Arabic, being a diversified language has a huge scope of research with potential challenges. A convolutional neural network model for recognizing handwritten numerals in Arabic language is proposed in this paper, where the dataset is subject to various augmentation in order to add robustness needed for deep learning approach. The proposed method is empowered by the presence of dropout regularization to do away with the problem of data overfitting. Moreover, suitable change is introduced in activation function to overcome the problem of vanishing gradient. With these modifications, the proposed system achieves an accuracy of 99.4\\% which performs better than every previous work on the dataset.",
        "url": "https://arxiv.org/pdf/1708.05969v5.pdf"
    },
    {
        "title": "Unsupervised Generative Modeling Using Matrix Product States",
        "abstract": "Generative modeling, which learns joint probability distribution from data\nand generates samples according to it, is an important task in machine learning\nand artificial intelligence. Inspired by probabilistic interpretation of\nquantum physics, we propose a generative model using matrix product states,\nwhich is a tensor network originally proposed for describing (particularly\none-dimensional) entangled quantum states. Our model enjoys efficient learning\nanalogous to the density matrix renormalization group method, which allows\ndynamically adjusting dimensions of the tensors and offers an efficient direct\nsampling approach for generative tasks. We apply our method to generative\nmodeling of several standard datasets including the Bars and Stripes, random\nbinary patterns and the MNIST handwritten digits to illustrate the abilities,\nfeatures and drawbacks of our model over popular generative models such as\nHopfield model, Boltzmann machines and generative adversarial networks. Our\nwork sheds light on many interesting directions of future exploration on the\ndevelopment of quantum-inspired algorithms for unsupervised machine learning,\nwhich are promisingly possible to be realized on quantum devices.",
        "url": "http://arxiv.org/pdf/1709.01662v3.pdf"
    },
    {
        "title": "An enhanced method to compute the similarity between concepts of ontology",
        "abstract": "With the use of ontologies in several domains such as semantic web,\ninformation retrieval, artificial intelligence, the concept of similarity\nmeasuring has become a very important domain of research. Therefore, in the\ncurrent paper, we propose our method of similarity measuring which uses the\nDijkstra algorithm to define and compute the shortest path. Then, we use this\none to compute the semantic distance between two concepts defined in the same\nhierarchy of ontology. Afterward, we base on this result to compute the\nsemantic similarity. Finally, we present an experimental comparison between our\nmethod and other methods of similarity measuring.",
        "url": "http://arxiv.org/pdf/1709.08880v1.pdf"
    },
    {
        "title": "Tweeting AI: Perceptions of Lay vs Expert Twitterati",
        "abstract": "With the recent advancements in Artificial Intelligence (AI), various\norganizations and individuals are debating about the progress of AI as a\nblessing or a curse for the future of the society. This paper conducts an\ninvestigation on how the public perceives the progress of AI by utilizing the\ndata shared on Twitter. Specifically, this paper performs a comparative\nanalysis on the understanding of users belonging to two categories -- general\nAI-Tweeters (AIT) and expert AI-Tweeters (EAIT) who share posts about AI on\nTwitter. Our analysis revealed that users from both the categories express\ndistinct emotions and interests towards AI. Users from both the categories\nregard AI as positive and are optimistic about the progress of AI but the\nexperts are more negative than the general AI-Tweeters. Expert AI-Tweeters\nshare relatively large percentage of tweets about their personal news compared\nto technical aspects of AI. However, the effects of automation on the future\nare of primary concern to AIT than to EAIT. When the expert category is\nsub-categorized, the emotion analysis revealed that students and industry\nprofessionals have more insights in their tweets about AI than academicians.",
        "url": "http://arxiv.org/pdf/1709.09534v1.pdf"
    },
    {
        "title": "Using NLU in Context for Question Answering: Improving on Facebook's bAbI Tasks",
        "abstract": "For the next step in human to machine interaction, Artificial Intelligence\n(AI) should interact predominantly using natural language because, if it\nworked, it would be the fastest way to communicate. Facebook's toy tasks (bAbI)\nprovide a useful benchmark to compare implementations for conversational AI.\nWhile the published experiments so far have been based on exploiting the\ndistributional hypothesis with machine learning, our model exploits natural\nlanguage understanding (NLU) with the decomposition of language based on Role\nand Reference Grammar (RRG) and the brain-based Patom theory. Our combinatorial\nsystem for conversational AI based on linguistics has many advantages: passing\nbAbI task tests without parsing or statistics while increasing scalability. Our\nmodel validates both the training and test data to find 'garbage' input and\noutput (GIGO). It is not rules-based, nor does it use parts of speech, but\ninstead relies on meaning. While Deep Learning is difficult to debug and fix,\nevery step in our model can be understood and changed like any non-statistical\ncomputer program. Deep Learning's lack of explicable reasoning has raised\nopposition to AI, partly due to fear of the unknown. To support the goals of\nAI, we propose extended tasks to use human-level statements with tense, aspect\nand voice, and embedded clauses with junctures: and answers to be natural\nlanguage generation (NLG) instead of keywords. While machine learning permits\ninvalid training data to produce incorrect test responses, our system cannot\nbecause the context tracking would need to be intentionally broken. We believe\nno existing learning systems can currently solve these extended natural\nlanguage tests. There appears to be a knowledge gap between NLP researchers and\nlinguists, but ongoing competitive results such as these promise to narrow that\ngap.",
        "url": "http://arxiv.org/pdf/1709.04558v2.pdf"
    },
    {
        "title": "Unsupervised Machine Learning for Networking: Techniques, Applications and Research Challenges",
        "abstract": "While machine learning and artificial intelligence have long been applied in\nnetworking research, the bulk of such works has focused on supervised learning.\nRecently there has been a rising trend of employing unsupervised machine\nlearning using unstructured raw network data to improve network performance and\nprovide services such as traffic engineering, anomaly detection, Internet\ntraffic classification, and quality of service optimization. The interest in\napplying unsupervised learning techniques in networking emerges from their\ngreat success in other fields such as computer vision, natural language\nprocessing, speech recognition, and optimal control (e.g., for developing\nautonomous self-driving cars). Unsupervised learning is interesting since it\ncan unconstrain us from the need of labeled data and manual handcrafted feature\nengineering thereby facilitating flexible, general, and automated methods of\nmachine learning. The focus of this survey paper is to provide an overview of\nthe applications of unsupervised learning in the domain of networking. We\nprovide a comprehensive survey highlighting the recent advancements in\nunsupervised learning techniques and describe their applications for various\nlearning tasks in the context of networking. We also provide a discussion on\nfuture directions and open research issues, while also identifying potential\npitfalls. While a few survey papers focusing on the applications of machine\nlearning in networking have previously been published, a survey of similar\nscope and breadth is missing in literature. Through this paper, we advance the\nstate of knowledge by carefully synthesizing the insights from these survey\npapers while also providing contemporary coverage of recent advances.",
        "url": "http://arxiv.org/pdf/1709.06599v1.pdf"
    },
    {
        "title": "Arguments for the Effectiveness of Human Problem Solving",
        "abstract": "The question of how humans solve problem has been addressed extensively.\nHowever, the direct study of the effectiveness of this process seems to be\noverlooked. In this paper, we address the issue of the effectiveness of human\nproblem solving: we analyze where this effectiveness comes from and what\ncognitive mechanisms or heuristics are involved. Our results are based on the\noptimal probabilistic problem solving strategy that appeared in Solomonoff\npaper on general problem solving system. We provide arguments that a certain\nset of cognitive mechanisms or heuristics drive human problem solving in the\nsimilar manner as the optimal Solomonoff strategy. The results presented in\nthis paper can serve both cognitive psychology in better understanding of human\nproblem solving processes as well as artificial intelligence in designing more\nhuman-like agents.",
        "url": "http://arxiv.org/pdf/1506.02930v2.pdf"
    },
    {
        "title": "Embodied Artificial Intelligence through Distributed Adaptive Control: An Integrated Framework",
        "abstract": "In this paper, we argue that the future of Artificial Intelligence research\nresides in two keywords: integration and embodiment. We support this claim by\nanalyzing the recent advances of the field. Regarding integration, we note that\nthe most impactful recent contributions have been made possible through the\nintegration of recent Machine Learning methods (based in particular on Deep\nLearning and Recurrent Neural Networks) with more traditional ones (e.g.\nMonte-Carlo tree search, goal babbling exploration or addressable memory\nsystems). Regarding embodiment, we note that the traditional benchmark tasks\n(e.g. visual classification or board games) are becoming obsolete as\nstate-of-the-art learning algorithms approach or even surpass human performance\nin most of them, having recently encouraged the development of first-person 3D\ngame platforms embedding realistic physics. Building upon this analysis, we\nfirst propose an embodied cognitive architecture integrating heterogenous\nsub-fields of Artificial Intelligence into a unified framework. We demonstrate\nthe utility of our approach by showing how major contributions of the field can\nbe expressed within the proposed framework. We then claim that benchmarking\nenvironments need to reproduce ecologically-valid conditions for bootstrapping\nthe acquisition of increasingly complex cognitive skills through the concept of\na cognitive arms race between embodied agents.",
        "url": "http://arxiv.org/pdf/1704.01407v3.pdf"
    },
    {
        "title": "Commonsense Scene Semantics for Cognitive Robotics: Towards Grounding Embodied Visuo-Locomotive Interactions",
        "abstract": "We present a commonsense, qualitative model for the semantic grounding of\nembodied visuo-spatial and locomotive interactions. The key contribution is an\nintegrative methodology combining low-level visual processing with high-level,\nhuman-centred representations of space and motion rooted in artificial\nintelligence. We demonstrate practical applicability with examples involving\nobject interactions, and indoor movement.",
        "url": "http://arxiv.org/pdf/1709.05293v1.pdf"
    },
    {
        "title": "A Streaming Accelerator for Deep Convolutional Neural Networks with Image and Feature Decomposition for Resource-limited System Applications",
        "abstract": "Deep convolutional neural networks (CNN) are widely used in modern artificial\nintelligence (AI) and smart vision systems but also limited by computation\nlatency, throughput, and energy efficiency on a resource-limited scenario, such\nas mobile devices, internet of things (IoT), unmanned aerial vehicles (UAV),\nand so on. A hardware streaming architecture is proposed to accelerate\nconvolution and pooling computations for state-of-the-art deep CNNs. It is\noptimized for energy efficiency by maximizing local data reuse to reduce\noff-chip DRAM data access. In addition, image and feature decomposition\ntechniques are introduced to optimize memory access pattern for an arbitrary\nsize of image and number of features within limited on-chip SRAM capacity. A\nprototype accelerator was implemented in TSMC 65 nm CMOS technology with 2.3 mm\nx 0.8 mm core area, which achieves 144 GOPS peak throughput and 0.8 TOPS/W peak\nenergy efficiency.",
        "url": "http://arxiv.org/pdf/1709.05116v1.pdf"
    },
    {
        "title": "Deep Reinforcement Learning for Conversational AI",
        "abstract": "Deep reinforcement learning is revolutionizing the artificial intelligence\nfield. Currently, it serves as a good starting point for constructing\nintelligent autonomous systems which offer a better knowledge of the visual\nworld. It is possible to scale deep reinforcement learning with the use of deep\nlearning and do amazing tasks such as use of pixels in playing video games. In\nthis paper, key concepts of deep reinforcement learning including reward\nfunction, differences between reinforcement learning and supervised learning\nand models for implementation of reinforcement are discussed. Key challenges\nrelated to the implementation of reinforcement learning in conversational AI\ndomain are identified as well as discussed in detail. Various conversational\nmodels which are based on deep reinforcement learning (as well as deep\nlearning) are also discussed. In summary, this paper discusses key aspects of\ndeep reinforcement learning which are crucial for designing an efficient\nconversational AI.",
        "url": "http://arxiv.org/pdf/1709.05067v1.pdf"
    },
    {
        "title": "Dynamic Capacity Estimation in Hopfield Networks",
        "abstract": "Understanding the memory capacity of neural networks remains a challenging\nproblem in implementing artificial intelligence systems. In this paper, we\naddress the notion of capacity with respect to Hopfield networks and propose a\ndynamic approach to monitoring a network's capacity. We define our\nunderstanding of capacity as the maximum number of stored patterns which can be\nretrieved when probed by the stored patterns. Prior work in this area has\npresented static expressions dependent on neuron count $N$, forcing network\ndesigners to assume worst-case input characteristics for bias and correlation\nwhen setting the capacity of the network. Instead, our model operates\nsimultaneously with the learning Hopfield network and concludes on a capacity\nestimate based on the patterns which were stored. By continuously updating the\ncrosstalk associated with the stored patterns, our model guards the network\nfrom overwriting its memory traces and exceeding its capacity. We simulate our\nmodel using artificially generated random patterns, which can be set to a\ndesired bias and correlation, and observe capacity estimates between 93% and\n97% accurate. As a result, our model doubles the memory efficiency of Hopfield\nnetworks in comparison to the static and worst-case capacity estimate while\nminimizing the risk of lost patterns.",
        "url": "http://arxiv.org/pdf/1709.05340v1.pdf"
    },
    {
        "title": "Abstractions for AI-Based User Interfaces and Systems",
        "abstract": "Novel user interfaces based on artificial intelligence, such as\nnatural-language agents, present new categories of engineering challenges.\nThese systems need to cope with uncertainty and ambiguity, interface with\nmachine learning algorithms, and compose information from multiple users to\nmake decisions. We propose to treat these challenges as language-design\nproblems. We describe three programming language abstractions for three core\nproblems in intelligent system design. First, hypothetical worlds support\nnondeterministic search over spaces of alternative actions. Second, a feature\ntype system abstracts the interaction between applications and learning\nalgorithms. Finally, constructs for collaborative execution extend hypothetical\nworlds across multiple machines while controlling access to private data. We\nenvision these features as first steps toward a complete language for\nimplementing AI-based interfaces and applications.",
        "url": "http://arxiv.org/pdf/1709.04991v1.pdf"
    },
    {
        "title": "Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games",
        "abstract": "Many artificial intelligence (AI) applications often require multiple\nintelligent agents to work in a collaborative effort. Efficient learning for\nintra-agent communication and coordination is an indispensable step towards\ngeneral AI. In this paper, we take StarCraft combat game as a case study, where\nthe task is to coordinate multiple agents as a team to defeat their enemies. To\nmaintain a scalable yet effective communication protocol, we introduce a\nMultiagent Bidirectionally-Coordinated Network (BiCNet ['bIknet]) with a\nvectorised extension of actor-critic formulation. We show that BiCNet can\nhandle different types of combats with arbitrary numbers of AI agents for both\nsides. Our analysis demonstrates that without any supervisions such as human\ndemonstrations or labelled data, BiCNet could learn various types of advanced\ncoordination strategies that have been commonly used by experienced game\nplayers. In our experiments, we evaluate our approach against multiple\nbaselines under different scenarios; it shows state-of-the-art performance, and\npossesses potential values for large-scale real-world applications.",
        "url": "http://arxiv.org/pdf/1703.10069v4.pdf"
    },
    {
        "title": "Perspectives for Evaluating Conversational AI",
        "abstract": "Conversational AI systems are becoming famous in day to day lives. In this\npaper, we are trying to address the following key question: To identify whether\ndesign, as well as development efforts for search oriented conversational AI\nare successful or not.It is tricky to define 'success' in the case of\nconversational AI and equally tricky part is to use appropriate metrics for the\nevaluation of conversational AI. We propose four different perspectives namely\nuser experience, information retrieval, linguistic and artificial intelligence\nfor the evaluation of conversational AI systems. Additionally, background\ndetails of conversational AI systems are provided including desirable\ncharacteristics of personal assistants, differences between chatbot and an AI\nbased personal assistant. An importance of personalization and how it can be\nachieved is explained in detail. Current challenges in the development of an\nideal conversational AI (personal assistant) are also highlighted along with\nguidelines for achieving personalized experience for users.",
        "url": "http://arxiv.org/pdf/1709.04734v1.pdf"
    },
    {
        "title": "Probability Reversal and the Disjunction Effect in Reasoning Systems",
        "abstract": "Data based judgments go into artificial intelligence applications but they\nundergo paradoxical reversal when seemingly unnecessary additional data is\nprovided. Examples of this are Simpson's reversal and the disjunction effect\nwhere the beliefs about the data change once it is presented or aggregated\ndifferently. Sometimes the significance of the difference can be evaluated\nusing statistical tests such as Pearson's chi-squared or Fisher's exact test,\nbut this may not be helpful in threshold-based decision systems that operate\nwith incomplete information. To mitigate risks in the use of algorithms in\ndecision-making, we consider the question of modeling of beliefs. We argue that\nevidence supports that beliefs are not classical statistical variables and they\nshould, in the general case, be considered as superposition states of disjoint\nor polar outcomes. We analyze the disjunction effect from the perspective of\nthe belief as a quantum vector.",
        "url": "http://arxiv.org/pdf/1709.04029v1.pdf"
    },
    {
        "title": "Machine learning \\& artificial intelligence in the quantum domain",
        "abstract": "Quantum information technologies, and intelligent learning systems, are both\nemergent technologies that will likely have a transforming impact on our\nsociety. The respective underlying fields of research -- quantum information\n(QI) versus machine learning (ML) and artificial intelligence (AI) -- have\ntheir own specific challenges, which have hitherto been investigated largely\nindependently. However, in a growing body of recent work, researchers have been\nprobing the question to what extent these fields can learn and benefit from\neach other. QML explores the interaction between quantum computing and ML,\ninvestigating how results and techniques from one field can be used to solve\nthe problems of the other. Recently, we have witnessed breakthroughs in both\ndirections of influence. For instance, quantum computing is finding a vital\napplication in providing speed-ups in ML, critical in our \"big data\" world.\nConversely, ML already permeates cutting-edge technologies, and may become\ninstrumental in advanced quantum technologies. Aside from quantum speed-up in\ndata analysis, or classical ML optimization used in quantum experiments,\nquantum enhancements have also been demonstrated for interactive learning,\nhighlighting the potential of quantum-enhanced learning agents. Finally, works\nexploring the use of AI for the very design of quantum experiments, and for\nperforming parts of genuine research autonomously, have reported their first\nsuccesses. Beyond the topics of mutual enhancement, researchers have also\nbroached the fundamental issue of quantum generalizations of ML/AI concepts.\nThis deals with questions of the very meaning of learning and intelligence in a\nworld that is described by quantum mechanics. In this review, we describe the\nmain ideas, recent developments, and progress in a broad spectrum of research\ninvestigating machine learning and artificial intelligence in the quantum\ndomain.",
        "url": "http://arxiv.org/pdf/1709.02779v1.pdf"
    },
    {
        "title": "TransG : A Generative Mixture Model for Knowledge Graph Embedding",
        "abstract": "Recently, knowledge graph embedding, which projects symbolic entities and\nrelations into continuous vector space, has become a new, hot topic in\nartificial intelligence. This paper addresses a new issue of multiple relation\nsemantics that a relation may have multiple meanings revealed by the entity\npairs associated with the corresponding triples, and proposes a novel Gaussian\nmixture model for embedding, TransG. The new model can discover latent\nsemantics for a relation and leverage a mixture of relation component vectors\nfor embedding a fact triple. To the best of our knowledge, this is the first\ngenerative model for knowledge graph embedding, which is able to deal with\nmultiple relation semantics. Extensive experiments show that the proposed model\nachieves substantial improvements against the state-of-the-art baselines.",
        "url": "http://arxiv.org/pdf/1509.05488v7.pdf"
    },
    {
        "title": "Learned Optimizers that Scale and Generalize",
        "abstract": "Learning to learn has emerged as an important direction for achieving\nartificial intelligence. Two of the primary barriers to its adoption are an\ninability to scale to larger problems and a limited ability to generalize to\nnew tasks. We introduce a learned gradient descent optimizer that generalizes\nwell to new tasks, and which has significantly reduced memory and computation\noverhead. We achieve this by introducing a novel hierarchical RNN architecture,\nwith minimal per-parameter overhead, augmented with additional architectural\nfeatures that mirror the known structure of optimization tasks. We also develop\na meta-training ensemble of small, diverse optimization tasks capturing common\nproperties of loss landscapes. The optimizer learns to outperform RMSProp/ADAM\non problems in this corpus. More importantly, it performs comparably or better\nwhen applied to small convolutional neural networks, despite seeing no neural\nnetworks in its meta-training set. Finally, it generalizes to train Inception\nV3 and ResNet V2 architectures on the ImageNet dataset for thousands of steps,\noptimization problems that are of a vastly different scale than those it was\ntrained on. We release an open source implementation of the meta-training\nalgorithm.",
        "url": "http://arxiv.org/pdf/1703.04813v4.pdf"
    },
    {
        "title": "Artificial Intelligence and Data Science in the Automotive Industry",
        "abstract": "Data science and machine learning are the key technologies when it comes to\nthe processes and products with automatic learning and optimization to be used\nin the automotive industry of the future. This article defines the terms \"data\nscience\" (also referred to as \"data analytics\") and \"machine learning\" and how\nthey are related. In addition, it defines the term \"optimizing analytics\" and\nillustrates the role of automatic optimization as a key technology in\ncombination with data analytics. It also uses examples to explain the way that\nthese technologies are currently being used in the automotive industry on the\nbasis of the major subprocesses in the automotive value chain (development,\nprocurement; logistics, production, marketing, sales and after-sales, connected\ncustomer). Since the industry is just starting to explore the broad range of\npotential uses for these technologies, visionary application examples are used\nto illustrate the revolutionary possibilities that they offer. Finally, the\narticle demonstrates how these technologies can make the automotive industry\nmore efficient and enhance its customer focus throughout all its operations and\nactivities, extending from the product and its development process to the\ncustomers and their connection to the product.",
        "url": "http://arxiv.org/pdf/1709.01989v1.pdf"
    },
    {
        "title": "Complexity Classification in Infinite-Domain Constraint Satisfaction",
        "abstract": "A constraint satisfaction problem (CSP) is a computational problem where the\ninput consists of a finite set of variables and a finite set of constraints,\nand where the task is to decide whether there exists a satisfying assignment of\nvalues to the variables. Depending on the type of constraints that we allow in\nthe input, a CSP might be tractable, or computationally hard. In recent years,\ngeneral criteria have been discovered that imply that a CSP is polynomial-time\ntractable, or that it is NP-hard. Finite-domain CSPs have become a major common\nresearch focus of graph theory, artificial intelligence, and finite model\ntheory. It turned out that the key questions for complexity classification of\nCSPs are closely linked to central questions in universal algebra.\n  This thesis studies CSPs where the variables can take values from an infinite\ndomain. This generalization enhances dramatically the range of computational\nproblems that can be modeled as a CSP. Many problems from areas that have so\nfar seen no interaction with constraint satisfaction theory can be formulated\nusing infinite domains, e.g. problems from temporal and spatial reasoning,\nphylogenetic reconstruction, and operations research.\n  It turns out that the universal-algebraic approach can also be applied to\nstudy large classes of infinite-domain CSPs, yielding elegant complexity\nclassification results. A new tool in this thesis that becomes relevant\nparticularly for infinite domains is Ramsey theory. We demonstrate the\nfeasibility of our approach with two complete complexity classification\nresults: one on CSPs in temporal reasoning, the other on a generalization of\nSchaefer's theorem for propositional logic to logic over graphs. We also study\nthe limits of complexity classification, and present classes of computational\nproblems provably do not exhibit a complexity dichotomy into hard and easy\nproblems.",
        "url": "http://arxiv.org/pdf/1201.0856v9.pdf"
    },
    {
        "title": "Gaussian Filter in CRF Based Semantic Segmentation",
        "abstract": "Artificial intelligence is making great changes in academy and industry with\nthe fast development of deep learning, which is a branch of machine learning\nand statistical learning. Fully convolutional network [1] is the standard model\nfor semantic segmentation. Conditional random fields coded as CNN [2] or RNN\n[3] and connected with FCN has been successfully applied in object detection\n[4]. In this paper, we introduce a multi-resolution neural network for FCN and\napply Gaussian filter to the extended CRF kernel neighborhood and the label\nimage to reduce the oscillating effect of CRF neural network segmentation, thus\nachieve higher precision and faster training speed.",
        "url": "http://arxiv.org/pdf/1709.00516v1.pdf"
    },
    {
        "title": "Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models",
        "abstract": "With the availability of large databases and recent improvements in deep\nlearning methodology, the performance of AI systems is reaching or even\nexceeding the human level on an increasing number of complex tasks. Impressive\nexamples of this development can be found in domains such as image\nclassification, sentiment analysis, speech understanding or strategic game\nplaying. However, because of their nested non-linear structure, these highly\nsuccessful machine learning and artificial intelligence models are usually\napplied in a black box manner, i.e., no information is provided about what\nexactly makes them arrive at their predictions. Since this lack of transparency\ncan be a major drawback, e.g., in medical applications, the development of\nmethods for visualizing, explaining and interpreting deep learning models has\nrecently attracted increasing attention. This paper summarizes recent\ndevelopments in this field and makes a plea for more interpretability in\nartificial intelligence. Furthermore, it presents two approaches to explaining\npredictions of deep learning models, one method which computes the sensitivity\nof the prediction with respect to changes in the input and one approach which\nmeaningfully decomposes the decision in terms of the input variables. These\nmethods are evaluated on three classification tasks.",
        "url": "http://arxiv.org/pdf/1708.08296v1.pdf"
    },
    {
        "title": "Electricity Theft Detection using Machine Learning",
        "abstract": "Non-technical losses (NTL) in electric power grids arise through electricity\ntheft, broken electric meters or billing errors. They can harm the power\nsupplier as well as the whole economy of a country through losses of up to 40%\nof the total power distribution. For NTL detection, researchers use artificial\nintelligence to analyse data. This work is about improving the extraction of\nmore meaningful features from a data set. With these features, the prediction\nquality will increase.",
        "url": "http://arxiv.org/pdf/1708.05907v1.pdf"
    },
    {
        "title": "BetaRun Soccer Simulation League Team: Variety, Complexity, and Learning",
        "abstract": "RoboCup offers a set of benchmark problems for Artificial Intelligence in\nform of official world championships since 1997. The most tactical advanced and\nrichest in terms of behavioural complexity of these is the 2D Soccer Simulation\nLeague, a simulated robotic soccer competition. BetaRun is a new attempt\ncombining both machine learning and manual programming approaches, with the\nultimate goal to arrive at a team that is trained entirely from observing and\nplaying games, and a new development based on agent2D.",
        "url": "http://arxiv.org/pdf/1703.04115v2.pdf"
    },
    {
        "title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning",
        "abstract": "When encountering novel objects, humans are able to infer a wide range of\nphysical properties such as mass, friction and deformability by interacting\nwith them in a goal driven way. This process of active interaction is in the\nsame spirit as a scientist performing experiments to discover hidden facts.\nRecent advances in artificial intelligence have yielded machines that can\nachieve superhuman performance in Go, Atari, natural language processing, and\ncomplex control problems; however, it is not clear that these systems can rival\nthe scientific intuition of even a young child. In this work we introduce a\nbasic set of tasks that require agents to estimate properties such as mass and\ncohesion of objects in an interactive simulated environment where they can\nmanipulate the objects and observe the consequences. We found that state of art\ndeep reinforcement learning methods can learn to perform the experiments\nnecessary to discover such hidden properties. By systematically manipulating\nthe problem difficulty and the cost incurred by the agent for performing\nexperiments, we found that agents learn different strategies that balance the\ncost of gathering information against the cost of making mistakes in different\nsituations.",
        "url": "http://arxiv.org/pdf/1611.01843v3.pdf"
    },
    {
        "title": "General AI Challenge - Round One: Gradual Learning",
        "abstract": "The General AI Challenge is an initiative to encourage the wider artificial\nintelligence community to focus on important problems in building intelligent\nmachines with more general scope than is currently possible. The challenge\ncomprises of multiple rounds, with the first round focusing on gradual\nlearning, i.e. the ability to re-use already learned knowledge for efficiently\nlearning to solve subsequent problems. In this article, we will present details\nof the first round of the challenge, its inspiration and aims. We also outline\na more formal description of the challenge and present a preliminary analysis\nof its curriculum, based on ideas from computational mechanics. We believe,\nthat such formalism will allow for a more principled approach towards\ninvestigating tasks in the challenge, building new curricula and for\npotentially improving consequent challenge rounds.",
        "url": "http://arxiv.org/pdf/1708.05346v1.pdf"
    },
    {
        "title": "mAnI: Movie Amalgamation using Neural Imitation",
        "abstract": "Cross-modal data retrieval has been the basis of various creative tasks\nperformed by Artificial Intelligence (AI). One such highly challenging task for\nAI is to convert a book into its corresponding movie, which most of the\ncreative film makers do as of today. In this research, we take the first step\ntowards it by visualizing the content of a book using its corresponding movie\nvisuals. Given a set of sentences from a book or even a fan-fiction written in\nthe same universe, we employ deep learning models to visualize the input by\nstitching together relevant frames from the movie. We studied and compared\nthree different types of setting to match the book with the movie content: (i)\nDialog model: using only the dialog from the movie, (ii) Visual model: using\nonly the visual content from the movie, and (iii) Hybrid model: using the\ndialog and the visual content from the movie. Experiments on the publicly\navailable MovieBook dataset shows the effectiveness of the proposed models.",
        "url": "http://arxiv.org/pdf/1708.04923v1.pdf"
    },
    {
        "title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey",
        "abstract": "Deep neural networks (DNNs) are currently widely used for many artificial\nintelligence (AI) applications including computer vision, speech recognition,\nand robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it\ncomes at the cost of high computational complexity. Accordingly, techniques\nthat enable efficient processing of DNNs to improve energy efficiency and\nthroughput without sacrificing application accuracy or increasing hardware cost\nare critical to the wide deployment of DNNs in AI systems.\n  This article aims to provide a comprehensive tutorial and survey about the\nrecent advances towards the goal of enabling efficient processing of DNNs.\nSpecifically, it will provide an overview of DNNs, discuss various hardware\nplatforms and architectures that support DNNs, and highlight key trends in\nreducing the computation cost of DNNs either solely via hardware design changes\nor via joint hardware design and DNN algorithm changes. It will also summarize\nvarious development resources that enable researchers and practitioners to\nquickly get started in this field, and highlight important benchmarking metrics\nand design considerations that should be used for evaluating the rapidly\ngrowing number of DNN hardware designs, optionally including algorithmic\nco-designs, being proposed in academia and industry.\n  The reader will take away the following concepts from this article:\nunderstand the key design considerations for DNNs; be able to evaluate\ndifferent DNN hardware implementations with benchmarks and comparison metrics;\nunderstand the trade-offs between various hardware architectures and platforms;\nbe able to evaluate the utility of various DNN design techniques for efficient\nprocessing; and understand recent implementation trends and opportunities.",
        "url": "http://arxiv.org/pdf/1703.09039v2.pdf"
    },
    {
        "title": "A System for Accessible Artificial Intelligence",
        "abstract": "While artificial intelligence (AI) has become widespread, many commercial AI\nsystems are not yet accessible to individual researchers nor the general public\ndue to the deep knowledge of the systems required to use them. We believe that\nAI has matured to the point where it should be an accessible technology for\neveryone. We present an ongoing project whose ultimate goal is to deliver an\nopen source, user-friendly AI system that is specialized for machine learning\nanalysis of complex data in the biomedical and health care domains. We discuss\nhow genetic programming can aid in this endeavor, and highlight specific\nexamples where genetic programming has automated machine learning analyses in\nprevious projects.",
        "url": "http://arxiv.org/pdf/1705.00594v2.pdf"
    },
    {
        "title": "Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge",
        "abstract": "This paper presents a state-of-the-art model for visual question answering\n(VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of\nsignificant importance for research in artificial intelligence, given its\nmultimodal nature, clear evaluation protocol, and potential real-world\napplications. The performance of deep neural networks for VQA is very dependent\non choices of architectures and hyperparameters. To help further research in\nthe area, we describe in detail our high-performing, though relatively simple\nmodel. Through a massive exploration of architectures and hyperparameters\nrepresenting more than 3,000 GPU-hours, we identified tips and tricks that lead\nto its success, namely: sigmoid outputs, soft training targets, image features\nfrom bottom-up attention, gated tanh activations, output embeddings initialized\nusing GloVe and Google Images, large mini-batches, and smart shuffling of\ntraining data. We provide a detailed analysis of their impact on performance to\nassist others in making an appropriate selection.",
        "url": "http://arxiv.org/pdf/1708.02711v1.pdf"
    },
    {
        "title": "One-Trial Correction of Legacy AI Systems and Stochastic Separation Theorems",
        "abstract": "We consider the problem of efficient \"on the fly\" tuning of existing, or {\\it\nlegacy}, Artificial Intelligence (AI) systems. The legacy AI systems are\nallowed to be of arbitrary class, albeit the data they are using for computing\ninterim or final decision responses should posses an underlying structure of a\nhigh-dimensional topological real vector space. The tuning method that we\npropose enables dealing with errors without the need to re-train the system.\nInstead of re-training a simple cascade of perceptron nodes is added to the\nlegacy system. The added cascade modulates the AI legacy system's decisions. If\napplied repeatedly, the process results in a network of modulating rules\n\"dressing up\" and improving performance of existing AI systems. Mathematical\nrationale behind the method is based on the fundamental property of measure\nconcentration in high dimensional spaces. The method is illustrated with an\nexample of fine-tuning a deep convolutional network that has been pre-trained\nto detect pedestrians in images.",
        "url": "http://arxiv.org/pdf/1610.00494v4.pdf"
    },
    {
        "title": "Identification of Probabilities",
        "abstract": "Within psychology, neuroscience and artificial intelligence, there has been\nincreasing interest in the proposal that the brain builds probabilistic models\nof sensory and linguistic input: that is, to infer a probabilistic model from a\nsample. The practical problems of such inference are substantial: the brain has\nlimited data and restricted computational resources. But there is a more\nfundamental question: is the problem of inferring a probabilistic model from a\nsample possible even in principle? We explore this question and find some\nsurprisingly positive and general results. First, for a broad class of\nprobability distributions characterised by computability restrictions, we\nspecify a learning algorithm that will almost surely identify a probability\ndistribution in the limit given a finite i.i.d. sample of sufficient but\nunknown length. This is similarly shown to hold for sequences generated by a\nbroad class of Markov chains, subject to computability assumptions. The\ntechnical tool is the strong law of large numbers. Second, for a large class of\ndependent sequences, we specify an algorithm which identifies in the limit a\ncomputable measure for which the sequence is typical, in the sense of\nMartin-Lof (there may be more than one such measure). The technical tool is the\ntheory of Kolmogorov complexity. We analyse the associated predictions in both\ncases. We also briefly consider special cases, including language learning, and\nwider theoretical implications for psychology.",
        "url": "http://arxiv.org/pdf/1708.01611v1.pdf"
    },
    {
        "title": "Stochastic Separation Theorems",
        "abstract": "The problem of non-iterative one-shot and non-destructive correction of\nunavoidable mistakes arises in all Artificial Intelligence applications in the\nreal world. Its solution requires robust separation of samples with errors from\nsamples where the system works properly. We demonstrate that in (moderately)\nhigh dimension this separation could be achieved with probability close to one\nby linear discriminants. Surprisingly, separation of a new image from a very\nlarge set of known images is almost always possible even in moderately high\ndimensions by linear functionals, and coefficients of these functionals can be\nfound explicitly. Based on fundamental properties of measure concentration, we\nshow that for $M<a\\exp(b{n})$ random $M$-element sets in $\\mathbb{R}^n$ are\nlinearly separable with probability $p$, $p>1-\\vartheta$, where $1>\\vartheta>0$\nis a given small constant. Exact values of $a,b>0$ depend on the probability\ndistribution that determines how the random $M$-element sets are drawn, and on\nthe constant $\\vartheta$. These {\\em stochastic separation theorems} provide a\nnew instrument for the development, analysis, and assessment of machine\nlearning methods and algorithms in high dimension. Theoretical statements are\nillustrated with numerical examples.",
        "url": "http://arxiv.org/pdf/1703.01203v3.pdf"
    },
    {
        "title": "A Novel Neural Network Model Specified for Representing Logical Relations",
        "abstract": "With computers to handle more and more complicated things in variable\nenvironments, it becomes an urgent requirement that the artificial intelligence\nhas the ability of automatic judging and deciding according to numerous\nspecific conditions so as to deal with the complicated and variable cases. ANNs\ninspired by brain is a good candidate. However, most of current numeric ANNs\nare not good at representing logical relations because these models still try\nto represent logical relations in the form of ratio based on functional\napproximation. On the other hand, researchers have been trying to design novel\nneural network models to make neural network model represent logical relations.\nIn this work, a novel neural network model specified for representing logical\nrelations is proposed and applied. New neurons and multiple kinds of links are\ndefined. Inhibitory links are introduced besides exciting links. Different from\ncurrent numeric ANNs, one end of an inhibitory link connects an exciting link\nrather than a neuron. Inhibitory links inhibit the connected exciting links\nconditionally to make this neural network model represent logical relations\ncorrectly. This model can simulate the operations of Boolean logic gates, and\nconstruct complex logical relations with the advantages of simpler neural\nnetwork structures than recent works in this area. This work provides some\nideas to make neural networks represent logical relations more directly and\nefficiently, and the model could be used as the complement to current numeric\nANN to deal with logical issues and expand the application areas of ANN.",
        "url": "http://arxiv.org/pdf/1708.00580v1.pdf"
    },
    {
        "title": "Multimodal Machine Learning: A Survey and Taxonomy",
        "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel\ntexture, smell odors, and taste flavors. Modality refers to the way in which\nsomething happens or is experienced and a research problem is characterized as\nmultimodal when it includes multiple such modalities. In order for Artificial\nIntelligence to make progress in understanding the world around us, it needs to\nbe able to interpret such multimodal signals together. Multimodal machine\nlearning aims to build models that can process and relate information from\nmultiple modalities. It is a vibrant multi-disciplinary field of increasing\nimportance and with extraordinary potential. Instead of focusing on specific\nmultimodal applications, this paper surveys the recent advances in multimodal\nmachine learning itself and presents them in a common taxonomy. We go beyond\nthe typical early and late fusion categorization and identify broader\nchallenges that are faced by multimodal machine learning, namely:\nrepresentation, translation, alignment, fusion, and co-learning. This new\ntaxonomy will enable researchers to better understand the state of the field\nand identify directions for future research.",
        "url": "http://arxiv.org/pdf/1705.09406v2.pdf"
    },
    {
        "title": "Using Thought-Provoking Children's Questions to Drive Artificial Intelligence Research",
        "abstract": "We propose to use thought-provoking children's questions (TPCQs), namely\nHighlights BrainPlay questions, as a new method to drive artificial\nintelligence research and to evaluate the capabilities of general-purpose AI\nsystems. These questions are designed to stimulate thought and learning in\nchildren, and they can be used to do the same thing in AI systems, while\ndemonstrating the system's reasoning capabilities to the evaluator. We\nintroduce the TPCQ task, which which takes a TPCQ question as input and\nproduces as output (1) answers to the question and (2) learned generalizations.\nWe discuss how BrainPlay questions stimulate learning. We analyze 244 BrainPlay\nquestions, and we report statistics on question type, question class, answer\ncardinality, answer class, types of knowledge needed, and types of reasoning\nneeded. We find that BrainPlay questions span many aspects of intelligence.\nBecause the answers to BrainPlay questions and the generalizations learned from\nthem are often highly open-ended, we suggest using human judges for evaluation.",
        "url": "http://arxiv.org/pdf/1508.06924v3.pdf"
    },
    {
        "title": "Proceedings Sixteenth Conference on Theoretical Aspects of Rationality and Knowledge",
        "abstract": "This volume consists of papers presented at the Sixteenth Conference on\nTheoretical Aspects of Rationality and Knowledge (TARK) held at the University\nof Liverpool, UK, from July 24 to 26, 2017.\n  TARK conferences bring together researchers from a wide variety of fields,\nincluding Computer Science (especially, Artificial Intelligence, Cryptography,\nDistributed Computing), Economics (especially, Decision Theory, Game Theory,\nSocial Choice Theory), Linguistics, Philosophy (especially, Philosophical\nLogic), and Cognitive Psychology, in order to further understand the issues\ninvolving reasoning about rationality and knowledge.",
        "url": "http://arxiv.org/pdf/1707.08250v1.pdf"
    },
    {
        "title": "The Challenge of Non-Technical Loss Detection using Artificial Intelligence: A Survey",
        "abstract": "Detection of non-technical losses (NTL) which include electricity theft,\nfaulty meters or billing errors has attracted increasing attention from\nresearchers in electrical engineering and computer science. NTLs cause\nsignificant harm to the economy, as in some countries they may range up to 40%\nof the total electricity distributed. The predominant research direction is\nemploying artificial intelligence to predict whether a customer causes NTL.\nThis paper first provides an overview of how NTLs are defined and their impact\non economies, which include loss of revenue and profit of electricity providers\nand decrease of the stability and reliability of electrical power grids. It\nthen surveys the state-of-the-art research efforts in a up-to-date and\ncomprehensive review of algorithms, features and data sets used. It finally\nidentifies the key scientific and engineering challenges in NTL detection and\nsuggests how they could be addressed in the future.",
        "url": "http://arxiv.org/pdf/1606.00626v3.pdf"
    },
    {
        "title": "Desensitized RDCA Subspaces for Compressive Privacy in Machine Learning",
        "abstract": "The quest for better data analysis and artificial intelligence has lead to\nmore and more data being collected and stored. As a consequence, more data are\nexposed to malicious entities. This paper examines the problem of privacy in\nmachine learning for classification. We utilize the Ridge Discriminant\nComponent Analysis (RDCA) to desensitize data with respect to a privacy label.\nBased on five experiments, we show that desensitization by RDCA can effectively\nprotect privacy (i.e. low accuracy on the privacy label) with small loss in\nutility. On HAR and CMU Faces datasets, the use of desensitized data results in\nrandom guess level accuracies for privacy at a cost of 5.14% and 0.04%, on\naverage, drop in the utility accuracies. For Semeion Handwritten Digit dataset,\naccuracies of the privacy-sensitive digits are almost zero, while the\naccuracies for the utility-relevant digits drop by 7.53% on average. This\npresents a promising solution to the problem of privacy in machine learning for\nclassification.",
        "url": "http://arxiv.org/pdf/1707.07770v1.pdf"
    },
    {
        "title": "Her2 Challenge Contest: A Detailed Assessment of Automated Her2 Scoring Algorithms in Whole Slide Images of Breast Cancer Tissues",
        "abstract": "Evaluating expression of the Human epidermal growth factor receptor 2 (Her2)\nby visual examination of immunohistochemistry (IHC) on invasive breast cancer\n(BCa) is a key part of the diagnostic assessment of BCa due to its recognised\nimportance as a predictive and prognostic marker in clinical practice. However,\nvisual scoring of Her2 is subjective and consequently prone to inter-observer\nvariability. Given the prognostic and therapeutic implications of Her2 scoring,\na more objective method is required. In this paper, we report on a recent\nautomated Her2 scoring contest, held in conjunction with the annual PathSoc\nmeeting held in Nottingham in June 2016, aimed at systematically comparing and\nadvancing the state-of-the-art Artificial Intelligence (AI) based automated\nmethods for Her2 scoring. The contest dataset comprised of digitised whole\nslide images (WSI) of sections from 86 cases of invasive breast carcinoma\nstained with both Haematoxylin & Eosin (H&E) and IHC for Her2. The contesting\nalgorithms automatically predicted scores of the IHC slides for an unseen\nsubset of the dataset and the predicted scores were compared with the 'ground\ntruth' (a consensus score from at least two experts). We also report on a\nsimple Man vs Machine contest for the scoring of Her2 and show that the\nautomated methods could beat the pathology experts on this contest dataset.\nThis paper presents a benchmark for comparing the performance of automated\nalgorithms for scoring of Her2. It also demonstrates the enormous potential of\nautomated algorithms in assisting the pathologist with objective IHC scoring.",
        "url": "http://arxiv.org/pdf/1705.08369v3.pdf"
    },
    {
        "title": "Guidelines for Artificial Intelligence Containment",
        "abstract": "With almost daily improvements in capabilities of artificial intelligence it\nis more important than ever to develop safety software for use by the AI\nresearch community. Building on our previous work on AI Containment Problem we\npropose a number of guidelines which should help AI safety researchers to\ndevelop reliable sandboxing software for intelligent programs of all levels.\nSuch safety container software will make it possible to study and analyze\nintelligent artificial agent while maintaining certain level of safety against\ninformation leakage, social engineering attacks and cyberattacks from within\nthe container.",
        "url": "http://arxiv.org/pdf/1707.08476v1.pdf"
    },
    {
        "title": "A study on text-score disagreement in online reviews",
        "abstract": "In this paper, we focus on online reviews and employ artificial intelligence\ntools, taken from the cognitive computing field, to help understanding the\nrelationships between the textual part of the review and the assigned numerical\nscore. We move from the intuitions that 1) a set of textual reviews expressing\ndifferent sentiments may feature the same score (and vice-versa); and 2)\ndetecting and analyzing the mismatches between the review content and the\nactual score may benefit both service providers and consumers, by highlighting\nspecific factors of satisfaction (and dissatisfaction) in texts.\n  To prove the intuitions, we adopt sentiment analysis techniques and we\nconcentrate on hotel reviews, to find polarity mismatches therein. In\nparticular, we first train a text classifier with a set of annotated hotel\nreviews, taken from the Booking website. Then, we analyze a large dataset, with\naround 160k hotel reviews collected from Tripadvisor, with the aim of detecting\na polarity mismatch, indicating if the textual content of the review is in\nline, or not, with the associated score.\n  Using well established artificial intelligence techniques and analyzing in\ndepth the reviews featuring a mismatch between the text polarity and the score,\nwe find that -on a scale of five stars- those reviews ranked with middle scores\ninclude a mixture of positive and negative aspects.\n  The approach proposed here, beside acting as a polarity detector, provides an\neffective selection of reviews -on an initial very large dataset- that may\nallow both consumers and providers to focus directly on the review subset\nfeaturing a text/score disagreement, which conveniently convey to the user a\nsummary of positive and negative features of the review target.",
        "url": "http://arxiv.org/pdf/1707.06932v1.pdf"
    },
    {
        "title": "Towards learning domain-independent planning heuristics",
        "abstract": "Automated planning remains one of the most general paradigms in Artificial\nIntelligence, providing means of solving problems coming from a wide variety of\ndomains. One of the key factors restricting the applicability of planning is\nits computational complexity resulting from exponentially large search spaces.\nHeuristic approaches are necessary to solve all but the simplest problems. In\nthis work, we explore the possibility of obtaining domain-independent heuristic\nfunctions using machine learning. This is a part of a wider research program\nwhose objective is to improve practical applicability of planning in systems\nfor which the planning domains evolve at run time. The challenge is therefore\nthe learning of (corrections of) domain-independent heuristics that can be\nreused across different planning domains.",
        "url": "http://arxiv.org/pdf/1707.06895v1.pdf"
    },
    {
        "title": "The Predictron: End-To-End Learning and Planning",
        "abstract": "One of the key challenges of artificial intelligence is to learn models that\nare effective in the context of planning. In this document we introduce the\npredictron architecture. The predictron consists of a fully abstract model,\nrepresented by a Markov reward process, that can be rolled forward multiple\n\"imagined\" planning steps. Each forward pass of the predictron accumulates\ninternal rewards and values over multiple planning depths. The predictron is\ntrained end-to-end so as to make these accumulated values accurately\napproximate the true value function. We applied the predictron to procedurally\ngenerated random mazes and a simulator for the game of pool. The predictron\nyielded significantly more accurate predictions than conventional deep neural\nnetwork architectures.",
        "url": "http://arxiv.org/pdf/1612.08810v3.pdf"
    },
    {
        "title": "Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science",
        "abstract": "Through the success of deep learning in various domains, artificial neural\nnetworks are currently among the most used artificial intelligence methods.\nTaking inspiration from the network properties of biological neural networks\n(e.g. sparsity, scale-freeness), we argue that (contrary to general practice)\nartificial neural networks, too, should not have fully-connected layers. Here\nwe propose sparse evolutionary training of artificial neural networks, an\nalgorithm which evolves an initial sparse topology (Erd\\H{o}s-R\\'enyi random\ngraph) of two consecutive layers of neurons into a scale-free topology, during\nlearning. Our method replaces artificial neural networks fully-connected layers\nwith sparse ones before training, reducing quadratically the number of\nparameters, with no decrease in accuracy. We demonstrate our claims on\nrestricted Boltzmann machines, multi-layer perceptrons, and convolutional\nneural networks for unsupervised and supervised learning on 15 datasets. Our\napproach has the potential to enable artificial neural networks to scale up\nbeyond what is currently possible.",
        "url": "http://arxiv.org/pdf/1707.04780v2.pdf"
    },
    {
        "title": "Advances in Artificial Intelligence Require Progress Across all of Computer Science",
        "abstract": "Advances in Artificial Intelligence require progress across all of computer\nscience.",
        "url": "http://arxiv.org/pdf/1707.04352v1.pdf"
    },
    {
        "title": "Large-scale Video Classification guided by Batch Normalized LSTM Translator",
        "abstract": "Youtube-8M dataset enhances the development of large-scale video recognition\ntechnology as ImageNet dataset has encouraged image classification, recognition\nand detection of artificial intelligence fields. For this large video dataset,\nit is a challenging task to classify a huge amount of multi-labels. By change\nof perspective, we propose a novel method by regarding labels as words. In\ndetails, we describe online learning approaches to multi-label video\nclassification that are guided by deep recurrent neural networks for video to\nsentence translator. We designed the translator based on LSTMs and found out\nthat a stochastic gating before the input of each LSTM cell can help us to\ndesign the structural details. In addition, we adopted batch normalizations\ninto our models to improve our LSTM models. Since our models are feature\nextractors, they can be used with other classifiers. Finally we report improved\nvalidation results of our models on large-scale Youtube-8M datasets and\ndiscussions for the further improvement.",
        "url": "http://arxiv.org/pdf/1707.04045v1.pdf"
    },
    {
        "title": "Learning Photography Aesthetics with Deep CNNs",
        "abstract": "Automatic photo aesthetic assessment is a challenging artificial intelligence\ntask. Existing computational approaches have focused on modeling a single\naesthetic score or a class (good or bad), however these do not provide any\ndetails on why the photograph is good or bad, or which attributes contribute to\nthe quality of the photograph. To obtain both accuracy and human interpretation\nof the score, we advocate learning the aesthetic attributes along with the\nprediction of the overall score. For this purpose, we propose a novel multitask\ndeep convolution neural network, which jointly learns eight aesthetic\nattributes along with the overall aesthetic score. We report near human\nperformance in the prediction of the overall aesthetic score. To understand the\ninternal representation of these attributes in the learned model, we also\ndevelop the visualization technique using back propagation of gradients. These\nvisualizations highlight the important image regions for the corresponding\nattributes, thus providing insights about model's representation of these\nattributes. We showcase the diversity and complexity associated with different\nattributes through a qualitative analysis of the activation maps.",
        "url": "http://arxiv.org/pdf/1707.03981v1.pdf"
    },
    {
        "title": "Learning Macromanagement in StarCraft from Replays using Deep Learning",
        "abstract": "The real-time strategy game StarCraft has proven to be a challenging\nenvironment for artificial intelligence techniques, and as a result, current\nstate-of-the-art solutions consist of numerous hand-crafted modules. In this\npaper, we show how macromanagement decisions in StarCraft can be learned\ndirectly from game replays using deep learning. Neural networks are trained on\n789,571 state-action pairs extracted from 2,005 replays of highly skilled\nplayers, achieving top-1 and top-3 error rates of 54.6% and 22.9% in predicting\nthe next build action. By integrating the trained network into UAlbertaBot, an\nopen source StarCraft bot, the system can significantly outperform the game's\nbuilt-in Terran bot, and play competitively against UAlbertaBot with a fixed\nrush strategy. To our knowledge, this is the first time macromanagement tasks\nare learned directly from replays in StarCraft. While the best hand-crafted\nstrategies are still the state-of-the-art, the deep network approach is able to\nexpress a wide range of different strategies and thus improving the network's\nperformance further with deep reinforcement learning is an immediately\npromising avenue for future research. Ultimately this approach could lead to\nstrong StarCraft bots that are less reliant on hard-coded strategies.",
        "url": "http://arxiv.org/pdf/1707.03743v1.pdf"
    },
    {
        "title": "The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously",
        "abstract": "This paper introduces the Intentional Unintentional (IU) agent. This agent\nendows the deep deterministic policy gradients (DDPG) agent for continuous\ncontrol with the ability to solve several tasks simultaneously. Learning to\nsolve many tasks simultaneously has been a long-standing, core goal of\nartificial intelligence, inspired by infant development and motivated by the\ndesire to build flexible robot manipulators capable of many diverse behaviours.\nWe show that the IU agent not only learns to solve many tasks simultaneously\nbut it also learns faster than agents that target a single task at-a-time. In\nsome cases, where the single task DDPG method completely fails, the IU agent\nsuccessfully solves the task. To demonstrate this, we build a playroom\nenvironment using the MuJoCo physics engine, and introduce a grounded formal\nlanguage to automatically generate tasks.",
        "url": "http://arxiv.org/pdf/1707.03300v1.pdf"
    },
    {
        "title": "A Machine Learning Based Intrusion Detection System for Software Defined 5G Network",
        "abstract": "As an inevitable trend of future 5G networks, Software Defined architecture\nhas many advantages in providing central- ized control and flexible resource\nmanagement. But it is also confronted with various security challenges and\npotential threats with emerging services and technologies. As the focus of\nnetwork security, Intrusion Detection Systems (IDS) are usually deployed\nseparately without collaboration. They are also unable to detect novel attacks\nwith limited intelligent abilities, which are hard to meet the needs of\nsoftware defined 5G. In this paper, we propose an intelligent intrusion system\ntaking the advances of software defined technology and artificial intelligence\nbased on Software Defined 5G architecture. It flexibly combines security\nfunction mod- ules which are adaptively invoked under centralized management\nand control with a globle view. It can also deal with unknown intrusions by\nusing machine learning algorithms. Evaluation results prove that the\nintelligent intrusion detection system achieves a better performance.",
        "url": "http://arxiv.org/pdf/1708.04571v1.pdf"
    },
    {
        "title": "Neural Machine Translation between Herbal Prescriptions and Diseases",
        "abstract": "The current study applies deep learning to herbalism. Toward the goal, we\nacquired the de-identified health insurance reimbursements that were claimed in\na 10-year period from 2004 to 2013 in the National Health Insurance Database of\nTaiwan, the total number of reimbursement records equaling 340 millions. Two\nartificial intelligence techniques were applied to the dataset: residual\nconvolutional neural network multitask classifier and attention-based recurrent\nneural network. The former works to translate from herbal prescriptions to\ndiseases; and the latter from diseases to herbal prescriptions. Analysis of the\nclassification results indicates that herbal prescriptions are specific to:\nanatomy, pathophysiology, sex and age of the patient, and season and year of\nthe prescription. Further analysis identifies temperature and gross domestic\nproduct as the meteorological and socioeconomic factors that are associated\nwith herbal prescriptions. Analysis of the neural machine transitional result\nindicates that the recurrent neural network learnt not only syntax but also\nsemantics of diseases and herbal prescriptions.",
        "url": "http://arxiv.org/pdf/1707.02575v1.pdf"
    },
    {
        "title": "Machine Learning, Deepest Learning: Statistical Data Assimilation Problems",
        "abstract": "We formulate a strong equivalence between machine learning, artificial\nintelligence methods and the formulation of statistical data assimilation as\nused widely in physical and biological sciences. The correspondence is that\nlayer number in the artificial network setting is the analog of time in the\ndata assimilation setting. Within the discussion of this equivalence we show\nthat adding more layers (making the network deeper) is analogous to adding\ntemporal resolution in a data assimilation framework.\n  How one can find a candidate for the global minimum of the cost functions in\nthe machine learning context using a method from data assimilation is\ndiscussed. Calculations on simple models from each side of the equivalence are\nreported.\n  Also discussed is a framework in which the time or layer label is taken to be\ncontinuous, providing a differential equation, the Euler-Lagrange equation,\nwhich shows that the problem being solved is a two point boundary value problem\nfamiliar in the discussion of variational methods. The use of continuous layers\nis denoted \"deepest learning\". These problems respect a symplectic symmetry in\ncontinuous time/layer phase space. Both Lagrangian versions and Hamiltonian\nversions of these problems are presented. Their well-studied implementation in\na discrete time/layer, while respected the symplectic structure, is addressed.\nThe Hamiltonian version provides a direct rationale for back propagation as a\nsolution method for the canonical momentum.",
        "url": "http://arxiv.org/pdf/1707.01415v1.pdf"
    },
    {
        "title": "Applying Deep Machine Learning for psycho-demographic profiling of Internet users using O.C.E.A.N. model of personality",
        "abstract": "In the modern era, each Internet user leaves enormous amounts of auxiliary\ndigital residuals (footprints) by using a variety of on-line services. All this\ndata is already collected and stored for many years. In recent works, it was\ndemonstrated that it's possible to apply simple machine learning methods to\nanalyze collected digital footprints and to create psycho-demographic profiles\nof individuals. However, while these works clearly demonstrated the\napplicability of machine learning methods for such an analysis, created simple\nprediction models still lacks accuracy necessary to be successfully applied for\npractical needs. We have assumed that using advanced deep machine learning\nmethods may considerably increase the accuracy of predictions. We started with\nsimple machine learning methods to estimate basic prediction performance and\nmoved further by applying advanced methods based on shallow and deep neural\nnetworks. Then we compared prediction power of studied models and made\nconclusions about its performance. Finally, we made hypotheses how prediction\naccuracy can be further improved. As result of this work, we provide full\nsource code used in the experiments for all interested researchers and\npractitioners in corresponding GitHub repository. We believe that applying deep\nmachine learning for psycho-demographic profiling may have an enormous impact\non the society (for good or worse) and provides means for Artificial\nIntelligence (AI) systems to better understand humans by creating their\npsychological profiles. Thus AI agents may achieve the human-like ability to\nparticipate in conversation (communication) flow by anticipating human\nopponents' reactions, expectations, and behavior.",
        "url": "http://arxiv.org/pdf/1703.06914v2.pdf"
    },
    {
        "title": "OPEB: Open Physical Environment Benchmark for Artificial Intelligence",
        "abstract": "Artificial Intelligence methods to solve continuous- control tasks have made\nsignificant progress in recent years. However, these algorithms have important\nlimitations and still need significant improvement to be used in industry and\nreal- world applications. This means that this area is still in an active\nresearch phase. To involve a large number of research groups, standard\nbenchmarks are needed to evaluate and compare proposed algorithms. In this\npaper, we propose a physical environment benchmark framework to facilitate\ncollaborative research in this area by enabling different research groups to\nintegrate their designed benchmarks in a unified cloud-based repository and\nalso share their actual implemented benchmarks via the cloud. We demonstrate\nthe proposed framework using an actual implementation of the classical\nmountain-car example and present the results obtained using a Reinforcement\nLearning algorithm.",
        "url": "http://arxiv.org/pdf/1707.00790v1.pdf"
    },
    {
        "title": "Modeling preference time in middle distance triathlons",
        "abstract": "Modeling preference time in triathlons means predicting the intermediate\ntimes of particular sports disciplines by a given overall finish time in a\nspecific triathlon course for the athlete with the known personal best result.\nThis is a hard task for athletes and sport trainers due to a lot of different\nfactors that need to be taken into account, e.g., athlete's abilities, health,\nmental preparations and even their current sports form. So far, this process\nwas calculated manually without any specific software tools or using the\nartificial intelligence. This paper presents the new solution for modeling\npreference time in middle distance triathlons based on particle swarm\noptimization algorithm and archive of existing sports results. Initial results\nare presented, which suggest the usefulness of proposed approach, while remarks\nfor future improvements and use are also emphasized.",
        "url": "http://arxiv.org/pdf/1707.00718v1.pdf"
    },
    {
        "title": "Elementary epistemological features of machine intelligence",
        "abstract": "Theoretical analysis of machine intelligence (MI) is useful for defining a\ncommon platform in both theoretical and applied artificial intelligence (AI).\nThe goal of this paper is to set canonical definitions that can assist\npragmatic research in both strong and weak AI. Described epistemological\nfeatures of machine intelligence include relationship between intelligent\nbehavior, intelligent and unintelligent machine characteristics, observable and\nunobservable entities and classification of intelligence. The paper also\nestablishes algebraic definitions of efficiency and accuracy of MI tests as\ntheir quality measure. The last part of the paper addresses the learning\nprocess with respect to the traditional epistemology and the epistemology of MI\ndescribed here. The proposed views on MI positively correlate to the Hegelian\nmonistic epistemology and contribute towards amalgamating idealistic\ndeliberations with the AI theory, particularly in a local frame of reference.",
        "url": "http://arxiv.org/pdf/0812.0885v4.pdf"
    },
    {
        "title": "A Roadmap for the Development of the \"SP Machine\" for Artificial Intelligence",
        "abstract": "This paper describes a roadmap for the development of the \"SP Machine\", based\non the \"SP Theory of Intelligence\" and its realisation in the \"SP Computer\nModel\". The SP Machine will be developed initially as a software virtual\nmachine with high levels of parallel processing, hosted on a high-performance\ncomputer. The system should help users visualise knowledge structures and\nprocessing. Research is needed into how the system may discover low-level\nfeatures in speech and in images. Strengths of the SP System in the processing\nof natural language may be augmented, in conjunction with the further\ndevelopment of the SP System's strengths in unsupervised learning. Strengths of\nthe SP System in pattern recognition may be developed for computer vision. Work\nis needed on the representation of numbers and the performance of arithmetic\nprocesses. A computer model is needed of \"SP-Neural\", the version of the SP\nTheory expressed in terms of neurons and their inter-connections. The SP\nMachine has potential in many areas of application, several of which may be\nrealised on short-to-medium timescales.",
        "url": "http://arxiv.org/pdf/1707.00614v3.pdf"
    },
    {
        "title": "Grounded Language Learning in a Simulated 3D World",
        "abstract": "We are increasingly surrounded by artificially intelligent technology that\ntakes decisions and executes actions on our behalf. This creates a pressing\nneed for general means to communicate with, instruct and guide artificial\nagents, with human language the most compelling means for such communication.\nTo achieve this in a scalable fashion, agents must be able to relate language\nto the world and to actions; that is, their understanding of language must be\ngrounded and embodied. However, learning grounded language is a notoriously\nchallenging problem in artificial intelligence research. Here we present an\nagent that learns to interpret language in a simulated 3D environment where it\nis rewarded for the successful execution of written instructions. Trained via a\ncombination of reinforcement and unsupervised learning, and beginning with\nminimal prior knowledge, the agent learns to relate linguistic symbols to\nemergent perceptual representations of its physical surroundings and to\npertinent sequences of actions. The agent's comprehension of language extends\nbeyond its prior experience, enabling it to apply familiar language to\nunfamiliar situations and to interpret entirely novel instructions. Moreover,\nthe speed with which this agent learns new words increases as its semantic\nknowledge grows. This facility for generalising and bootstrapping semantic\nknowledge indicates the potential of the present approach for reconciling\nambiguous natural language with the complexity of the physical world.",
        "url": "http://arxiv.org/pdf/1706.06551v2.pdf"
    },
    {
        "title": "Preserving Intermediate Objectives: One Simple Trick to Improve Learning for Hierarchical Models",
        "abstract": "Hierarchical models are utilized in a wide variety of problems which are\ncharacterized by task hierarchies, where predictions on smaller subtasks are\nuseful for trying to predict a final task. Typically, neural networks are first\ntrained for the subtasks, and the predictions of these networks are\nsubsequently used as additional features when training a model and doing\ninference for a final task. In this work, we focus on improving learning for\nsuch hierarchical models and demonstrate our method on the task of speaker\ntrait prediction. Speaker trait prediction aims to computationally identify\nwhich personality traits a speaker might be perceived to have, and has been of\ngreat interest to both the Artificial Intelligence and Social Science\ncommunities. Persuasiveness prediction in particular has been of interest, as\npersuasive speakers have a large amount of influence on our thoughts, opinions\nand beliefs. In this work, we examine how leveraging the relationship between\nrelated speaker traits in a hierarchical structure can help improve our ability\nto predict how persuasive a speaker is. We present a novel algorithm that\nallows us to backpropagate through this hierarchy. This hierarchical model\nachieves a 25% relative error reduction in classification accuracy over current\nstate-of-the art methods on the publicly available POM dataset.",
        "url": "http://arxiv.org/pdf/1706.07867v1.pdf"
    },
    {
        "title": "A Useful Motif for Flexible Task Learning in an Embodied Two-Dimensional Visual Environment",
        "abstract": "Animals (especially humans) have an amazing ability to learn new tasks\nquickly, and switch between them flexibly. How brains support this ability is\nlargely unknown, both neuroscientifically and algorithmically. One reasonable\nsupposition is that modules drawing on an underlying general-purpose sensory\nrepresentation are dynamically allocated on a per-task basis. Recent results\nfrom neuroscience and artificial intelligence suggest the role of the general\npurpose visual representation may be played by a deep convolutional neural\nnetwork, and give some clues how task modules based on such a representation\nmight be discovered and constructed. In this work, we investigate module\narchitectures in an embodied two-dimensional touchscreen environment, in which\nan agent's learning must occur via interactions with an environment that emits\nimages and rewards, and accepts touches as input. This environment is designed\nto capture the physical structure of the task environments that are commonly\ndeployed in visual neuroscience and psychophysics. We show that in this\ncontext, very simple changes in the nonlinear activations used by such a module\ncan significantly influence how fast it is at learning visual tasks and how\nsuitable it is for switching to new tasks.",
        "url": "http://arxiv.org/pdf/1706.07147v1.pdf"
    },
    {
        "title": "Expert and Non-Expert Opinion about Technological Unemployment",
        "abstract": "There is significant concern that technological advances, especially in\nRobotics and Artificial Intelligence (AI), could lead to high levels of\nunemployment in the coming decades. Studies have estimated that around half of\nall current jobs are at risk of automation. To look into this issue in more\ndepth, we surveyed experts in Robotics and AI about the risk, and compared\ntheir views with those of non-experts. Whilst the experts predicted a\nsignificant number of occupations were at risk of automation in the next two\ndecades, they were more cautious than people outside the field in predicting\noccupations at risk. Their predictions were consistent with their estimates for\nwhen computers might be expected to reach human level performance across a wide\nrange of skills. These estimates were typically decades later than those of the\nnon-experts. Technological barriers may therefore provide society with more\ntime to prepare for an automated future than the public fear. In addition,\npublic expectations may need to be dampened about the speed of progress to be\nexpected in Robotics and AI.",
        "url": "http://arxiv.org/pdf/1706.06906v1.pdf"
    },
    {
        "title": "Structured Best Arm Identification with Fixed Confidence",
        "abstract": "We study the problem of identifying the best action among a set of possible\noptions when the value of each action is given by a mapping from a number of\nnoisy micro-observables in the so-called fixed confidence setting. Our main\nmotivation is the application to the minimax game search, which has been a\nmajor topic of interest in artificial intelligence. In this paper we introduce\nan abstract setting to clearly describe the essential properties of the\nproblem. While previous work only considered a two-move game tree search\nproblem, our abstract setting can be applied to the general minimax games where\nthe depth can be non-uniform and arbitrary, and transpositions are allowed. We\nintroduce a new algorithm (LUCB-micro) for the abstract setting, and give its\nlower and upper sample complexity results. Our bounds recover some previous\nresults, which were only available in more limited settings, while they also\nshed further light on how the structure of minimax problems influence sample\ncomplexity.",
        "url": "http://arxiv.org/pdf/1706.05198v2.pdf"
    },
    {
        "title": "Improving Scalability of Inductive Logic Programming via Pruning and Best-Effort Optimisation",
        "abstract": "Inductive Logic Programming (ILP) combines rule-based and statistical\nartificial intelligence methods, by learning a hypothesis comprising a set of\nrules given background knowledge and constraints for the search space. We focus\non extending the XHAIL algorithm for ILP which is based on Answer Set\nProgramming and we evaluate our extensions using the Natural Language\nProcessing application of sentence chunking. With respect to processing natural\nlanguage, ILP can cater for the constant change in how we use language on a\ndaily basis. At the same time, ILP does not require huge amounts of training\nexamples such as other statistical methods and produces interpretable results,\nthat means a set of rules, which can be analysed and tweaked if necessary. As\ncontributions we extend XHAIL with (i) a pruning mechanism within the\nhypothesis generalisation algorithm which enables learning from larger\ndatasets, (ii) a better usage of modern solver technology using recently\ndeveloped optimisation methods, and (iii) a time budget that permits the usage\nof suboptimal results. We evaluate these improvements on the task of sentence\nchunking using three datasets from a recent SemEval competition. Results show\nthat our improvements allow for learning on bigger datasets with results that\nare of similar quality to state-of-the-art systems on the same task. Moreover,\nwe compare the hypotheses obtained on datasets to gain insights on the\nstructure of each dataset.",
        "url": "http://arxiv.org/pdf/1706.05171v1.pdf"
    },
    {
        "title": "AI-Powered Social Bots",
        "abstract": "This paper gives an overview of impersonation bots that generate output in\none, or possibly, multiple modalities. We also discuss rapidly advancing areas\nof machine learning and artificial intelligence that could lead to\nfrighteningly powerful new multi-modal social bots. Our main conclusion is that\nmost commonly known bots are one dimensional (i.e., chatterbot), and far from\ndeceiving serious interrogators. However, using recent advances in machine\nlearning, it is possible to unleash incredibly powerful, human-like armies of\nsocial bots, in potentially well coordinated campaigns of deception and\ninfluence.",
        "url": "http://arxiv.org/pdf/1706.05143v1.pdf"
    },
    {
        "title": "Accelerating Exact and Approximate Inference for (Distributed) Discrete Optimization with GPUs",
        "abstract": "Discrete optimization is a central problem in artificial intelligence. The\noptimization of the aggregated cost of a network of cost functions arises in a\nvariety of problems including (W)CSP, DCOP, as well as optimization in\nstochastic variants such as the tasks of finding the most probable explanation\n(MPE) in belief networks. Inference-based algorithms are powerful techniques\nfor solving discrete optimization problems, which can be used independently or\nin combination with other techniques. However, their applicability is often\nlimited by their compute intensive nature and their space requirements. This\npaper proposes the design and implementation of a novel inference-based\ntechnique, which exploits modern massively parallel architectures, such as\nthose found in Graphical Processing Units (GPUs), to speed up the resolution of\nexact and approximated inference-based algorithms for discrete optimization.\nThe paper studies the proposed algorithm in both centralized and distributed\noptimization contexts. The paper demonstrates that the use of GPUs provides\nsignificant advantages in terms of runtime and scalability, achieving up to two\norders of magnitude in speedups and showing a considerable reduction in\nexecution time (up to 345 times faster) with respect to a sequential version.",
        "url": "http://arxiv.org/pdf/1608.05288v2.pdf"
    },
    {
        "title": "A New Probabilistic Algorithm for Approximate Model Counting",
        "abstract": "Constrained counting is important in domains ranging from artificial\nintelligence to software analysis. There are already a few approaches for\ncounting models over various types of constraints. Recently, hashing-based\napproaches achieve both theoretical guarantees and scalability, but still rely\non solution enumeration. In this paper, a new probabilistic polynomial time\napproximate model counter is proposed, which is also a hashing-based universal\nframework, but with only satisfiability queries. A variant with a dynamic\nstopping criterion is also presented. Empirical evaluation over benchmarks on\npropositional logic formulas and SMT(BV) formulas shows that the approach is\npromising.",
        "url": "http://arxiv.org/pdf/1706.03906v1.pdf"
    },
    {
        "title": "Decoupling Learning Rules from Representations",
        "abstract": "In the artificial intelligence field, learning often corresponds to changing\nthe parameters of a parameterized function. A learning rule is an algorithm or\nmathematical expression that specifies precisely how the parameters should be\nchanged. When creating an artificial intelligence system, we must make two\ndecisions: what representation should be used (i.e., what parameterized\nfunction should be used) and what learning rule should be used to search\nthrough the resulting set of representable functions. Using most learning\nrules, these two decisions are coupled in a subtle (and often unintentional)\nway. That is, using the same learning rule with two different representations\nthat can represent the same sets of functions can result in two different\noutcomes. After arguing that this coupling is undesirable, particularly when\nusing artificial neural networks, we present a method for partially decoupling\nthese two decisions for a broad class of learning rules that span unsupervised\nlearning, reinforcement learning, and supervised learning.",
        "url": "http://arxiv.org/pdf/1706.03100v1.pdf"
    },
    {
        "title": "A Tutor Agent for MOBA Games",
        "abstract": "Digital games have become a key player in the entertainment industry,\nattracting millions of new players each year. In spite of that, novice players\nmay have a hard time when playing certain types of games, such as MOBAs and\nMMORPGs, due to their steep learning curves and not so friendly online\ncommunities. In this paper, we present an approach to help novice players in\nMOBA games overcome these problems. An artificial intelligence agent plays\nalongside the player analyzing his/her performance and giving tips about the\ngame. Experiments performed with the game {\\em League of Legends} show the\npotential of this approach.",
        "url": "http://arxiv.org/pdf/1706.02832v1.pdf"
    },
    {
        "title": "Dynamic Difficulty Adjustment on MOBA Games",
        "abstract": "This paper addresses the dynamic difficulty adjustment on MOBA games as a way\nto improve the player's entertainment. Although MOBA is currently one of the\nmost played genres around the world, it is known as a game that offer less\nautonomy, more challenges and consequently more frustration. Due to these\ncharacteristics, the use of a mechanism that performs the difficulty balance\ndynamically seems to be an interesting alternative to minimize and/or avoid\nthat players experience such frustrations. In this sense, this paper presents a\ndynamic difficulty adjustment mechanism for MOBA games. The main idea is to\ncreate a computer controlled opponent that adapts dynamically to the player\nperformance, trying to offer to the player a better game experience. This is\ndone by evaluating the performance of the player using a metric based on some\ngame features and switching the difficulty of the opponent's artificial\nintelligence behavior accordingly. Quantitative and qualitative experiments\nwere performed and the results showed that the system is capable of adapting\ndynamically to the opponent's skills. In spite of that, the qualitative\nexperiments with users showed that the player's expertise has a greater\ninfluence on the perception of the difficulty level and dynamic adaptation.",
        "url": "http://arxiv.org/pdf/1706.02796v1.pdf"
    },
    {
        "title": "Rapid Randomized Restarts for Multi-Agent Path Finding Solvers",
        "abstract": "Multi-Agent Path Finding (MAPF) is an NP-hard problem well studied in\nartificial intelligence and robotics. It has many real-world applications for\nwhich existing MAPF solvers use various heuristics. However, these solvers are\ndeterministic and perform poorly on \"hard\" instances typically characterized by\nmany agents interfering with each other in a small region. In this paper, we\nenhance MAPF solvers with randomization and observe that they exhibit\nheavy-tailed distributions of runtimes on hard instances. This leads us to\ndevelop simple rapid randomized restart (RRR) strategies with the intuition\nthat, given a hard instance, multiple short runs have a better chance of\nsolving it compared to one long run. We validate this intuition through\nexperiments and show that our RRR strategies indeed boost the performance of\nstate-of-the-art MAPF solvers such as iECBS and M*.",
        "url": "http://arxiv.org/pdf/1706.02794v1.pdf"
    },
    {
        "title": "The Morphospace of Consciousness",
        "abstract": "We construct a complexity-based morphospace to study systems-level properties\nof conscious & intelligent systems. The axes of this space label 3 complexity\ntypes: autonomous, cognitive & social. Given recent proposals to synthesize\nconsciousness, a generic complexity-based conceptualization provides a useful\nframework for identifying defining features of conscious & synthetic systems.\nBased on current clinical scales of consciousness that measure cognitive\nawareness and wakefulness, we take a perspective on how contemporary\nartificially intelligent machines & synthetically engineered life forms measure\non these scales. It turns out that awareness & wakefulness can be associated to\ncomputational & autonomous complexity respectively. Subsequently, building on\ninsights from cognitive robotics, we examine the function that consciousness\nserves, & argue the role of consciousness as an evolutionary game-theoretic\nstrategy. This makes the case for a third type of complexity for describing\nconsciousness: social complexity. Having identified these complexity types,\nallows for a representation of both, biological & synthetic systems in a common\nmorphospace. A consequence of this classification is a taxonomy of possible\nconscious machines. We identify four types of consciousness, based on\nembodiment: (i) biological consciousness, (ii) synthetic consciousness, (iii)\ngroup consciousness (resulting from group interactions), & (iv) simulated\nconsciousness (embodied by virtual agents within a simulated reality). This\ntaxonomy helps in the investigation of comparative signatures of consciousness\nacross domains, in order to highlight design principles necessary to engineer\nconscious machines. This is particularly relevant in the light of recent\ndevelopments at the crossroads of cognitive neuroscience, biomedical\nengineering, artificial intelligence & biomimetics.",
        "url": "http://arxiv.org/pdf/1705.11190v3.pdf"
    },
    {
        "title": "Learning to Represent Mechanics via Long-term Extrapolation and Interpolation",
        "abstract": "While the basic laws of Newtonian mechanics are well understood, explaining a\nphysical scenario still requires manually modeling the problem with suitable\nequations and associated parameters. In order to adopt such models for\nartificial intelligence, researchers have handcrafted the relevant states, and\nthen used neural networks to learn the state transitions using simulation runs\nas training data. Unfortunately, such approaches can be unsuitable for modeling\ncomplex real-world scenarios, where manually authoring relevant state spaces\ntend to be challenging. In this work, we investigate if neural networks can\nimplicitly learn physical states of real-world mechanical processes only based\non visual data, and thus enable long-term physical extrapolation. We develop a\nrecurrent neural network architecture for this task and also characterize\nresultant uncertainties in the form of evolving variance estimates. We evaluate\nour setup to extrapolate motion of a rolling ball on bowl of varying shape and\norientation using only images as input, and report competitive results with\napproaches that assume access to internal physics models and parameters.",
        "url": "http://arxiv.org/pdf/1706.02179v2.pdf"
    },
    {
        "title": "Types of Cognition and its Implications for future High-Level Cognitive Machines",
        "abstract": "This work summarizes part of current knowledge on High-level Cognitive\nprocess and its relation with biological hardware. Thus, it is possible to\nidentify some paradoxes which could impact the development of future\ntechnologies and artificial intelligence: we may make a High-level Cognitive\nMachine, sacrificing the principal attribute of a machine, its accuracy.",
        "url": "http://arxiv.org/pdf/1706.01443v1.pdf"
    },
    {
        "title": "Brain Intelligence: Go Beyond Artificial Intelligence",
        "abstract": "Artificial intelligence (AI) is an important technology that supports daily\nsocial life and economic activities. It contributes greatly to the sustainable\ngrowth of Japan's economy and solves various social problems. In recent years,\nAI has attracted attention as a key for growth in developed countries such as\nEurope and the United States and developing countries such as China and India.\nThe attention has been focused mainly on developing new artificial intelligence\ninformation communication technology (ICT) and robot technology (RT). Although\nrecently developed AI technology certainly excels in extracting certain\npatterns, there are many limitations. Most ICT models are overly dependent on\nbig data, lack a self-idea function, and are complicated. In this paper, rather\nthan merely developing next-generation artificial intelligence technology, we\naim to develop a new concept of general-purpose intelligence cognition\ntechnology called Beyond AI. Specifically, we plan to develop an intelligent\nlearning model called Brain Intelligence (BI) that generates new ideas about\nevents without having experienced them by using artificial life with an imagine\nfunction. We will also conduct demonstrations of the developed BI intelligence\nlearning model on automatic driving, precision medical care, and industrial\nrobots.",
        "url": "http://arxiv.org/pdf/1706.01040v1.pdf"
    },
    {
        "title": "I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation",
        "abstract": "Translating information between text and image is a fundamental problem in\nartificial intelligence that connects natural language processing and computer\nvision. In the past few years, performance in image caption generation has seen\nsignificant improvement through the adoption of recurrent neural networks\n(RNN). Meanwhile, text-to-image generation begun to generate plausible images\nusing datasets of specific categories like birds and flowers. We've even seen\nimage generation from multi-category datasets such as the Microsoft Common\nObjects in Context (MSCOCO) through the use of generative adversarial networks\n(GANs). Synthesizing objects with a complex shape, however, is still\nchallenging. For example, animals and humans have many degrees of freedom,\nwhich means that they can take on many complex shapes. We propose a new\ntraining method called Image-Text-Image (I2T2I) which integrates text-to-image\nand image-to-text (image captioning) synthesis to improve the performance of\ntext-to-image synthesis. We demonstrate that %the capability of our method to\nunderstand the sentence descriptions, so as to I2T2I can generate better\nmulti-categories images using MSCOCO than the state-of-the-art. We also\ndemonstrate that I2T2I can achieve transfer learning by using a pre-trained\nimage captioning module to generate human images on the MPII Human Pose",
        "url": "http://arxiv.org/pdf/1703.06676v3.pdf"
    },
    {
        "title": "Low Impact Artificial Intelligences",
        "abstract": "There are many goals for an AI that could become dangerous if the AI becomes\nsuperintelligent or otherwise powerful. Much work on the AI control problem has\nbeen focused on constructing AI goals that are safe even for such AIs. This\npaper looks at an alternative approach: defining a general concept of `low\nimpact'. The aim is to ensure that a powerful AI which implements low impact\nwill not modify the world extensively, even if it is given a simple or\ndangerous goal. The paper proposes various ways of defining and grounding low\nimpact, and discusses methods for ensuring that the AI can still be allowed to\nhave a (desired) impact despite the restriction. The end of the paper addresses\nknown issues with this approach and avenues for future research.",
        "url": "http://arxiv.org/pdf/1705.10720v1.pdf"
    },
    {
        "title": "MOBA: a New Arena for Game AI",
        "abstract": "Games have always been popular testbeds for Artificial Intelligence (AI). In\nthe last decade, we have seen the rise of the Multiple Online Battle Arena\n(MOBA) games, which are the most played games nowadays. In spite of this, there\nare few works that explore MOBA as a testbed for AI Research. In this paper we\npresent and discuss the main features and opportunities offered by MOBA games\nto Game AI Research. We describe the various challenges faced along the game\nand also propose a discrete model that can be used to better understand and\nexplore the game. With this, we aim to encourage the use of MOBA as a novel\nresearch platform for Game AI.",
        "url": "http://arxiv.org/pdf/1705.10443v1.pdf"
    },
    {
        "title": "A proposal for ethically traceable artificial intelligence",
        "abstract": "Although the problem of a critique of robotic behavior in near-unanimous\nagreement to human norms seems intractable, a starting point of such an\nambition is a framework of the collection of knowledge a priori and experience\na posteriori categorized as a set of synthetical judgments available to the\nintelligence, translated into computer code. If such a proposal were\nsuccessful, an algorithm with ethically traceable behavior and cogent\nequivalence to human cognition is established. This paper will propose the\napplication of Kant's critique of reason to current programming constructs of\nan autonomous intelligent system.",
        "url": "http://arxiv.org/pdf/1703.01908v2.pdf"
    },
    {
        "title": "Listen, Interact and Talk: Learning to Speak via Interaction",
        "abstract": "One of the long-term goals of artificial intelligence is to build an agent\nthat can communicate intelligently with human in natural language. Most\nexisting work on natural language learning relies heavily on training over a\npre-collected dataset with annotated labels, leading to an agent that\nessentially captures the statistics of the fixed external training data. As the\ntraining data is essentially a static snapshot representation of the knowledge\nfrom the annotator, the agent trained this way is limited in adaptiveness and\ngeneralization of its behavior. Moreover, this is very different from the\nlanguage learning process of humans, where language is acquired during\ncommunication by taking speaking action and learning from the consequences of\nspeaking action in an interactive manner. This paper presents an interactive\nsetting for grounded natural language learning, where an agent learns natural\nlanguage by interacting with a teacher and learning from feedback, thus\nlearning and improving language skills while taking part in the conversation.\nTo achieve this goal, we propose a model which incorporates both imitation and\nreinforcement by leveraging jointly sentence and reward feedbacks from the\nteacher. Experiments are conducted to validate the effectiveness of the\nproposed approach.",
        "url": "http://arxiv.org/pdf/1705.09906v1.pdf"
    },
    {
        "title": "Human Trajectory Prediction using Spatially aware Deep Attention Models",
        "abstract": "Trajectory Prediction of dynamic objects is a widely studied topic in the\nfield of artificial intelligence. Thanks to a large number of applications like\npredicting abnormal events, navigation system for the blind, etc. there have\nbeen many approaches to attempt learning patterns of motion directly from data\nusing a wide variety of techniques ranging from hand-crafted features to\nsophisticated deep learning models for unsupervised feature learning. All these\napproaches have been limited by problems like inefficient features in the case\nof hand crafted features, large error propagation across the predicted\ntrajectory and no information of static artefacts around the dynamic moving\nobjects. We propose an end to end deep learning model to learn the motion\npatterns of humans using different navigational modes directly from data using\nthe much popular sequence to sequence model coupled with a soft attention\nmechanism. We also propose a novel approach to model the static artefacts in a\nscene and using these to predict the dynamic trajectories. The proposed method,\ntested on trajectories of pedestrians, consistently outperforms previously\nproposed state of the art approaches on a variety of large scale data sets. We\nalso show how our architecture can be naturally extended to handle multiple\nmodes of movement (say pedestrians, skaters, bikers and buses) simultaneously.",
        "url": "http://arxiv.org/pdf/1705.09436v1.pdf"
    },
    {
        "title": "Semantics derived automatically from language corpora contain human-like biases",
        "abstract": "Artificial intelligence and machine learning are in a period of astounding\ngrowth. However, there are concerns that these technologies may be used, either\nwith or without intention, to perpetuate the prejudice and unfairness that\nunfortunately characterizes many human institutions. Here we show for the first\ntime that human-like semantic biases result from the application of standard\nmachine learning to ordinary language---the same sort of language humans are\nexposed to every day. We replicate a spectrum of standard human biases as\nexposed by the Implicit Association Test and other well-known psychological\nstudies. We replicate these using a widely used, purely statistical\nmachine-learning model---namely, the GloVe word embedding---trained on a corpus\nof text from the Web. Our results indicate that language itself contains\nrecoverable and accurate imprints of our historic biases, whether these are\nmorally neutral as towards insects or flowers, problematic as towards race or\ngender, or even simply veridical, reflecting the {\\em status quo} for the\ndistribution of gender with respect to careers or first names. These\nregularities are captured by machine learning along with the rest of semantics.\nIn addition to our empirical findings concerning language, we also contribute\nnew methods for evaluating bias in text, the Word Embedding Association Test\n(WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results\nhave implications not only for AI and machine learning, but also for the fields\nof psychology, sociology, and human ethics, since they raise the possibility\nthat mere exposure to everyday language can account for the biases we replicate\nhere.",
        "url": "http://arxiv.org/pdf/1608.07187v4.pdf"
    },
    {
        "title": "An Empirical Analysis of Approximation Algorithms for the Euclidean Traveling Salesman Problem",
        "abstract": "With applications to many disciplines, the traveling salesman problem (TSP)\nis a classical computer science optimization problem with applications to\nindustrial engineering, theoretical computer science, bioinformatics, and\nseveral other disciplines. In recent years, there have been a plethora of novel\napproaches for approximate solutions ranging from simplistic greedy to\ncooperative distributed algorithms derived from artificial intelligence. In\nthis paper, we perform an evaluation and analysis of cornerstone algorithms for\nthe Euclidean TSP. We evaluate greedy, 2-opt, and genetic algorithms. We use\nseveral datasets as input for the algorithms including a small dataset, a\nmediumsized dataset representing cities in the United States, and a synthetic\ndataset consisting of 200 cities to test algorithm scalability. We discover\nthat the greedy and 2-opt algorithms efficiently calculate solutions for\nsmaller datasets. Genetic algorithm has the best performance for optimality for\nmedium to large datasets, but generally have longer runtime. Our\nimplementations is public available.",
        "url": "http://arxiv.org/pdf/1705.09058v1.pdf"
    },
    {
        "title": "Salient Object Detection with Semantic Priors",
        "abstract": "Salient object detection has increasingly become a popular topic in cognitive\nand computational sciences, including computer vision and artificial\nintelligence research. In this paper, we propose integrating \\textit{semantic\npriors} into the salient object detection process. Our algorithm consists of\nthree basic steps. Firstly, the explicit saliency map is obtained based on the\nsemantic segmentation refined by the explicit saliency priors learned from the\ndata. Next, the implicit saliency map is computed based on a trained model\nwhich maps the implicit saliency priors embedded into regional features with\nthe saliency values. Finally, the explicit semantic map and the implicit map\nare adaptively fused to form a pixel-accurate saliency map which uniformly\ncovers the objects of interest. We further evaluate the proposed framework on\ntwo challenging datasets, namely, ECSSD and HKUIS. The extensive experimental\nresults demonstrate that our method outperforms other state-of-the-art methods.",
        "url": "http://arxiv.org/pdf/1705.08207v1.pdf"
    },
    {
        "title": "Logical Learning Through a Hybrid Neural Network with Auxiliary Inputs",
        "abstract": "The human reasoning process is seldom a one-way process from an input leading\nto an output. Instead, it often involves a systematic deduction by ruling out\nother possible outcomes as a self-checking mechanism. In this paper, we\ndescribe the design of a hybrid neural network for logical learning that is\nsimilar to the human reasoning through the introduction of an auxiliary input,\nnamely the indicators, that act as the hints to suggest logical outcomes. We\ngenerate these indicators by digging into the hidden information buried\nunderneath the original training data for direct or indirect suggestions. We\nused the MNIST data to demonstrate the design and use of these indicators in a\nconvolutional neural network. We trained a series of such hybrid neural\nnetworks with variations of the indicators. Our results show that these hybrid\nneural networks are very robust in generating logical outcomes with inherently\nhigher prediction accuracy than the direct use of the original input and output\nin apparent models. Such improved predictability with reassured logical\nconfidence is obtained through the exhaustion of all possible indicators to\nrule out all illogical outcomes, which is not available in the apparent models.\nOur logical learning process can effectively cope with the unknown unknowns\nusing a full exploitation of all existing knowledge available for learning. The\ndesign and implementation of the hints, namely the indicators, become an\nessential part of artificial intelligence for logical learning. We also\nintroduce an ongoing application setup for this hybrid neural network in an\nautonomous grasping robot, namely as_DeepClaw, aiming at learning an optimized\ngrasping pose through logical learning.",
        "url": "http://arxiv.org/pdf/1705.08200v1.pdf"
    },
    {
        "title": "Adaptive Maximization of Pointwise Submodular Functions With Budget Constraint",
        "abstract": "We study the worst-case adaptive optimization problem with budget constraint\nthat is useful for modeling various practical applications in artificial\nintelligence and machine learning. We investigate the near-optimality of greedy\nalgorithms for this problem with both modular and non-modular cost functions.\nIn both cases, we prove that two simple greedy algorithms are not near-optimal\nbut the best between them is near-optimal if the utility function satisfies\npointwise submodularity and pointwise cost-sensitive submodularity\nrespectively. This implies a combined algorithm that is near-optimal with\nrespect to the optimal algorithm that uses half of the budget. We discuss\napplications of our theoretical results and also report experiments comparing\nthe greedy algorithms on the active learning problem.",
        "url": "http://arxiv.org/pdf/1603.09029v2.pdf"
    },
    {
        "title": "AIXIjs: A Software Demo for General Reinforcement Learning",
        "abstract": "Reinforcement learning is a general and powerful framework with which to\nstudy and implement artificial intelligence. Recent advances in deep learning\nhave enabled RL algorithms to achieve impressive performance in restricted\ndomains such as playing Atari video games (Mnih et al., 2015) and, recently,\nthe board game Go (Silver et al., 2016). However, we are still far from\nconstructing a generally intelligent agent. Many of the obstacles and open\nquestions are conceptual: What does it mean to be intelligent? How does one\nexplore and learn optimally in general, unknown environments? What, in fact,\ndoes it mean to be optimal in the general sense? The universal Bayesian agent\nAIXI (Hutter, 2005) is a model of a maximally intelligent agent, and plays a\ncentral role in the sub-field of general reinforcement learning (GRL).\nRecently, AIXI has been shown to be flawed in important ways; it doesn't\nexplore enough to be asymptotically optimal (Orseau, 2010), and it can perform\npoorly with certain priors (Leike and Hutter, 2015). Several variants of AIXI\nhave been proposed to attempt to address these shortfalls: among them are\nentropy-seeking agents (Orseau, 2011), knowledge-seeking agents (Orseau et al.,\n2013), Bayes with bursts of exploration (Lattimore, 2013), MDL agents (Leike,\n2016a), Thompson sampling (Leike et al., 2016), and optimism (Sunehag and\nHutter, 2015). We present AIXIjs, a JavaScript implementation of these GRL\nagents. This implementation is accompanied by a framework for running\nexperiments against various environments, similar to OpenAI Gym (Brockman et\nal., 2016), and a suite of interactive demos that explore different properties\nof the agents, similar to REINFORCEjs (Karpathy, 2015). We use AIXIjs to\npresent numerous experiments illustrating fundamental properties of, and\ndifferences between, these agents.",
        "url": "http://arxiv.org/pdf/1705.07615v1.pdf"
    },
    {
        "title": "Learning to Multi-Task by Active Sampling",
        "abstract": "One of the long-standing challenges in Artificial Intelligence for learning\ngoal-directed behavior is to build a single agent which can solve multiple\ntasks. Recent progress in multi-task learning for goal-directed sequential\nproblems has been in the form of distillation based learning wherein a student\nnetwork learns from multiple task-specific expert networks by mimicking the\ntask-specific policies of the expert networks. While such approaches offer a\npromising solution to the multi-task learning problem, they require supervision\nfrom large expert networks which require extensive data and computation time\nfor training. In this work, we propose an efficient multi-task learning\nframework which solves multiple goal-directed tasks in an on-line setup without\nthe need for expert supervision. Our work uses active learning principles to\nachieve multi-task learning by sampling the harder tasks more than the easier\nones. We propose three distinct models under our active sampling framework. An\nadaptive method with extremely competitive multi-tasking performance. A\nUCB-based meta-learner which casts the problem of picking the next task to\ntrain on as a multi-armed bandit problem. A meta-learning method that casts the\nnext-task picking problem as a full Reinforcement Learning problem and uses\nactor critic methods for optimizing the multi-tasking performance directly. We\ndemonstrate results in the Atari 2600 domain on seven multi-tasking instances:\nthree 6-task instances, one 8-task instance, two 12-task instances and one\n21-task instance.",
        "url": "http://arxiv.org/pdf/1702.06053v4.pdf"
    },
    {
        "title": "Pitfalls and Best Practices in Algorithm Configuration",
        "abstract": "Good parameter settings are crucial to achieve high performance in many areas\nof artificial intelligence (AI), such as propositional satisfiability solving,\nAI planning, scheduling, and machine learning (in particular deep learning).\nAutomated algorithm configuration methods have recently received much attention\nin the AI community since they replace tedious, irreproducible and error-prone\nmanual parameter tuning and can lead to new state-of-the-art performance.\nHowever, practical applications of algorithm configuration are prone to several\n(often subtle) pitfalls in the experimental design that can render the\nprocedure ineffective. We identify several common issues and propose best\npractices for avoiding them. As one possibility for automatically handling as\nmany of these as possible, we also propose a tool called GenericWrapper4AC.",
        "url": "http://arxiv.org/pdf/1705.06058v3.pdf"
    },
    {
        "title": "Ethical Artificial Intelligence - An Open Question",
        "abstract": "Artificial Intelligence (AI) is an effective science which employs strong\nenough approaches, methods, and techniques to solve unsolvable real world based\nproblems. Because of its unstoppable rise towards the future, there are also\nsome discussions about its ethics and safety. Shaping an AI friendly\nenvironment for people and a people friendly environment for AI can be a\npossible answer for finding a shared context of values for both humans and\nrobots. In this context, objective of this paper is to address the ethical\nissues of AI and explore the moral dilemmas that arise from ethical algorithms,\nfrom pre set or acquired values. In addition, the paper will also focus on the\nsubject of AI safety. As general, the paper will briefly analyze the concerns\nand potential solutions to solving the ethical issues presented and increase\nreaders awareness on AI safety as another related research interest.",
        "url": "http://arxiv.org/pdf/1706.03021v1.pdf"
    },
    {
        "title": "A Survey of Question Answering for Math and Science Problem",
        "abstract": "Turing test was long considered the measure for artificial intelligence. But\nwith the advances in AI, it has proved to be insufficient measure. We can now\naim to mea- sure machine intelligence like we measure human intelligence. One\nof the widely accepted measure of intelligence is standardized math and science\ntest. In this paper, we explore the progress we have made towards the goal of\nmaking a machine smart enough to pass the standardized test. We see the\nchallenges and opportunities posed by the domain, and note that we are quite\nsome ways from actually making a system as smart as a even a middle school\nscholar.",
        "url": "http://arxiv.org/pdf/1705.04530v1.pdf"
    },
    {
        "title": "Basic protocols in quantum reinforcement learning with superconducting circuits",
        "abstract": "Superconducting circuit technologies have recently achieved quantum protocols\ninvolving closed feedback loops. Quantum artificial intelligence and quantum\nmachine learning are emerging fields inside quantum technologies which may\nenable quantum devices to acquire information from the outer world and improve\nthemselves via a learning process. Here we propose the implementation of basic\nprotocols in quantum reinforcement learning, with superconducting circuits\nemploying feedback-loop control. We introduce diverse scenarios for\nproof-of-principle experiments with state-of-the-art superconducting circuit\ntechnologies and analyze their feasibility in presence of imperfections. The\nfield of quantum artificial intelligence implemented with superconducting\ncircuits paves the way for enhanced quantum control and quantum computation\nprotocols.",
        "url": "http://arxiv.org/pdf/1701.05131v3.pdf"
    },
    {
        "title": "An Anthropic Argument against the Future Existence of Superintelligent Artificial Intelligence",
        "abstract": "This paper uses anthropic reasoning to argue for a reduced likelihood that\nsuperintelligent AI will come into existence in the future. To make this\nargument, a new principle is introduced: the Super-Strong Self-Sampling\nAssumption (SSSSA), building on the Self-Sampling Assumption (SSA) and the\nStrong Self-Sampling Assumption (SSSA). SSA uses as its sample the relevant\nobservers, whereas SSSA goes further by using observer-moments. SSSSA goes\nfurther still and weights each sample proportionally, according to the size of\na mind in cognitive terms. SSSSA is required for human observer-samples to be\ntypical, given by how much non-human animals outnumber humans. Given SSSSA, the\nassumption that humans experience typical observer-samples relies on a future\nwhere superintelligent AI does not dominate, which in turn reduces the\nlikelihood of it being created at all.",
        "url": "http://arxiv.org/pdf/1705.03078v1.pdf"
    },
    {
        "title": "Beating the World's Best at Super Smash Bros. with Deep Reinforcement Learning",
        "abstract": "There has been a recent explosion in the capabilities of game-playing\nartificial intelligence. Many classes of RL tasks, from Atari games to motor\ncontrol to board games, are now solvable by fairly generic algorithms, based on\ndeep learning, that learn to play from experience with minimal knowledge of the\nspecific domain of interest. In this work, we will investigate the performance\nof these methods on Super Smash Bros. Melee (SSBM), a popular console fighting\ngame. The SSBM environment has complex dynamics and partial observability,\nmaking it challenging for human and machine alike. The multi-player aspect\nposes an additional challenge, as the vast majority of recent advances in RL\nhave focused on single-agent environments. Nonetheless, we will show that it is\npossible to train agents that are competitive against and even surpass human\nprofessionals, a new result for the multi-player video game setting.",
        "url": "http://arxiv.org/pdf/1702.06230v3.pdf"
    },
    {
        "title": "Artificial Intelligence Based Malware Analysis",
        "abstract": "Artificial intelligence methods have often been applied to perform specific\nfunctions or tasks in the cyber-defense realm. However, as adversary methods\nbecome more complex and difficult to divine, piecemeal efforts to understand\ncyber-attacks, and malware-based attacks in particular, are not providing\nsufficient means for malware analysts to understand the past, present and\nfuture characteristics of malware.\n  In this paper, we present the Malware Analysis and Attributed using Genetic\nInformation (MAAGI) system. The underlying idea behind the MAAGI system is that\nthere are strong similarities between malware behavior and biological organism\nbehavior, and applying biologically inspired methods to corpora of malware can\nhelp analysts better understand the ecosystem of malware attacks. Due to the\nsophistication of the malware and the analysis, the MAAGI system relies heavily\non artificial intelligence techniques to provide this capability. It has\nalready yielded promising results over its development life, and will hopefully\ninspire more integration between the artificial intelligence and cyber--defense\ncommunities.",
        "url": "http://arxiv.org/pdf/1704.08716v1.pdf"
    },
    {
        "title": "Learning Representations by Stochastic Meta-Gradient Descent in Neural Networks",
        "abstract": "Representations are fundamental to artificial intelligence. The performance\nof a learning system depends on the type of representation used for\nrepresenting the data. Typically, these representations are hand-engineered\nusing domain knowledge. More recently, the trend is to learn these\nrepresentations through stochastic gradient descent in multi-layer neural\nnetworks, which is called backprop. Learning the representations directly from\nthe incoming data stream reduces the human labour involved in designing a\nlearning system. More importantly, this allows in scaling of a learning system\nfor difficult tasks. In this paper, we introduce a new incremental learning\nalgorithm called crossprop, which learns incoming weights of hidden units based\non the meta-gradient descent approach, that was previously introduced by Sutton\n(1992) and Schraudolph (1999) for learning step-sizes. The final update\nequation introduces an additional memory parameter for each of these weights\nand generalizes the backprop update equation. From our experiments, we show\nthat crossprop learns and reuses its feature representation while tackling new\nand unseen tasks whereas backprop relearns a new feature representation.",
        "url": "http://arxiv.org/pdf/1612.02879v2.pdf"
    },
    {
        "title": "Enhanced LSTM for Natural Language Inference",
        "abstract": "Reasoning and inference are central to human and artificial intelligence.\nModeling inference in human language is very challenging. With the availability\nof large annotated data (Bowman et al., 2015), it has recently become feasible\nto train neural network based inference models, which have shown to be very\neffective. In this paper, we present a new state-of-the-art result, achieving\nthe accuracy of 88.6% on the Stanford Natural Language Inference Dataset.\nUnlike the previous top models that use very complicated network architectures,\nwe first demonstrate that carefully designing sequential inference models based\non chain LSTMs can outperform all previous models. Based on this, we further\nshow that by explicitly considering recursive architectures in both local\ninference modeling and inference composition, we achieve additional\nimprovement. Particularly, incorporating syntactic parsing information\ncontributes to our best result---it further improves the performance even when\nadded to the already very strong model.",
        "url": "http://arxiv.org/pdf/1609.06038v3.pdf"
    },
    {
        "title": "General Video Game AI: Learning from Screen Capture",
        "abstract": "General Video Game Artificial Intelligence is a general game playing\nframework for Artificial General Intelligence research in the video-games\ndomain. In this paper, we propose for the first time a screen capture learning\nagent for General Video Game AI framework. A Deep Q-Network algorithm was\napplied and improved to develop an agent capable of learning to play different\ngames in the framework. After testing this algorithm using various games of\ndifferent categories and difficulty levels, the results suggest that our\nproposed screen capture learning agent has the potential to learn many\ndifferent games using only a single learning algorithm.",
        "url": "http://arxiv.org/pdf/1704.06945v1.pdf"
    },
    {
        "title": "Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds",
        "abstract": "Monte Carlo Tree Search (MCTS), most famously used in game-play artificial\nintelligence (e.g., the game of Go), is a well-known strategy for constructing\napproximate solutions to sequential decision problems. Its primary innovation\nis the use of a heuristic, known as a default policy, to obtain Monte Carlo\nestimates of downstream values for states in a decision tree. This information\nis used to iteratively expand the tree towards regions of states and actions\nthat an optimal policy might visit. However, to guarantee convergence to the\noptimal action, MCTS requires the entire tree to be expanded asymptotically. In\nthis paper, we propose a new technique called Primal-Dual MCTS that utilizes\nsampled information relaxation upper bounds on potential actions, creating the\npossibility of \"ignoring\" parts of the tree that stem from highly suboptimal\nchoices. This allows us to prove that despite converging to a partial decision\ntree in the limit, the recommended action from Primal-Dual MCTS is optimal. The\nnew approach shows significant promise when used to optimize the behavior of a\nsingle driver navigating a graph while operating on a ride-sharing platform.\nNumerical experiments on a real dataset of 7,000 trips in New Jersey suggest\nthat Primal-Dual MCTS improves upon standard MCTS by producing deeper decision\ntrees and exhibits a reduced sensitivity to the size of the action space.",
        "url": "http://arxiv.org/pdf/1704.05963v1.pdf"
    },
    {
        "title": "Semantic Similarity from Natural Language and Ontology Analysis",
        "abstract": "Artificial Intelligence federates numerous scientific fields in the aim of\ndeveloping machines able to assist human operators performing complex\ntreatments -- most of which demand high cognitive skills (e.g. learning or\ndecision processes). Central to this quest is to give machines the ability to\nestimate the likeness or similarity between things in the way human beings\nestimate the similarity between stimuli.\n  In this context, this book focuses on semantic measures: approaches designed\nfor comparing semantic entities such as units of language, e.g. words,\nsentences, or concepts and instances defined into knowledge bases. The aim of\nthese measures is to assess the similarity or relatedness of such semantic\nentities by taking into account their semantics, i.e. their meaning --\nintuitively, the words tea and coffee, which both refer to stimulating\nbeverage, will be estimated to be more semantically similar than the words\ntoffee (confection) and coffee, despite that the last pair has a higher\nsyntactic similarity. The two state-of-the-art approaches for estimating and\nquantifying semantic similarities/relatedness of semantic entities are\npresented in detail: the first one relies on corpora analysis and is based on\nNatural Language Processing techniques and semantic models while the second is\nbased on more or less formal, computer-readable and workable forms of knowledge\nsuch as semantic networks, thesaurus or ontologies. (...) Beyond a simple\ninventory and categorization of existing measures, the aim of this monograph is\nto convey novices as well as researchers of these domains towards a better\nunderstanding of semantic similarity estimation and more generally semantic\nmeasures.",
        "url": "http://arxiv.org/pdf/1704.05295v1.pdf"
    },
    {
        "title": "Approximating the Backbone in the Weighted Maximum Satisfiability Problem",
        "abstract": "The weighted Maximum Satisfiability problem (weighted MAX-SAT) is a NP-hard\nproblem with numerous applications arising in artificial intelligence. As an\nefficient tool for heuristic design, the backbone has been applied to\nheuristics design for many NP-hard problems. In this paper, we investigated the\ncomputational complexity for retrieving the backbone in weighted MAX-SAT and\ndeveloped a new algorithm for solving this problem. We showed that it is\nintractable to retrieve the full backbone under the assumption that . Moreover,\nit is intractable to retrieve a fixed fraction of the backbone as well. And\nthen we presented a backbone guided local search (BGLS) with Walksat operator\nfor weighted MAX-SAT. BGLS consists of two phases: the first phase samples the\nbackbone information from local optima and the backbone phase conducts local\nsearch under the guideline of backbone. Extensive experimental results on the\nbenchmark showed that BGLS outperforms the existing heuristics in both solution\nquality and runtime.",
        "url": "http://arxiv.org/pdf/1704.04775v1.pdf"
    },
    {
        "title": "iCaRL: Incremental Classifier and Representation Learning",
        "abstract": "A major open problem on the road to artificial intelligence is the\ndevelopment of incrementally learning systems that learn about more and more\nconcepts over time from a stream of data. In this work, we introduce a new\ntraining strategy, iCaRL, that allows learning in such a class-incremental way:\nonly the training data for a small number of classes has to be present at the\nsame time and new classes can be added progressively. iCaRL learns strong\nclassifiers and a data representation simultaneously. This distinguishes it\nfrom earlier works that were fundamentally limited to fixed data\nrepresentations and therefore incompatible with deep learning architectures. We\nshow by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can\nlearn many classes incrementally over a long period of time where other\nstrategies quickly fail.",
        "url": "http://arxiv.org/pdf/1611.07725v2.pdf"
    },
    {
        "title": "Ultrafast photonic reinforcement learning based on laser chaos",
        "abstract": "Reinforcement learning involves decision making in dynamic and uncertain\nenvironments, and constitutes one important element of artificial intelligence\n(AI). In this paper, we experimentally demonstrate that the ultrafast chaotic\noscillatory dynamics of lasers efficiently solve the multi-armed bandit problem\n(MAB), which requires decision making concerning a class of difficult\ntrade-offs called the exploration-exploitation dilemma. To solve the MAB, a\ncertain degree of randomness is required for exploration purposes. However,\npseudo-random numbers generated using conventional electronic circuitry\nencounter severe limitations in terms of their data rate and the quality of\nrandomness due to their algorithmic foundations. We generate laser chaos\nsignals using a semiconductor laser sampled at a maximum rate of 100 GSample/s,\nand combine it with a simple decision-making principle called tug-of-war with a\nvariable threshold, to ensure ultrafast, adaptive and accurate decision making\nat a maximum adaptation speed of 1 GHz. We found that decision-making\nperformance was maximized with an optimal sampling interval, and we highlight\nthe exact coincidence between the negative autocorrelation inherent in laser\nchaos and decision-making performance. This study paves the way for a new realm\nof ultrafast photonics in the age of AI, where the ultrahigh bandwidth of\nphotons can provide new value.",
        "url": "http://arxiv.org/pdf/1704.04379v1.pdf"
    },
    {
        "title": "Detection, Recognition and Tracking of Moving Objects from Real-time Video via SP Theory of Intelligence and Species Inspired PSO",
        "abstract": "In this paper, we address the basic problem of recognizing moving objects in\nvideo images using SP Theory of Intelligence. The concept of SP Theory of\nIntelligence which is a framework of artificial intelligence, was first\nintroduced by Gerard J Wolff, where S stands for Simplicity and P stands for\nPower. Using the concept of multiple alignment, we detect and recognize object\nof our interest in video frames with multilevel hierarchical parts and\nsubparts, based on polythetic categories. We track the recognized objects using\nthe species based Particle Swarm Optimization (PSO). First, we extract the\nmultiple alignment of our object of interest from training images. In order to\nrecognize accurately and handle occlusion, we use the polythetic concepts on\nraw data line to omit the redundant noise via searching for best alignment\nrepresenting the features from the extracted alignments. We recognize the\ndomain of interest from the video scenes in form of wide variety of multiple\nalignments to handle scene variability. Unsupervised learning is done in the SP\nmodel following the DONSVIC principle and natural structures are discovered via\ninformation compression and pattern analysis. After successful recognition of\nobjects, we use species based PSO algorithm as the alignments of our object of\ninterest is analogues to observation likelihood and fitness ability of species.\nSubsequently, we analyze the competition and repulsion among species with\nannealed Gaussian based PSO. We have tested our algorithms on David, Walking2,\nFaceOcc1, Jogging and Dudek, obtaining very satisfactory and competitive\nresults.",
        "url": "http://arxiv.org/pdf/1704.07312v1.pdf"
    },
    {
        "title": "Improving content marketing processes with the approaches by artificial intelligence",
        "abstract": "Content marketing is todays one of the most remarkable approaches in the\ncontext of marketing processes of companies. Value of this kind of marketing\nhas improved in time, thanks to the latest developments regarding to computer\nand communication technologies. Nowadays, especially social media based\nplatforms have a great importance on enabling companies to design multimedia\noriented, interactive content. But on the other hand, there is still something\nmore to do for improved content marketing approaches. In this context,\nobjective of this study is to focus on intelligent content marketing, which can\nbe done by using artificial intelligence. Artificial Intelligence is todays one\nof the most remarkable research fields and it can be used easily as\nmultidisciplinary. So, this study has aimed to discuss about its potential on\nimproving content marketing. In detail, the study has enabled readers to\nimprove their awareness about the intersection point of content marketing and\nartificial intelligence. Furthermore, the authors have introduced some example\nmodels of intelligent content marketing, which can be achieved by using current\nWeb technologies and artificial intelligence techniques.",
        "url": "http://arxiv.org/pdf/1704.02114v1.pdf"
    },
    {
        "title": "Realizing an optimization approach inspired from Piagets theory on cognitive development",
        "abstract": "The objective of this paper is to introduce an artificial intelligence based\noptimization approach, which is inspired from Piagets theory on cognitive\ndevelopment. The approach has been designed according to essential processes\nthat an individual may experience while learning something new or improving his\n/ her knowledge. These processes are associated with the Piagets ideas on an\nindividuals cognitive development. The approach expressed in this paper is a\nsimple algorithm employing swarm intelligence oriented tasks in order to\novercome single-objective optimization problems. For evaluating effectiveness\nof this early version of the algorithm, test operations have been done via some\nbenchmark functions. The obtained results show that the approach / algorithm\ncan be an alternative to the literature in terms of single-objective\noptimization. The authors have suggested the name: Cognitive Development\nOptimization Algorithm (CoDOA) for the related intelligent optimization\napproach.",
        "url": "http://arxiv.org/pdf/1704.05904v1.pdf"
    },
    {
        "title": "On the idea of a new artificial intelligence based optimization algorithm inspired from the nature of vortex",
        "abstract": "In this paper, the idea of a new artificial intelligence based optimization\nalgorithm, which is inspired from the nature of vortex, has been provided\nbriefly. As also a bio-inspired computation algorithm, the idea is generally\nfocused on a typical vortex flow / behavior in nature and inspires from some\ndynamics that are occurred in the sense of vortex nature. Briefly, the\nalgorithm is also a swarm-oriented evolutional problem solution approach;\nbecause it includes many methods related to elimination of weak swarm members\nand trying to improve the solution process by supporting the solution space via\nnew swarm members. In order have better idea about success of the algorithm; it\nhas been tested via some benchmark functions. At this point, the obtained\nresults show that the algorithm can be an alternative to the literature in\nterms of single-objective optimization solution ways. Vortex Optimization\nAlgorithm (VOA) is the name suggestion by the authors; for this new idea of\nintelligent optimization approach.",
        "url": "http://arxiv.org/pdf/1704.00797v1.pdf"
    },
    {
        "title": "Rational Choice and Artificial Intelligence",
        "abstract": "The theory of rational choice assumes that when people make decisions they do\nso in order to maximize their utility. In order to achieve this goal they ought\nto use all the information available and consider all the choices available to\nchoose an optimal choice. This paper investigates what happens when decisions\nare made by artificially intelligent machines in the market rather than human\nbeings. Firstly, the expectations of the future are more consistent if they are\nmade by an artificially intelligent machine and the decisions are more rational\nand thus marketplace becomes more rational.",
        "url": "http://arxiv.org/pdf/1703.10098v1.pdf"
    },
    {
        "title": "Probabilistic Models for Computerized Adaptive Testing",
        "abstract": "In this paper we follow our previous research in the area of Computerized\nAdaptive Testing (CAT). We present three different methods for CAT. One of\nthem, the item response theory, is a well established method, while the other\ntwo, Bayesian and neural networks, are new in the area of educational testing.\nIn the first part of this paper, we present the concept of CAT and its\nadvantages and disadvantages. We collected data from paper tests performed with\ngrammar school students. We provide the summary of data used for our\nexperiments in the second part. Next, we present three different model types\nfor CAT. They are based on the item response theory, Bayesian networks, and\nneural networks. The general theory associated with each type is briefly\nexplained and the utilization of these models for CAT is analyzed. Future\nresearch is outlined in the concluding part of the paper. It shows many\ninteresting research paths that are important not only for CAT but also for\nother areas of artificial intelligence.",
        "url": "http://arxiv.org/pdf/1703.09794v1.pdf"
    },
    {
        "title": "Artificial Intelligence and Economic Theories",
        "abstract": "The advent of artificial intelligence has changed many disciplines such as\nengineering, social science and economics. Artificial intelligence is a\ncomputational technique which is inspired by natural intelligence such as the\nswarming of birds, the working of the brain and the pathfinding of the ants.\nThese techniques have impact on economic theories. This book studies the impact\nof artificial intelligence on economic theories, a subject that has not been\nextensively studied. The theories that are considered are: demand and supply,\nasymmetrical information, pricing, rational choice, rational expectation, game\ntheory, efficient market hypotheses, mechanism design, prospect, bounded\nrationality, portfolio theory, rational counterfactual and causality. The\nbenefit of this book is that it evaluates existing theories of economics and\nupdate them based on the developments in artificial intelligence field.",
        "url": "http://arxiv.org/pdf/1703.06597v1.pdf"
    },
    {
        "title": "Application of Fuzzy Logic in Design of Smart Washing Machine",
        "abstract": "Washing machine is of great domestic necessity as it frees us from the burden\nof washing our clothes and saves ample of our time. This paper will cover the\naspect of designing and developing of Fuzzy Logic based, Smart Washing Machine.\nThe regular washing machine (timer based) makes use of multi-turned timer based\nstart-stop mechanism which is mechanical as is prone to breakage. In addition\nto its starting and stopping issues, the mechanical timers are not efficient\nwith respect of maintenance and electricity usage. Recent developments have\nshown that merger of digital electronics in optimal functionality of this\nmachine is possible and nowadays in practice. A number of international\nrenowned companies have developed the machine with the introduction of smart\nartificial intelligence. Such a machine makes use of sensors and smartly\ncalculates the amount of run-time (washing time) for the main machine motor.\nRealtime calculations and processes are also catered in optimizing the run-time\nof the machine. The obvious result is smart time management, better economy of\nelectricity and efficiency of work. This paper deals with the indigenization of\nFLC (Fuzzy Logic Controller) based Washing Machine, which is capable of\nautomating the inputs and getting the desired output (wash-time).",
        "url": "http://arxiv.org/pdf/1701.01654v2.pdf"
    },
    {
        "title": "On Quantum Decision Trees",
        "abstract": "Quantum decision systems are being increasingly considered for use in\nartificial intelligence applications. Classical and quantum nodes can be\ndistinguished based on certain correlations in their states. This paper\ninvestigates some properties of the states obtained in a decision tree\nstructure. How these correlations may be mapped to the decision tree is\nconsidered. Classical tree representations and approximations to quantum states\nare provided.",
        "url": "http://arxiv.org/pdf/1703.03693v1.pdf"
    },
    {
        "title": "Deep Reservoir Computing Using Cellular Automata",
        "abstract": "Recurrent Neural Networks (RNNs) have been a prominent concept within\nartificial intelligence. They are inspired by Biological Neural Networks (BNNs)\nand provide an intuitive and abstract representation of how BNNs work. Derived\nfrom the more generic Artificial Neural Networks (ANNs), the recurrent ones are\nmeant to be used for temporal tasks, such as speech recognition, because they\nare capable of memorizing historic input. However, such networks are very time\nconsuming to train as a result of their inherent nature. Recently, Echo State\nNetworks and Liquid State Machines have been proposed as possible RNN\nalternatives, under the name of Reservoir Computing (RC). RCs are far more easy\nto train. In this paper, Cellular Automata are used as reservoir, and are\ntested on the 5-bit memory task (a well known benchmark within the RC\ncommunity). The work herein provides a method of mapping binary inputs from the\ntask onto the automata, and a recurrent architecture for handling the\nsequential aspects of it. Furthermore, a layered (deep) reservoir architecture\nis proposed. Performances are compared towards earlier work, in addition to its\nsingle-layer version. Results show that the single CA reservoir system yields\nsimilar results to state-of-the-art work. The system comprised of two layered\nreservoirs do show a noticeable improvement compared to a single CA reservoir.\nThis indicates potential for further research and provides valuable insight on\nhow to design CA reservoir systems.",
        "url": "http://arxiv.org/pdf/1703.02806v1.pdf"
    },
    {
        "title": "Don't Fear the Reaper: Refuting Bostrom's Superintelligence Argument",
        "abstract": "In recent years prominent intellectuals have raised ethical concerns about\nthe consequences of artificial intelligence. One concern is that an autonomous\nagent might modify itself to become \"superintelligent\" and, in supremely\neffective pursuit of poorly specified goals, destroy all of humanity. This\npaper considers and rejects the possibility of this outcome. We argue that this\nscenario depends on an agent's ability to rapidly improve its ability to\npredict its environment through self-modification. Using a Bayesian model of a\nreasoning agent, we show that there are important limitations to how an agent\nmay improve its predictive ability through self-modification alone. We conclude\nthat concern about this artificial intelligence outcome is misplaced and better\ndirected at policy questions around data access and storage.",
        "url": "http://arxiv.org/pdf/1702.08495v2.pdf"
    },
    {
        "title": "Morphognosis: the shape of knowledge in space and time",
        "abstract": "Artificial intelligence research to a great degree focuses on the brain and\nbehaviors that the brain generates. But the brain, an extremely complex\nstructure resulting from millions of years of evolution, can be viewed as a\nsolution to problems posed by an environment existing in space and time. The\nenvironment generates signals that produce sensory events within an organism.\nBuilding an internal spatial and temporal model of the environment allows an\norganism to navigate and manipulate the environment. Higher intelligence might\nbe the ability to process information coming from a larger extent of\nspace-time. In keeping with nature's penchant for extending rather than\nreplacing, the purpose of the mammalian neocortex might then be to record\nevents from distant reaches of space and time and render them, as though yet\nnear and present, to the older, deeper brain whose instinctual roles have\nchanged little over eons. Here this notion is embodied in a model called\nmorphognosis (morpho = shape and gnosis = knowledge). Its basic structure is a\npyramid of event recordings called a morphognostic. At the apex of the pyramid\nare the most recent and nearby events. Receding from the apex are less recent\nand possibly more distant events. A morphognostic can thus be viewed as a\nstructure of progressively larger chunks of space-time knowledge. A set of\nmorphognostics forms long-term memories that are learned by exposure to the\nenvironment. A cellular automaton is used as the platform to investigate the\nmorphognosis model, using a simulated organism that learns to forage in its\nworld for food, build a nest, and play the game of Pong.",
        "url": "http://arxiv.org/pdf/1701.02272v2.pdf"
    },
    {
        "title": "DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker",
        "abstract": "Artificial intelligence has seen several breakthroughs in recent years, with\ngames often serving as milestones. A common feature of these games is that\nplayers have perfect information. Poker is the quintessential game of imperfect\ninformation, and a longstanding challenge problem in artificial intelligence.\nWe introduce DeepStack, an algorithm for imperfect information settings. It\ncombines recursive reasoning to handle information asymmetry, decomposition to\nfocus computation on the relevant decision, and a form of intuition that is\nautomatically learned from self-play using deep learning. In a study involving\n44,000 hands of poker, DeepStack defeated with statistical significance\nprofessional poker players in heads-up no-limit Texas hold'em. The approach is\ntheoretically sound and is shown to produce more difficult to exploit\nstrategies than prior approaches.",
        "url": "http://arxiv.org/pdf/1701.01724v3.pdf"
    },
    {
        "title": "Learning A Physical Long-term Predictor",
        "abstract": "Evolution has resulted in highly developed abilities in many natural\nintelligences to quickly and accurately predict mechanical phenomena. Humans\nhave successfully developed laws of physics to abstract and model such\nmechanical phenomena. In the context of artificial intelligence, a recent line\nof work has focused on estimating physical parameters based on sensory data and\nuse them in physical simulators to make long-term predictions. In contrast, we\ninvestigate the effectiveness of a single neural network for end-to-end\nlong-term prediction of mechanical phenomena. Based on extensive evaluation, we\ndemonstrate that such networks can outperform alternate approaches having even\naccess to ground-truth physical simulators, especially when some physical\nparameters are unobserved or not known a-priori. Further, our network outputs a\ndistribution of outcomes to capture the inherent uncertainty in the data. Our\napproach demonstrates for the first time the possibility of making actionable\nlong-term predictions from sensor data without requiring to explicitly model\nthe underlying physical laws.",
        "url": "http://arxiv.org/pdf/1703.00247v1.pdf"
    },
    {
        "title": "Improving the Neural GPU Architecture for Algorithm Learning",
        "abstract": "Algorithm learning is a core problem in artificial intelligence with\nsignificant implications on automation level that can be achieved by machines.\nRecently deep learning methods are emerging for synthesizing an algorithm from\nits input-output examples, the most successful being the Neural GPU, capable of\nlearning multiplication. We present several improvements to the Neural GPU that\nsubstantially reduces training time and improves generalization. We introduce a\nnew technique - hard nonlinearities with saturation costs- that has general\napplicability. We also introduce a technique of diagonal gates that can be\napplied to active-memory models. The proposed architecture is the first capable\nof learning decimal multiplication end-to-end.",
        "url": "http://arxiv.org/pdf/1702.08727v2.pdf"
    },
    {
        "title": "Developing a comprehensive framework for multimodal feature extraction",
        "abstract": "Feature extraction is a critical component of many applied data science\nworkflows. In recent years, rapid advances in artificial intelligence and\nmachine learning have led to an explosion of feature extraction tools and\nservices that allow data scientists to cheaply and effectively annotate their\ndata along a vast array of dimensions---ranging from detecting faces in images\nto analyzing the sentiment expressed in coherent text. Unfortunately, the\nproliferation of powerful feature extraction services has been mirrored by a\ncorresponding expansion in the number of distinct interfaces to feature\nextraction services. In a world where nearly every new service has its own API,\ndocumentation, and/or client library, data scientists who need to combine\ndiverse features obtained from multiple sources are often forced to write and\nmaintain ever more elaborate feature extraction pipelines. To address this\nchallenge, we introduce a new open-source framework for comprehensive\nmultimodal feature extraction. Pliers is an open-source Python package that\nsupports standardized annotation of diverse data types (video, images, audio,\nand text), and is expressly with both ease-of-use and extensibility in mind.\nUsers can apply a wide range of pre-existing feature extraction tools to their\ndata in just a few lines of Python code, and can also easily add their own\ncustom extractors by writing modular classes. A graph-based API enables rapid\ndevelopment of complex feature extraction pipelines that output results in a\nsingle, standardized format. We describe the package's architecture, detail its\nmajor advantages over previous feature extraction toolboxes, and use a sample\napplication to a large functional MRI dataset to illustrate how pliers can\nsignificantly reduce the time and effort required to construct sophisticated\nfeature extraction workflows while increasing code clarity and maintainability.",
        "url": "http://arxiv.org/pdf/1702.06151v1.pdf"
    },
    {
        "title": "The Absent-Minded Driver Problem Redux",
        "abstract": "This paper reconsiders the problem of the absent-minded driver who must\nchoose between alternatives with different payoff with imperfect recall and\nvarying degrees of knowledge of the system. The classical absent-minded driver\nproblem represents the case with limited information and it has bearing on the\ngeneral area of communication and learning, social choice, mechanism design,\nauctions, theories of knowledge, belief, and rational agency. Within the\nframework of extensive games, this problem has applications to many artificial\nintelligence scenarios. It is obvious that the performance of the agent\nimproves as information available increases. It is shown that a non-uniform\nassignment strategy for successive choices does better than a fixed probability\nstrategy. We consider both classical and quantum approaches to the problem. We\nargue that the superior performance of quantum decisions with access to\nentanglement cannot be fairly compared to a classical algorithm. If the\ncognitive systems of agents are taken to have access to quantum resources, or\nhave a quantum mechanical basis, then that can be leveraged into superior\nperformance.",
        "url": "http://arxiv.org/pdf/1702.05778v1.pdf"
    },
    {
        "title": "Overview: Generalizations of Multi-Agent Path Finding to Real-World Scenarios",
        "abstract": "Multi-agent path finding (MAPF) is well-studied in artificial intelligence,\nrobotics, theoretical computer science and operations research. We discuss\nissues that arise when generalizing MAPF methods to real-world scenarios and\nfour research directions that address them. We emphasize the importance of\naddressing these issues as opposed to developing faster methods for the\nstandard formulation of the MAPF problem.",
        "url": "http://arxiv.org/pdf/1702.05515v1.pdf"
    },
    {
        "title": "Entropy Non-increasing Games for the Improvement of Dataflow Programming",
        "abstract": "In this article, we introduce a new conception of a family of esport games\ncalled Samu Entropy to try to improve dataflow program graphs like the ones\nthat are based on Google's TensorFlow. Currently, the Samu Entropy project\nspecifies only requirements for new esport games to be developed with\nparticular attention to the investigation of the relationship between esport\nand artificial intelligence. It is quite obvious that there is a very close and\nnatural relationship between esport games and artificial intelligence.\nFurthermore, the project Samu Entropy focuses not only on using artificial\nintelligence, but on creating AI in a new way. We present a reference game\ncalled Face Battle that implements the Samu Entropy requirements.",
        "url": "http://arxiv.org/pdf/1702.04389v1.pdf"
    },
    {
        "title": "Towards Better Analysis of Machine Learning Models: A Visual Analytics Perspective",
        "abstract": "Interactive model analysis, the process of understanding, diagnosing, and\nrefining a machine learning model with the help of interactive visualization,\nis very important for users to efficiently solve real-world artificial\nintelligence and data mining problems. Dramatic advances in big data analytics\nhas led to a wide variety of interactive model analysis tasks. In this paper,\nwe present a comprehensive analysis and interpretation of this rapidly\ndeveloping area. Specifically, we classify the relevant work into three\ncategories: understanding, diagnosis, and refinement. Each category is\nexemplified by recent influential work. Possible future research opportunities\nare also explored and discussed.",
        "url": "http://arxiv.org/pdf/1702.01226v1.pdf"
    },
    {
        "title": "Deep learning in color: towards automated quark/gluon jet discrimination",
        "abstract": "Artificial intelligence offers the potential to automate challenging\ndata-processing tasks in collider physics. To establish its prospects, we\nexplore to what extent deep learning with convolutional neural networks can\ndiscriminate quark and gluon jets better than observables designed by\nphysicists. Our approach builds upon the paradigm that a jet can be treated as\nan image, with intensity given by the local calorimeter deposits. We supplement\nthis construction by adding color to the images, with red, green and blue\nintensities given by the transverse momentum in charged particles, transverse\nmomentum in neutral particles, and pixel-level charged particle counts.\nOverall, the deep networks match or outperform traditional jet variables. We\nalso find that, while various simulations produce different quark and gluon\njets, the neural networks are surprisingly insensitive to these differences,\nsimilar to traditional observables. This suggests that the networks can extract\nrobust physical information from imperfect simulations.",
        "url": "http://arxiv.org/pdf/1612.01551v3.pdf"
    },
    {
        "title": "Blue Sky Ideas in Artificial Intelligence Education from the EAAI 2017 New and Future AI Educator Program",
        "abstract": "The 7th Symposium on Educational Advances in Artificial Intelligence\n(EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and\nFuture AI Educator Program to support the training of early-career university\nfaculty, secondary school faculty, and future educators (PhD candidates or\npostdocs who intend a career in academia). As part of the program, awardees\nwere asked to address one of the following \"blue sky\" questions:\n  * How could/should Artificial Intelligence (AI) courses incorporate ethics\ninto the curriculum?\n  * How could we teach AI topics at an early undergraduate or a secondary\nschool level?\n  * AI has the potential for broad impact to numerous disciplines. How could we\nmake AI education more interdisciplinary, specifically to benefit\nnon-engineering fields?\n  This paper is a collection of their responses, intended to help motivate\ndiscussion around these issues in AI education.",
        "url": "http://arxiv.org/pdf/1702.00137v1.pdf"
    },
    {
        "title": "Ethical Considerations in Artificial Intelligence Courses",
        "abstract": "The recent surge in interest in ethics in artificial intelligence may leave many educators wondering how to address moral, ethical, and philosophical issues in their AI courses. As instructors we want to develop curriculum that not only prepares students to be artificial intelligence practitioners, but also to understand the moral, ethical, and philosophical impacts that artificial intelligence will have on society. In this article we provide practical case studies and links to resources for use by AI educators. We also provide concrete suggestions on how to integrate AI ethics into a general artificial intelligence course and how to teach a stand-alone artificial intelligence ethics course.",
        "url": "https://arxiv.org/pdf/1701.07769v1.pdf"
    },
    {
        "title": "Overcoming catastrophic forgetting in neural networks",
        "abstract": "The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially.",
        "url": "http://arxiv.org/pdf/1612.00796v2.pdf"
    },
    {
        "title": "Artificial Intelligence Approaches To UCAV Autonomy",
        "abstract": "This paper covers a number of approaches that leverage Artificial\nIntelligence algorithms and techniques to aid Unmanned Combat Aerial Vehicle\n(UCAV) autonomy. An analysis of current approaches to autonomous control is\nprovided followed by an exploration of how these techniques can be extended and\nenriched with AI techniques including Artificial Neural Networks (ANN),\nEnsembling and Reinforcement Learning (RL) to evolve control strategies for\nUCAVs.",
        "url": "http://arxiv.org/pdf/1701.07103v1.pdf"
    },
    {
        "title": "Minimally Naturalistic Artificial Intelligence",
        "abstract": "The rapid advancement of machine learning techniques has re-energized\nresearch into general artificial intelligence. While the idea of\ndomain-agnostic meta-learning is appealing, this emerging field must come to\nterms with its relationship to human cognition and the statistics and structure\nof the tasks humans perform. The position of this article is that only by\naligning our agents' abilities and environments with those of humans do we\nstand a chance at developing general artificial intelligence (GAI). A broad\nreading of the famous 'No Free Lunch' theorem is that there is no universally\noptimal inductive bias or, equivalently, bias-free learning is impossible. This\nfollows from the fact that there are an infinite number of ways to extrapolate\ndata, any of which might be the one used by the data generating environment; an\ninductive bias prefers some of these extrapolations to others, which lowers\nperformance in environments using these adversarial extrapolations. We may\nposit that the optimal GAI is the one that maximally exploits the statistics of\nits environment to create its inductive bias; accepting the fact that this\nagent is guaranteed to be extremely sub-optimal for some alternative\nenvironments. This trade-off appears benign when thinking about the environment\nas being the physical universe, as performance on any fictive universe is\nobviously irrelevant. But, we should expect a sharper inductive bias if we\nfurther constrain our environment. Indeed, we implicitly do so by defining GAI\nin terms of accomplishing that humans consider useful. One common version of\nthis is need the for 'common-sense reasoning', which implicitly appeals to the\nstatistics of physical universe as perceived by humans.",
        "url": "http://arxiv.org/pdf/1701.03868v1.pdf"
    },
    {
        "title": "Just an Update on PMING Distance for Web-based Semantic Similarity in Artificial Intelligence and Data Mining",
        "abstract": "One of the main problems that emerges in the classic approach to semantics is\nthe difficulty in acquisition and maintenance of ontologies and semantic\nannotations. On the other hand, the Internet explosion and the massive\ndiffusion of mobile smart devices lead to the creation of a worldwide system,\nwhich information is daily checked and fueled by the contribution of millions\nof users who interacts in a collaborative way. Search engines, continually\nexploring the Web, are a natural source of information on which to base a\nmodern approach to semantic annotation. A promising idea is that it is possible\nto generalize the semantic similarity, under the assumption that semantically\nsimilar terms behave similarly, and define collaborative proximity measures\nbased on the indexing information returned by search engines. The PMING\nDistance is a proximity measure used in data mining and information retrieval,\nwhich collaborative information express the degree of relationship between two\nterms, using only the number of documents returned as result for a query on a\nsearch engine. In this work, the PMINIG Distance is updated, providing a novel\nformal algebraic definition, which corrects previous works. The novel point of\nview underlines the features of the PMING to be a locally normalized linear\ncombination of the Pointwise Mutual Information and Normalized Google Distance.\nThe analyzed measure dynamically reflects the collaborative change made on the\nweb resources.",
        "url": "http://arxiv.org/pdf/1701.02163v1.pdf"
    },
    {
        "title": "Designing a Safe Autonomous Artificial Intelligence Agent based on Human Self-Regulation",
        "abstract": "There is a growing focus on how to design safe artificial intelligent (AI)\nagents. As systems become more complex, poorly specified goals or control\nmechanisms may cause AI agents to engage in unwanted and harmful outcomes. Thus\nit is necessary to design AI agents that follow initial programming intentions\nas the program grows in complexity. How to specify these initial intentions has\nalso been an obstacle to designing safe AI agents. Finally, there is a need for\nthe AI agent to have redundant safety mechanisms to ensure that any programming\nerrors do not cascade into major problems. Humans are autonomous intelligent\nagents that have avoided these problems and the present manuscript argues that\nby understanding human self-regulation and goal setting, we may be better able\nto design safe AI agents. Some general principles of human self-regulation are\noutlined and specific guidance for AI design is given.",
        "url": "http://arxiv.org/pdf/1701.01487v1.pdf"
    },
    {
        "title": "A Review of Neural Network Based Machine Learning Approaches for Rotor Angle Stability Control",
        "abstract": "This paper reviews the current status and challenges of Neural Networks (NNs)\nbased machine learning approaches for modern power grid stability control\nincluding their design and implementation methodologies. NNs are widely\naccepted as Artificial Intelligence (AI) approaches offering an alternative way\nto control complex and ill-defined problems. In this paper various application\nof NNs for power system rotor angle stabilization and control problem is\ndiscussed. The main focus of this paper is on the use of Reinforcement Learning\n(RL) and Supervised Learning (SL) algorithms in power system wide-area control\n(WAC). Generally, these algorithms due to their capability in modeling\nnonlinearities and uncertainties are used for transient classification,\nneuro-control, wide-area monitoring and control, renewable energy management\nand control, and so on. The works of researchers in the field of conventional\nand renewable energy systems are reported and categorized. Paper concludes by\npresenting, comparing and evaluating various learning techniques and\ninfrastructure configurations based on efficiency.",
        "url": "http://arxiv.org/pdf/1701.01214v1.pdf"
    },
    {
        "title": "p-DLA: A Predictive System Model for Onshore Oil and Gas Pipeline Dataset Classification and Monitoring - Part 1",
        "abstract": "With the rise in militant activity and rogue behaviour in oil and gas regions\naround the world, oil pipeline disturbances is on the increase leading to huge\nlosses to multinational operators and the countries where such facilities\nexist. However, this situation can be averted if adequate predictive monitoring\nschemes are put in place. We propose in the first part of this paper, an\nartificial intelligence predictive monitoring system capable of predictive\nclassification and pattern recognition of pipeline datasets. The predictive\nsystem is based on a highly sparse predictive Deviant Learning Algorithm\n(p-DLA) designed to synthesize a sequence of memory predictive clusters for\neventual monitoring, control and decision making. The DLA (p-DLA) is compared\nwith a popular machine learning algorithm, the Long Short-Term Memory (LSTM)\nwhich is based on a temporal version of the standard feed-forward\nback-propagation trained artificial neural networks (ANNs). The results of\nsimulations study show impressive results and validates the sparse memory\npredictive approach which favours the sub-synthesis of a highly compressed and\nlow dimensional knowledge discovery and information prediction scheme. It also\nshows that the proposed new approach is competitive with a well-known and\nproven AI approach such as the LSTM.",
        "url": "http://arxiv.org/pdf/1701.00040v1.pdf"
    },
    {
        "title": "Counterfactual Prediction with Deep Instrumental Variables Networks",
        "abstract": "We are in the middle of a remarkable rise in the use and capability of\nartificial intelligence. Much of this growth has been fueled by the success of\ndeep learning architectures: models that map from observables to outputs via\nmultiple layers of latent representations. These deep learning algorithms are\neffective tools for unstructured prediction, and they can be combined in AI\nsystems to solve complex automated reasoning problems. This paper provides a\nrecipe for combining ML algorithms to solve for causal effects in the presence\nof instrumental variables -- sources of treatment randomization that are\nconditionally independent from the response. We show that a flexible IV\nspecification resolves into two prediction tasks that can be solved with deep\nneural nets: a first-stage network for treatment prediction and a second-stage\nnetwork whose loss function involves integration over the conditional treatment\ndistribution. This Deep IV framework imposes some specific structure on the\nstochastic gradient descent routine used for training, but it is general enough\nthat we can take advantage of off-the-shelf ML capabilities and avoid extensive\nalgorithm customization. We outline how to obtain out-of-sample causal\nvalidation in order to avoid over-fit. We also introduce schemes for both\nBayesian and frequentist inference: the former via a novel adaptation of\ndropout training, and the latter via a data splitting routine.",
        "url": "http://arxiv.org/pdf/1612.09596v1.pdf"
    },
    {
        "title": "Accelerated Convolutions for Efficient Multi-Scale Time to Contact Computation in Julia",
        "abstract": "Convolutions have long been regarded as fundamental to applied mathematics,\nphysics and engineering. Their mathematical elegance allows for common tasks\nsuch as numerical differentiation to be computed efficiently on large data\nsets. Efficient computation of convolutions is critical to artificial\nintelligence in real-time applications, like machine vision, where convolutions\nmust be continuously and efficiently computed on tens to hundreds of kilobytes\nper second. In this paper, we explore how convolutions are used in fundamental\nmachine vision applications. We present an accelerated n-dimensional\nconvolution package in the high performance computing language, Julia, and\ndemonstrate its efficacy in solving the time to contact problem for machine\nvision. Results are measured against synthetically generated videos and\nquantitatively assessed according to their mean squared error from the ground\ntruth. We achieve over an order of magnitude decrease in compute time and\nallocated memory for comparable machine vision applications. All code is\npackaged and integrated into the official Julia Package Manager to be used in\nvarious other scenarios.",
        "url": "http://arxiv.org/pdf/1612.08825v1.pdf"
    },
    {
        "title": "Solving Set Optimization Problems by Cardinality Optimization via Weak Constraints with an Application to Argumentation",
        "abstract": "Optimization - minimization or maximization - in the lattice of subsets is a\nfrequent operation in Artificial Intelligence tasks. Examples are\nsubset-minimal model-based diagnosis, nonmonotonic reasoning by means of\ncircumscription, or preferred extensions in abstract argumentation. Finding the\noptimum among many admissible solutions is often harder than finding admissible\nsolutions with respect to both computational complexity and methodology. This\npaper addresses the former issue by means of an effective method for finding\nsubset-optimal solutions. It is based on the relationship between\ncardinality-optimal and subset-optimal solutions, and the fact that many\nlogic-based declarative programming systems provide constructs for finding\ncardinality-optimal solutions, for example maximum satisfiability (MaxSAT) or\nweak constraints in Answer Set Programming (ASP). Clearly each\ncardinality-optimal solution is also a subset-optimal one, and if the language\nalso allows for the addition of particular restricting constructs (both MaxSAT\nand ASP do) then all subset-optimal solutions can be found by an iterative\ncomputation of cardinality-optimal solutions. As a showcase, the computation of\npreferred extensions of abstract argumentation frameworks using the proposed\nmethod is studied.",
        "url": "http://arxiv.org/pdf/1612.07589v1.pdf"
    },
    {
        "title": "The SP Theory of Intelligence as a Foundation for the Development of a General, Human-Level Thinking Machine",
        "abstract": "This paper summarises how the \"SP theory of intelligence\" and its realisation\nin the \"SP computer model\" simplifies and integrates concepts across artificial\nintelligence and related areas, and thus provides a promising foundation for\nthe development of a general, human-level thinking machine, in accordance with\nthe main goal of research in artificial general intelligence.\n  The key to this simplification and integration is the powerful concept of\n\"multiple alignment\", borrowed and adapted from bioinformatics. This concept\nhas the potential to be the \"double helix\" of intelligence, with as much\nsignificance for human-level intelligence as has DNA for biological sciences.\n  Strengths of the SP system include: versatility in the representation of\ndiverse kinds of knowledge; versatility in aspects of intelligence (including:\nstrengths in unsupervised learning; the processing of natural language; pattern\nrecognition at multiple levels of abstraction that is robust in the face of\nerrors in data; several kinds of reasoning (including: one-step `deductive'\nreasoning; chains of reasoning; abductive reasoning; reasoning with\nprobabilistic networks and trees; reasoning with 'rules'; nonmonotonic\nreasoning and reasoning with default values; Bayesian reasoning with\n'explaining away'; and more); planning; problem solving; and more); seamless\nintegration of diverse kinds of knowledge and diverse aspects of intelligence\nin any combination; and potential for application in several areas (including:\nhelping to solve nine problems with big data; helping to develop human-level\nintelligence in autonomous robots; serving as a database with intelligence and\nwith versatility in the representation and integration of several forms of\nknowledge; serving as a vehicle for medical knowledge and as an aid to medical\ndiagnosis; and several more).",
        "url": "http://arxiv.org/pdf/1612.07555v1.pdf"
    },
    {
        "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
        "abstract": "When building artificial intelligence systems that can reason and answer\nquestions about visual data, we need diagnostic tests to analyze our progress\nand discover shortcomings. Existing benchmarks for visual question answering\ncan help, but have strong biases that models can exploit to correctly answer\nquestions without reasoning. They also conflate multiple sources of error,\nmaking it hard to pinpoint model weaknesses. We present a diagnostic dataset\nthat tests a range of visual reasoning abilities. It contains minimal biases\nand has detailed annotations describing the kind of reasoning each question\nrequires. We use this dataset to analyze a variety of modern visual reasoning\nsystems, providing novel insights into their abilities and limitations.",
        "url": "http://arxiv.org/pdf/1612.06890v1.pdf"
    },
    {
        "title": "Machine Reading with Background Knowledge",
        "abstract": "Intelligent systems capable of automatically understanding natural language\ntext are important for many artificial intelligence applications including\nmobile phone voice assistants, computer vision, and robotics. Understanding\nlanguage often constitutes fitting new information into a previously acquired\nview of the world. However, many machine reading systems rely on the text alone\nto infer its meaning. In this paper, we pursue a different approach; machine\nreading methods that make use of background knowledge to facilitate language\nunderstanding. To this end, we have developed two methods: The first method\naddresses prepositional phrase attachment ambiguity. It uses background\nknowledge within a semi-supervised machine learning algorithm that learns from\nboth labeled and unlabeled data. This approach yields state-of-the-art results\non two datasets against strong baselines; The second method extracts\nrelationships from compound nouns. Our knowledge-aware method for compound noun\nanalysis accurately extracts relationships and significantly outperforms a\nbaseline that does not make use of background knowledge.",
        "url": "http://arxiv.org/pdf/1612.05348v1.pdf"
    },
    {
        "title": "A Survey of Inductive Biases for Factorial Representation-Learning",
        "abstract": "With the resurgence of interest in neural networks, representation learning\nhas re-emerged as a central focus in artificial intelligence. Representation\nlearning refers to the discovery of useful encodings of data that make\ndomain-relevant information explicit. Factorial representations identify\nunderlying independent causal factors of variation in data. A factorial\nrepresentation is compact and faithful, makes the causal factors explicit, and\nfacilitates human interpretation of data. Factorial representations support a\nvariety of applications, including the generation of novel examples, indexing\nand search, novelty detection, and transfer learning.\n  This article surveys various constraints that encourage a learning algorithm\nto discover factorial representations. I dichotomize the constraints in terms\nof unsupervised and supervised inductive bias. Unsupervised inductive biases\nexploit assumptions about the environment, such as the statistical distribution\nof factor coefficients, assumptions about the perturbations a factor should be\ninvariant to (e.g. a representation of an object can be invariant to rotation,\ntranslation or scaling), and assumptions about how factors are combined to\nsynthesize an observation. Supervised inductive biases are constraints on the\nrepresentations based on additional information connected to observations.\nSupervisory labels come in variety of types, which vary in how strongly they\nconstrain the representation, how many factors are labeled, how many\nobservations are labeled, and whether or not we know the associations between\nthe constraints and the factors they are related to.\n  This survey brings together a wide variety of models that all touch on the\nproblem of learning factorial representations and lays out a framework for\ncomparing these models based on the strengths of the underlying supervised and\nunsupervised inductive biases.",
        "url": "http://arxiv.org/pdf/1612.05299v1.pdf"
    },
    {
        "title": "Interpretable Semantic Textual Similarity: Finding and explaining differences between sentences",
        "abstract": "User acceptance of artificial intelligence agents might depend on their\nability to explain their reasoning, which requires adding an interpretability\nlayer that fa- cilitates users to understand their behavior. This paper focuses\non adding an in- terpretable layer on top of Semantic Textual Similarity (STS),\nwhich measures the degree of semantic equivalence between two sentences. The\ninterpretability layer is formalized as the alignment between pairs of segments\nacross the two sentences, where the relation between the segments is labeled\nwith a relation type and a similarity score. We present a publicly available\ndataset of sentence pairs annotated following the formalization. We then\ndevelop a system trained on this dataset which, given a sentence pair, explains\nwhat is similar and different, in the form of graded and typed segment\nalignments. When evaluated on the dataset, the system performs better than an\ninformed baseline, showing that the dataset and task are well-defined and\nfeasible. Most importantly, two user studies show how the system output can be\nused to automatically produce explanations in natural language. Users performed\nbetter when having access to the explanations, pro- viding preliminary evidence\nthat our dataset and method to automatically produce explanations is useful in\nreal applications.",
        "url": "http://arxiv.org/pdf/1612.04868v1.pdf"
    },
    {
        "title": "DeepMind Lab",
        "abstract": "DeepMind Lab is a first-person 3D game platform designed for research and\ndevelopment of general artificial intelligence and machine learning systems.\nDeepMind Lab can be used to study how autonomous artificial agents may learn\ncomplex tasks in large, partially observed, and visually diverse worlds.\nDeepMind Lab has a simple and flexible API enabling creative task-designs and\nnovel AI-designs to be explored and quickly iterated upon. It is powered by a\nfast and widely recognised game engine, and tailored for effective use by the\nresearch community.",
        "url": "http://arxiv.org/pdf/1612.03801v2.pdf"
    },
    {
        "title": "Composing Music with Grammar Argumented Neural Networks and Note-Level Encoding",
        "abstract": "Creating aesthetically pleasing pieces of art, including music, has been a\nlong-term goal for artificial intelligence research. Despite recent successes\nof long-short term memory (LSTM) recurrent neural networks (RNNs) in sequential\nlearning, LSTM neural networks have not, by themselves, been able to generate\nnatural-sounding music conforming to music theory. To transcend this\ninadequacy, we put forward a novel method for music composition that combines\nthe LSTM with Grammars motivated by music theory. The main tenets of music\ntheory are encoded as grammar argumented (GA) filters on the training data,\nsuch that the machine can be trained to generate music inheriting the\nnaturalness of human-composed pieces from the original dataset while adhering\nto the rules of music theory. Unlike previous approaches, pitches and durations\nare encoded as one semantic entity, which we refer to as note-level encoding.\nThis allows easy implementation of music theory grammars, as well as closer\nemulation of the thinking pattern of a musician. Although the GA rules are\napplied to the training data and never directly to the LSTM music generation,\nour machine still composes music that possess high incidences of diatonic scale\nnotes, small pitch intervals and chords, in deference to music theory.",
        "url": "http://arxiv.org/pdf/1611.05416v2.pdf"
    },
    {
        "title": "Contextual RNN-GANs for Abstract Reasoning Diagram Generation",
        "abstract": "Understanding, predicting, and generating object motions and transformations\nis a core problem in artificial intelligence. Modeling sequences of evolving\nimages may provide better representations and models of motion and may\nultimately be used for forecasting, simulation, or video generation.\nDiagrammatic Abstract Reasoning is an avenue in which diagrams evolve in\ncomplex patterns and one needs to infer the underlying pattern sequence and\ngenerate the next image in the sequence. For this, we develop a novel\nContextual Generative Adversarial Network based on Recurrent Neural Networks\n(Context-RNN-GANs), where both the generator and the discriminator modules are\nbased on contextual history (modeled as RNNs) and the adversarial discriminator\nguides the generator to produce realistic images for the particular time step\nin the image sequence. We evaluate the Context-RNN-GAN model (and its variants)\non a novel dataset of Diagrammatic Abstract Reasoning, where it performs\ncompetitively with 10th-grade human performance but there is still scope for\ninteresting improvements as compared to college-grade human performance. We\nalso evaluate our model on a standard video next-frame prediction task,\nachieving improved performance over comparable state-of-the-art.",
        "url": "http://arxiv.org/pdf/1609.09444v2.pdf"
    },
    {
        "title": "Review of state-of-the-arts in artificial intelligence with application to AI safety problem",
        "abstract": "Here, I review current state-of-the-arts in many areas of AI to estimate when\nit's reasonable to expect human level AI development. Predictions of prominent\nAI researchers vary broadly from very pessimistic predictions of Andrew Ng to\nmuch more moderate predictions of Geoffrey Hinton and optimistic predictions of\nShane Legg, DeepMind cofounder. Given huge rate of progress in recent years and\nthis broad range of predictions of AI experts, AI safety questions are also\ndiscussed.",
        "url": "http://arxiv.org/pdf/1605.04232v2.pdf"
    },
    {
        "title": "AI Researchers, Video Games Are Your Friends!",
        "abstract": "If you are an artificial intelligence researcher, you should look to video\ngames as ideal testbeds for the work you do. If you are a video game developer,\nyou should look to AI for the technology that makes completely new types of\ngames possible. This chapter lays out the case for both of these propositions.\nIt asks the question \"what can video games do for AI\", and discusses how in\nparticular general video game playing is the ideal testbed for artificial\ngeneral intelligence research. It then asks the question \"what can AI do for\nvideo games\", and lays out a vision for what video games might look like if we\nhad significantly more advanced AI at our disposal. The chapter is based on my\nkeynote at IJCCI 2015, and is written in an attempt to be accessible to a broad\naudience.",
        "url": "http://arxiv.org/pdf/1612.01608v1.pdf"
    },
    {
        "title": "Evaluating the Performance of ANN Prediction System at Shanghai Stock Market in the Period 21-Sep-2016 to 11-Oct-2016",
        "abstract": "This research evaluates the performance of an Artificial Neural Network based\nprediction system that was employed on the Shanghai Stock Exchange for the\nperiod 21-Sep-2016 to 11-Oct-2016. It is a follow-up to a previous paper in\nwhich the prices were predicted and published before September 21. Stock market\nprice prediction remains an important quest for investors and researchers. This\nresearch used an Artificial Intelligence system, being an Artificial Neural\nNetwork that is feedforward multi-layer perceptron with error backpropagation\nfor prediction, unlike other methods such as technical, fundamental or time\nseries analysis. While these alternative methods tend to guide on trends and\nnot the exact likely prices, neural networks on the other hand have the ability\nto predict the real value prices, as was done on this research. Nonetheless,\ndetermination of suitable network parameters remains a challenge in neural\nnetwork design, with this research settling on a configuration of 5:21:21:1\nwith 80% training data or 4-year of training data as a good enough model for\nstock prediction, as already determined in a previous research by the author.\nThe comparative results indicate that neural network can predict typical stock\nmarket prices with mean absolute percentage errors that are as low as 1.95%\nover the ten prediction instances that was studied in this research.",
        "url": "http://arxiv.org/pdf/1612.02666v1.pdf"
    },
    {
        "title": "Short-term traffic flow forecasting with spatial-temporal correlation in a hybrid deep learning framework",
        "abstract": "Deep learning approaches have reached a celebrity status in artificial\nintelligence field, its success have mostly relied on Convolutional Networks\n(CNN) and Recurrent Networks. By exploiting fundamental spatial properties of\nimages and videos, the CNN always achieves dominant performance on visual\ntasks. And the Recurrent Networks (RNN) especially long short-term memory\nmethods (LSTM) can successfully characterize the temporal correlation, thus\nexhibits superior capability for time series tasks. Traffic flow data have\nplentiful characteristics on both time and space domain. However, applications\nof CNN and LSTM approaches on traffic flow are limited. In this paper, we\npropose a novel deep architecture combined CNN and LSTM to forecast future\ntraffic flow (CLTFP). An 1-dimension CNN is exploited to capture spatial\nfeatures of traffic flow, and two LSTMs are utilized to mine the short-term\nvariability and periodicities of traffic flow. Given those meaningful features,\nthe feature-level fusion is performed to achieve short-term forecasting. The\nproposed CLTFP is compared with other popular forecasting methods on an open\ndatasets. Experimental results indicate that the CLTFP has considerable\nadvantages in traffic flow forecasting. in additional, the proposed CLTFP is\nanalyzed from the view of Granger Causality, and several interesting properties\nof CLTFP are discovered and discussed .",
        "url": "http://arxiv.org/pdf/1612.01022v1.pdf"
    },
    {
        "title": "Long-Term Trends in the Public Perception of Artificial Intelligence",
        "abstract": "Analyses of text corpora over time can reveal trends in beliefs, interest,\nand sentiment about a topic. We focus on views expressed about artificial\nintelligence (AI) in the New York Times over a 30-year period. General\ninterest, awareness, and discussion about AI has waxed and waned since the\nfield was founded in 1956. We present a set of measures that captures levels of\nengagement, measures of pessimism and optimism, the prevalence of specific\nhopes and concerns, and topics that are linked to discussions about AI over\ndecades. We find that discussion of AI has increased sharply since 2009, and\nthat these discussions have been consistently more optimistic than pessimistic.\nHowever, when we examine specific concerns, we find that worries of loss of\ncontrol of AI, ethical concerns for AI, and the negative impact of AI on work\nhave grown in recent years. We also find that hopes for AI in healthcare and\neducation have increased over time.",
        "url": "http://arxiv.org/pdf/1609.04904v2.pdf"
    },
    {
        "title": "Machine Learning for Dental Image Analysis",
        "abstract": "In order to study the application of artificial intelligence (AI) to dental\nimaging, we applied AI technology to classify a set of panoramic radiographs\nusing (a) a convolutional neural network (CNN) which is a form of an artificial\nneural network (ANN), (b) representative image cognition algorithms that\nimplement scale-invariant feature transform (SIFT), and (c) histogram of\noriented gradients (HOG).",
        "url": "http://arxiv.org/pdf/1611.09958v2.pdf"
    },
    {
        "title": "Interaction Networks for Learning about Objects, Relations and Physics",
        "abstract": "Reasoning about objects, relations, and physics is central to human\nintelligence, and a key goal of artificial intelligence. Here we introduce the\ninteraction network, a model which can reason about how objects in complex\nsystems interact, supporting dynamical predictions, as well as inferences about\nthe abstract properties of the system. Our model takes graphs as input,\nperforms object- and relation-centric reasoning in a way that is analogous to a\nsimulation, and is implemented using deep neural networks. We evaluate its\nability to reason about several challenging physical domains: n-body problems,\nrigid-body collision, and non-rigid dynamics. Our results show it can be\ntrained to accurately simulate the physical trajectories of dozens of objects\nover thousands of time steps, estimate abstract quantities such as energy, and\ngeneralize automatically to systems with different numbers and configurations\nof objects and relations. Our interaction network implementation is the first\ngeneral-purpose, learnable physics engine, and a powerful general framework for\nreasoning about object and relations in a wide variety of complex real-world\ndomains.",
        "url": "http://arxiv.org/pdf/1612.00222v1.pdf"
    },
    {
        "title": "Quantum Enhanced Inference in Markov Logic Networks",
        "abstract": "Markov logic networks (MLNs) reconcile two opposing schools in machine\nlearning and artificial intelligence: causal networks, which account for\nuncertainty extremely well, and first-order logic, which allows for formal\ndeduction. An MLN is essentially a first-order logic template to generate\nMarkov networks. Inference in MLNs is probabilistic and it is often performed\nby approximate methods such as Markov chain Monte Carlo (MCMC) Gibbs sampling.\nAn MLN has many regular, symmetric structures that can be exploited at both\nfirst-order level and in the generated Markov network. We analyze the graph\nstructures that are produced by various lifting methods and investigate the\nextent to which quantum protocols can be used to speed up Gibbs sampling with\nstate preparation and measurement schemes. We review different such approaches,\ndiscuss their advantages, theoretical limitations, and their appeal to\nimplementations. We find that a straightforward application of a recent result\nyields exponential speedup compared to classical heuristics in approximate\nprobabilistic inference, thereby demonstrating another example where advanced\nquantum resources can potentially prove useful in machine learning.",
        "url": "http://arxiv.org/pdf/1611.08104v1.pdf"
    },
    {
        "title": "Double-quantitative $\u03b3^{\\ast}-$fuzzy coverings approximation operators",
        "abstract": "In digital-based information boom, the fuzzy covering rough set model is an\nimportant mathematical tool for artificial intelligence, and how to build the\nbridge between the fuzzy covering rough set theory and Pawlak's model is\nbecoming a hot research topic. In this paper, we first present the\n$\\gamma-$fuzzy covering based probabilistic and grade approximation operators\nand double-quantitative approximation operators. We also study the\nrelationships among the three types of $\\gamma-$fuzzy covering based\napproximation operators. Second, we propose the $\\gamma^{\\ast}-$fuzzy coverings\nbased multi-granulation probabilistic and grade lower and upper approximation\noperators and multi-granulation double-quantitative lower and upper\napproximation operators. We also investigate the relationships among these\ntypes of $\\gamma-$fuzzy coverings based approximation operators. Finally, we\nemploy several examples to illustrate how to construct the lower and upper\napproximations of fuzzy sets with the absolute and relative quantitative\ninformation.",
        "url": "http://arxiv.org/pdf/1611.08103v1.pdf"
    },
    {
        "title": "Latent Dependency Forest Models",
        "abstract": "Probabilistic modeling is one of the foundations of modern machine learning\nand artificial intelligence. In this paper, we propose a novel type of\nprobabilistic models named latent dependency forest models (LDFMs). A LDFM\nmodels the dependencies between random variables with a forest structure that\ncan change dynamically based on the variable values. It is therefore capable of\nmodeling context-specific independence. We parameterize a LDFM using a\nfirst-order non-projective dependency grammar. Learning LDFMs from data can be\nformulated purely as a parameter learning problem, and hence the difficult\nproblem of model structure learning is circumvented. Our experimental results\nshow that LDFMs are competitive with existing probabilistic models.",
        "url": "http://arxiv.org/pdf/1609.02236v2.pdf"
    },
    {
        "title": "Finite LTL Synthesis is EXPTIME-complete",
        "abstract": "LTL synthesis -- the construction of a function to satisfy a logical\nspecification formulated in Linear Temporal Logic -- is a 2EXPTIME-complete\nproblem with relevant applications in controller synthesis and a myriad of\nartificial intelligence applications. In this research note we consider De\nGiacomo and Vardi's variant of the synthesis problem for LTL formulas\ninterpreted over finite rather than infinite traces. Rather surprisingly, given\nthe existing claims on complexity, we establish that LTL synthesis is\nEXPTIME-complete for the finite interpretation, and not 2EXPTIME-complete as\npreviously reported. Our result coincides nicely with the planning perspective\nwhere non-deterministic planning with full observability is EXPTIME-complete\nand partial observability increases the complexity to 2EXPTIME-complete; a\nrecent related result for LTL synthesis shows that in the finite case with\npartial observability, the problem is 2EXPTIME-complete.",
        "url": "http://arxiv.org/pdf/1609.04371v2.pdf"
    },
    {
        "title": "PCT and Beyond: Towards a Computational Framework for `Intelligent' Communicative Systems",
        "abstract": "Recent years have witnessed increasing interest in the potential benefits of\n`intelligent' autonomous machines such as robots. Honda's Asimo humanoid robot,\niRobot's Roomba robot vacuum cleaner and Google's driverless cars have fired\nthe imagination of the general public, and social media buzz with speculation\nabout a utopian world of helpful robot assistants or the coming robot\napocalypse! However, there is a long way to go before autonomous systems reach\nthe level of capabilities required for even the simplest of tasks involving\nhuman-robot interaction - especially if it involves communicative behaviour\nsuch as speech and language. Of course the field of Artificial Intelligence\n(AI) has made great strides in these areas, and has moved on from abstract\nhigh-level rule-based paradigms to embodied architectures whose operations are\ngrounded in real physical environments. What is still missing, however, is an\noverarching theory of intelligent communicative behaviour that informs\nsystem-level design decisions in order to provide a more coherent approach to\nsystem integration. This chapter introduces the beginnings of such a framework\ninspired by the principles of Perceptual Control Theory (PCT). In particular,\nit is observed that PCT has hitherto tended to view perceptual processes as a\nrelatively straightforward series of transformations from sensation to\nperception, and has overlooked the potential of powerful generative model-based\nsolutions that have emerged in practical fields such as visual or auditory\nscene analysis. Starting from first principles, a sequence of arguments is\npresented which not only shows how these ideas might be integrated into PCT,\nbut which also extend PCT towards a remarkably symmetric architecture for a\nneeds-driven communicative agent. It is concluded that, if behaviour is the\ncontrol of perception, then perception is the simulation of behaviour.",
        "url": "http://arxiv.org/pdf/1611.05379v1.pdf"
    },
    {
        "title": "ASPeRiX, a First Order Forward Chaining Approach for Answer Set Computing",
        "abstract": "The natural way to use Answer Set Programming (ASP) to represent knowledge in\nArtificial Intelligence or to solve a combinatorial problem is to elaborate a\nfirst order logic program with default negation. In a preliminary step this\nprogram with variables is translated in an equivalent propositional one by a\nfirst tool: the grounder. Then, the propositional program is given to a second\ntool: the solver. This last one computes (if they exist) one or many answer\nsets (stable models) of the program, each answer set encoding one solution of\nthe initial problem. Until today, almost all ASP systems apply this two steps\ncomputation. In this article, the project ASPeRiX is presented as a first order\nforward chaining approach for Answer Set Computing. This project was amongst\nthe first to introduce an approach of answer set computing that escapes the\npreliminary phase of rule instantiation by integrating it in the search\nprocess. The methodology applies a forward chaining of first order rules that\nare grounded on the fly by means of previously produced atoms. Theoretical\nfoundations of the approach are presented, the main algorithms of the ASP\nsolver ASPeRiX are detailed and some experiments and comparisons with existing\nsystems are provided.",
        "url": "http://arxiv.org/pdf/1503.07717v2.pdf"
    },
    {
        "title": "Generating Images Part by Part with Composite Generative Adversarial Networks",
        "abstract": "Image generation remains a fundamental problem in artificial intelligence in\ngeneral and deep learning in specific. The generative adversarial network (GAN)\nwas successful in generating high quality samples of natural images. We propose\na model called composite generative adversarial network, that reveals the\ncomplex structure of images with multiple generators in which each generator\ngenerates some part of the image. Those parts are combined by alpha blending\nprocess to create a new single image. It can generate, for example, background\nand face sequentially with two generators, after training on face dataset.\nTraining was done in an unsupervised way without any labels about what each\ngenerator should generate. We found possibilities of learning the structure by\nusing this generative model empirically.",
        "url": "http://arxiv.org/pdf/1607.05387v2.pdf"
    },
    {
        "title": "Harnessing disordered quantum dynamics for machine learning",
        "abstract": "Quantum computer has an amazing potential of fast information processing.\nHowever, realisation of a digital quantum computer is still a challenging\nproblem requiring highly accurate controls and key application strategies. Here\nwe propose a novel platform, quantum reservoir computing, to solve these issues\nsuccessfully by exploiting natural quantum dynamics, which is ubiquitous in\nlaboratories nowadays, for machine learning. In this framework, nonlinear\ndynamics including classical chaos can be universally emulated in quantum\nsystems. A number of numerical experiments show that quantum systems consisting\nof at most seven qubits possess computational capabilities comparable to\nconventional recurrent neural networks of 500 nodes. This discovery opens up a\nnew paradigm for information processing with artificial intelligence powered by\nquantum physics.",
        "url": "http://arxiv.org/pdf/1602.08159v2.pdf"
    },
    {
        "title": "Building Machines That Learn and Think Like People",
        "abstract": "Recent progress in artificial intelligence (AI) has renewed interest in\nbuilding systems that learn and think like people. Many advances have come from\nusing deep neural networks trained end-to-end in tasks such as object\nrecognition, video games, and board games, achieving performance that equals or\neven beats humans in some respects. Despite their biological inspiration and\nperformance achievements, these systems differ from human intelligence in\ncrucial ways. We review progress in cognitive science suggesting that truly\nhuman-like learning and thinking machines will have to reach beyond current\nengineering trends in both what they learn, and how they learn it.\nSpecifically, we argue that these machines should (a) build causal models of\nthe world that support explanation and understanding, rather than merely\nsolving pattern recognition problems; (b) ground learning in intuitive theories\nof physics and psychology, to support and enrich the knowledge that is learned;\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\ngeneralize knowledge to new tasks and situations. We suggest concrete\nchallenges and promising routes towards these goals that can combine the\nstrengths of recent neural network advances with more structured cognitive\nmodels.",
        "url": "http://arxiv.org/pdf/1604.00289v3.pdf"
    },
    {
        "title": "A Framework for Searching for General Artificial Intelligence",
        "abstract": "There is a significant lack of unified approaches to building generally\nintelligent machines. The majority of current artificial intelligence research\noperates within a very narrow field of focus, frequently without considering\nthe importance of the 'big picture'. In this document, we seek to describe and\nunify principles that guide the basis of our development of general artificial\nintelligence. These principles revolve around the idea that intelligence is a\ntool for searching for general solutions to problems. We define intelligence as\nthe ability to acquire skills that narrow this search, diversify it and help\nsteer it to more promising areas. We also provide suggestions for studying,\nmeasuring, and testing the various skills and abilities that a human-level\nintelligent machine needs to acquire. The document aims to be both\nimplementation agnostic, and to provide an analytic, systematic, and scalable\nway to generate hypotheses that we believe are needed to meet the necessary\nconditions in the search for general artificial intelligence. We believe that\nsuch a framework is an important stepping stone for bringing together\ndefinitions, highlighting open problems, connecting researchers willing to\ncollaborate, and for unifying the arguably most significant search of this\ncentury.",
        "url": "http://arxiv.org/pdf/1611.00685v1.pdf"
    },
    {
        "title": "Using Artificial Intelligence to Identify State Secrets",
        "abstract": "Whether officials can be trusted to protect national security information has\nbecome a matter of great public controversy, reigniting a long-standing debate\nabout the scope and nature of official secrecy. The declassification of\nmillions of electronic records has made it possible to analyze these issues\nwith greater rigor and precision. Using machine-learning methods, we examined\nnearly a million State Department cables from the 1970s to identify features of\nrecords that are more likely to be classified, such as international\nnegotiations, military operations, and high-level communications. Even with\nincomplete data, algorithms can use such features to identify 90% of classified\ncables with <11% false positives. But our results also show that there are\nlongstanding problems in the identification of sensitive information. Error\nanalysis reveals many examples of both overclassification and\nunderclassification. This indicates both the need for research on inter-coder\nreliability among officials as to what constitutes classified material and the\nopportunity to develop recommender systems to better manage both classification\nand declassification.",
        "url": "http://arxiv.org/pdf/1611.00356v1.pdf"
    },
    {
        "title": "Embodiment of Learning in Electro-Optical Signal Processors",
        "abstract": "Delay-coupled electro-optical systems have received much attention for their\ndynamical properties and their potential use in signal processing. In\nparticular it has recently been demonstrated, using the artificial intelligence\nalgorithm known as reservoir computing, that photonic implementations of such\nsystems solve complex tasks such as speech recognition. Here we show how the\nbackpropagation algorithm can be physically implemented on the same\nelectro-optical delay-coupled architecture used for computation with only minor\nchanges to the original design. We find that, compared when the backpropagation\nalgorithm is not used, the error rate of the resulting computing device,\nevaluated on three benchmark tasks, decreases considerably. This demonstrates\nthat electro-optical analog computers can embody a large part of their own\ntraining process, allowing them to be applied to new, more difficult tasks.",
        "url": "http://arxiv.org/pdf/1610.06269v2.pdf"
    },
    {
        "title": "Quantum-enhanced machine learning",
        "abstract": "The emerging field of quantum machine learning has the potential to\nsubstantially aid in the problems and scope of artificial intelligence. This is\nonly enhanced by recent successes in the field of classical machine learning.\nIn this work we propose an approach for the systematic treatment of machine\nlearning, from the perspective of quantum information. Our approach is general\nand covers all three main branches of machine learning: supervised,\nunsupervised and reinforcement learning. While quantum improvements in\nsupervised and unsupervised learning have been reported, reinforcement learning\nhas received much less attention. Within our approach, we tackle the problem of\nquantum enhancements in reinforcement learning as well, and propose a\nsystematic scheme for providing improvements. As an example, we show that\nquadratic improvements in learning efficiency, and exponential improvements in\nperformance over limited time periods, can be obtained for a broad class of\nlearning problems.",
        "url": "http://arxiv.org/pdf/1610.08251v1.pdf"
    },
    {
        "title": "Intelligence in Artificial Intelligence",
        "abstract": "The elusive quest for intelligence in artificial intelligence prompts us to\nconsider that instituting human-level intelligence in systems may be (still) in\nthe realm of utopia. In about a quarter century, we have witnessed the winter\nof AI (1990) being transformed and transported to the zenith of tabloid fodder\nabout AI (2015). The discussion at hand is about the elements that constitute\nthe canonical idea of intelligence. The delivery of intelligence as a\npay-per-use-service, popping out of an app or from a shrink-wrapped software\ndefined point solution, is in contrast to the bio-inspired view of intelligence\nas an outcome, perhaps formed from a tapestry of events, cross-pollinated by\ninstances, each with its own microcosm of experiences and learning, which may\nnot be discrete all-or-none functions but continuous, over space and time. The\nenterprise world may not require, aspire or desire such an engaged solution to\nimprove its services for enabling digital transformation through the deployment\nof digital twins, for example. One might ask whether the \"work-flow on\nsteroids\" version of decision support may suffice for intelligence? Are we\nharking back to the era of rule based expert systems? The image conjured by the\npublicity machines offers deep solutions with human-level AI and preposterous\nclaims about capturing the \"brain in a box\" by 2020. Even emulating insects may\nbe difficult in terms of real progress. Perhaps we can try to focus on worms\n(Caenorhabditis elegans) which may be better suited for what business needs to\nquench its thirst for so-called intelligence in AI.",
        "url": "http://arxiv.org/pdf/1610.07862v2.pdf"
    },
    {
        "title": "Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures",
        "abstract": "In this work, we present and analyze reported failures of artificially\nintelligent systems and extrapolate our analysis to future AIs. We suggest that\nboth the frequency and the seriousness of future AI failures will steadily\nincrease. AI Safety can be improved based on ideas developed by cybersecurity\nexperts. For narrow AIs safety failures are at the same, moderate, level of\ncriticality as in cybersecurity, however for general AI, failures have a\nfundamentally different impact. A single failure of a superintelligent system\nmay cause a catastrophic event without a chance for recovery. The goal of\ncybersecurity is to reduce the number of successful attacks on the system; the\ngoal of AI Safety is to make sure zero attacks succeed in bypassing the safety\nmechanisms. Unfortunately, such a level of performance is unachievable. Every\nsecurity system will eventually fail; there is no such thing as a 100% secure\nsystem.",
        "url": "http://arxiv.org/pdf/1610.07997v1.pdf"
    },
    {
        "title": "Virtual Embodiment: A Scalable Long-Term Strategy for Artificial Intelligence Research",
        "abstract": "Meaning has been called the \"holy grail\" of a variety of scientific\ndisciplines, ranging from linguistics to philosophy, psychology and the\nneurosciences. The field of Artifical Intelligence (AI) is very much a part of\nthat list: the development of sophisticated natural language semantics is a\nsine qua non for achieving a level of intelligence comparable to humans.\nEmbodiment theories in cognitive science hold that human semantic\nrepresentation depends on sensori-motor experience; the abundant evidence that\nhuman meaning representation is grounded in the perception of physical reality\nleads to the conclusion that meaning must depend on a fusion of multiple\n(perceptual) modalities. Despite this, AI research in general, and its\nsubdisciplines such as computational linguistics and computer vision in\nparticular, have focused primarily on tasks that involve a single modality.\nHere, we propose virtual embodiment as an alternative, long-term strategy for\nAI research that is multi-modal in nature and that allows for the kind of\nscalability required to develop the field coherently and incrementally, in an\nethically responsible fashion.",
        "url": "http://arxiv.org/pdf/1610.07432v1.pdf"
    },
    {
        "title": "Semantic Measures for the Comparison of Units of Language, Concepts or Instances from Text and Knowledge Base Analysis",
        "abstract": "Semantic measures are widely used today to estimate the strength of the\nsemantic relationship between elements of various types: units of language\n(e.g., words, sentences, documents), concepts or even instances semantically\ncharacterized (e.g., diseases, genes, geographical locations). Semantic\nmeasures play an important role to compare such elements according to semantic\nproxies: texts and knowledge representations, which support their meaning or\ndescribe their nature. Semantic measures are therefore essential for designing\nintelligent agents which will for example take advantage of semantic analysis\nto mimic human ability to compare abstract or concrete objects. This paper\nproposes a comprehensive survey of the broad notion of semantic measure for the\ncomparison of units of language, concepts or instances based on semantic proxy\nanalyses. Semantic measures generalize the well-known notions of semantic\nsimilarity, semantic relatedness and semantic distance, which have been\nextensively studied by various communities over the last decades (e.g.,\nCognitive Sciences, Linguistics, and Artificial Intelligence to mention a few).",
        "url": "http://arxiv.org/pdf/1310.1285v3.pdf"
    },
    {
        "title": "Introduzione all'Intelligenza Artificiale",
        "abstract": "The paper presents an introduction to Artificial Intelligence (AI) in an accessible and informal but precise form. The paper focuses on the algorithmic aspects of the discipline, presenting the main techniques used in AI systems groped in symbolic and subsymbolic. The last part of the paper is devoted to the discussion ongoing among experts in the field and the public at large about on the advantages and disadvantages of AI and in particular on the possible dangers. The personal opinion of the author on this subject concludes the paper. -- -- L'articolo presenta un'introduzione all'Intelligenza Artificiale (IA) in forma divulgativa e informale ma precisa. L'articolo affronta prevalentemente gli aspetti informatici della disciplina, presentando le principali tecniche usate nei sistemi di IA divise in simboliche e subsimboliche. L'ultima parte dell'articolo presenta il dibattito in corso tra gli esperi e il pubblico su vantaggi e svantaggi dell'IA e in particolare sui possibili pericoli. L'articolo termina con l'opinione dell'autore al riguardo.",
        "url": "https://arxiv.org/pdf/1511.04352v3.pdf"
    },
    {
        "title": "Limits to Verification and Validation of Agentic Behavior",
        "abstract": "Verification and validation of agentic behavior have been suggested as\nimportant research priorities in efforts to reduce risks associated with the\ncreation of general artificial intelligence (Russell et al 2015). In this paper\nwe question the appropriateness of using language of certainty with respect to\nefforts to manage that risk. We begin by establishing a very general formalism\nto characterize agentic behavior and to describe standards of acceptable\nbehavior. We show that determination of whether an agent meets any particular\nstandard is not computable. We discuss the extent of the burden associated with\nverification by manual proof and by automated behavioral governance. We show\nthat to ensure decidability of the behavioral standard itself, one must further\nlimit the capabilities of the agent. We then demonstrate that if our concerns\nrelate to outcomes in the physical world, attempts at validation are futile.\nFinally, we show that layered architectures aimed at making these challenges\ntractable mistakenly equate intentions with actions or outcomes, thereby\nfailing to provide any guarantees. We conclude with a discussion of why\nlanguage of certainty should be eradicated from the conversation about the\nsafety of general artificial intelligence.",
        "url": "http://arxiv.org/pdf/1604.06963v2.pdf"
    },
    {
        "title": "Revisiting Multiple Instance Neural Networks",
        "abstract": "Recently neural networks and multiple instance learning are both attractive\ntopics in Artificial Intelligence related research fields. Deep neural networks\nhave achieved great success in supervised learning problems, and multiple\ninstance learning as a typical weakly-supervised learning method is effective\nfor many applications in computer vision, biometrics, nature language\nprocessing, etc. In this paper, we revisit the problem of solving multiple\ninstance learning problems using neural networks. Neural networks are appealing\nfor solving multiple instance learning problem. The multiple instance neural\nnetworks perform multiple instance learning in an end-to-end way, which take a\nbag with various number of instances as input and directly output bag label.\nAll of the parameters in a multiple instance network are able to be optimized\nvia back-propagation. We propose a new multiple instance neural network to\nlearn bag representations, which is different from the existing multiple\ninstance neural networks that focus on estimating instance label. In addition,\nrecent tricks developed in deep learning have been studied in multiple instance\nnetworks, we find deep supervision is effective for boosting bag classification\naccuracy. In the experiments, the proposed multiple instance networks achieve\nstate-of-the-art or competitive performance on several MIL benchmarks.\nMoreover, it is extremely fast for both testing and training, e.g., it takes\nonly 0.0003 second to predict a bag and a few seconds to train on a MIL\ndatasets on a moderate CPU.",
        "url": "http://arxiv.org/pdf/1610.02501v1.pdf"
    },
    {
        "title": "Deep Convolutional Networks as Models of Generalization and Blending Within Visual Creativity",
        "abstract": "We examine two recent artificial intelligence (AI) based deep learning algorithms for visual blending in convolutional neural networks (Mordvintsev et al. 2015, Gatys et al. 2015). To investigate the potential value of these algorithms as tools for computational creativity research, we explain and schematize the essential aspects of the algorithms' operation and give visual examples of their output. We discuss the relationship of the two algorithms to human cognitive science theories of creativity such as conceptual blending theory and honing theory, and characterize the algorithms with respect to generation of novelty and aesthetic quality.",
        "url": "https://arxiv.org/pdf/1610.02478v2.pdf"
    },
    {
        "title": "Metaheuristic Algorithms for Convolution Neural Network",
        "abstract": "A typical modern optimization technique is usually either heuristic or\nmetaheuristic. This technique has managed to solve some optimization problems\nin the research area of science, engineering, and industry. However,\nimplementation strategy of metaheuristic for accuracy improvement on\nconvolution neural networks (CNN), a famous deep learning method, is still\nrarely investigated. Deep learning relates to a type of machine learning\ntechnique, where its aim is to move closer to the goal of artificial\nintelligence of creating a machine that could successfully perform any\nintellectual tasks that can be carried out by a human. In this paper, we\npropose the implementation strategy of three popular metaheuristic approaches,\nthat is, simulated annealing, differential evolution, and harmony search, to\noptimize CNN. The performances of these metaheuristic methods in optimizing CNN\non classifying MNIST and CIFAR dataset were evaluated and compared.\nFurthermore, the proposed methods are also compared with the original CNN.\nAlthough the proposed methods show an increase in the computation time, their\naccuracy has also been improved (up to 7.14 percent).",
        "url": "http://arxiv.org/pdf/1610.01925v1.pdf"
    },
    {
        "title": "A Tour of TensorFlow",
        "abstract": "Deep learning is a branch of artificial intelligence employing deep neural\nnetwork architectures that has significantly advanced the state-of-the-art in\ncomputer vision, speech recognition, natural language processing and other\ndomains. In November 2015, Google released $\\textit{TensorFlow}$, an open\nsource deep learning software library for defining, training and deploying\nmachine learning models. In this paper, we review TensorFlow and put it in\ncontext of modern deep learning concepts and software. We discuss its basic\ncomputational paradigms and distributed execution model, its programming\ninterface as well as accompanying visualization toolkits. We then compare\nTensorFlow to alternative libraries such as Theano, Torch or Caffe on a\nqualitative as well as quantitative basis and finally comment on observed\nuse-cases of TensorFlow in academia and industry.",
        "url": "http://arxiv.org/pdf/1610.01178v1.pdf"
    },
    {
        "title": "Comprehensive Evaluation of OpenCL-based Convolutional Neural Network Accelerators in Xilinx and Altera FPGAs",
        "abstract": "Deep learning has significantly advanced the state of the art in artificial\nintelligence, gaining wide popularity from both industry and academia. Special\ninterest is around Convolutional Neural Networks (CNN), which take inspiration\nfrom the hierarchical structure of the visual cortex, to form deep layers of\nconvolutional operations, along with fully connected classifiers. Hardware\nimplementations of these deep CNN architectures are challenged with memory\nbottlenecks that require many convolution and fully-connected layers demanding\nlarge amount of communication for parallel computation. Multi-core CPU based\nsolutions have demonstrated their inadequacy for this problem due to the memory\nwall and low parallelism. Many-core GPU architectures show superior performance\nbut they consume high power and also have memory constraints due to\ninconsistencies between cache and main memory. FPGA design solutions are also\nactively being explored, which allow implementing the memory hierarchy using\nembedded BlockRAM. This boosts the parallel use of shared memory elements\nbetween multiple processing units, avoiding data replicability and\ninconsistencies. This makes FPGAs potentially powerful solutions for real-time\nclassification of CNNs. Both Altera and Xilinx have adopted OpenCL co-design\nframework from GPU for FPGA designs as a pseudo-automatic development solution.\nIn this paper, a comprehensive evaluation and comparison of Altera and Xilinx\nOpenCL frameworks for a 5-layer deep CNN is presented. Hardware resources,\ntemporal performance and the OpenCL architecture for CNNs are discussed. Xilinx\ndemonstrates faster synthesis, better FPGA resource utilization and more\ncompact boards. Altera provides multi-platforms tools, mature design community\nand better execution times.",
        "url": "http://arxiv.org/pdf/1609.09296v1.pdf"
    },
    {
        "title": "Topic Browsing for Research Papers with Hierarchical Latent Tree Analysis",
        "abstract": "Academic researchers often need to face with a large collection of research\npapers in the literature. This problem may be even worse for postgraduate\nstudents who are new to a field and may not know where to start. To address\nthis problem, we have developed an online catalog of research papers where the\npapers have been automatically categorized by a topic model. The catalog\ncontains 7719 papers from the proceedings of two artificial intelligence\nconferences from 2000 to 2015. Rather than the commonly used Latent Dirichlet\nAllocation, we use a recently proposed method called hierarchical latent tree\nanalysis for topic modeling. The resulting topic model contains a hierarchy of\ntopics so that users can browse the topics from the top level to the bottom\nlevel. The topic model contains a manageable number of general topics at the\ntop level and allows thousands of fine-grained topics at the bottom level. It\nalso can detect topics that have emerged recently.",
        "url": "http://arxiv.org/pdf/1609.09188v1.pdf"
    },
    {
        "title": "Correct classification for big/smart/fast data machine learning",
        "abstract": "Table (database) / Relational database Classification for big/smart/fast data\nmachine learning is one of the most important tasks of predictive analytics and\nextracting valuable information from data. It is core applied technique for\nwhat now understood under data science and/or artificial intelligence. Widely\nused Decision Tree (Random Forest) and rare used rule based PRISM , VFST, etc\nclassifiers are empirical substitutions of theoretically correct to use Boolean\nfunctions minimization. Developing Minimization of Boolean functions algorithms\nis started long time ago by Edward Veitch's 1952. Since it, big efforts by wide\nscientific/industrial community was done to find feasible solution of Boolean\nfunctions minimization. In this paper we propose consider table data\nclassification from mathematical point of view, as minimization of Boolean\nfunctions. It is shown that data representation may be transformed to Boolean\nfunctions form and how to use known algorithms. For simplicity, binary output\nfunction is used for development, what opens doors for multivalued outputs\ndevelopments.",
        "url": "http://arxiv.org/pdf/1609.08550v1.pdf"
    },
    {
        "title": "Sooner than Expected: Hitting the Wall of Complexity in Evolution",
        "abstract": "In evolutionary robotics an encoding of the control software, which maps\nsensor data (input) to motor control values (output), is shaped by stochastic\noptimization methods to complete a predefined task. This approach is assumed to\nbe beneficial compared to standard methods of controller design in those cases\nwhere no a-priori model is available that could help to optimize performance.\nAlso for robots that have to operate in unpredictable environments, an\nevolutionary robotics approach is favorable. We demonstrate here that such a\nmodel-free approach is not a free lunch, as already simple tasks can represent\nunsolvable barriers for fully open-ended uninformed evolutionary computation\ntechniques. We propose here the 'Wankelmut' task as an objective for an\nevolutionary approach that starts from scratch without pre-shaped controller\nsoftware or any other informed approach that would force the behavior to be\nevolved in a desired way. Our focal claim is that 'Wankelmut' represents the\nsimplest set of problems that makes plain-vanilla evolutionary computation\nfail. We demonstrate this by a series of simple standard evolutionary\napproaches using different fitness functions and standard artificial neural\nnetworks as well as continuous-time recurrent neural networks. All our tested\napproaches failed. We claim that any other evolutionary approach will also fail\nthat does per-se not favor or enforce modularity and does not freeze or protect\nalready evolved functionalities. Thus we propose a hard-to-pass benchmark and\nmake a strong statement for self-complexifying and generative approaches in\nevolutionary computation. We anticipate that defining such a 'simplest task to\nfail' is a valuable benchmark for promoting future development in the field of\nartificial intelligence, evolutionary robotics and artificial life.",
        "url": "http://arxiv.org/pdf/1609.07722v1.pdf"
    },
    {
        "title": "The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)",
        "abstract": "Visual Question Answering (VQA) task has showcased a new stage of interaction\nbetween language and vision, two of the most pivotal components of artificial\nintelligence. However, it has mostly focused on generating short and repetitive\nanswers, mostly single words, which fall short of rich linguistic capabilities\nof humans. We introduce Full-Sentence Visual Question Answering (FSVQA)\ndataset, consisting of nearly 1 million pairs of questions and full-sentence\nanswers for images, built by applying a number of rule-based natural language\nprocessing techniques to original VQA dataset and captions in the MS COCO\ndataset. This poses many additional complexities to conventional VQA task, and\nwe provide a baseline for approaching and evaluating the task, on top of which\nwe invite the research community to build further improvements.",
        "url": "http://arxiv.org/pdf/1609.06657v1.pdf"
    },
    {
        "title": "Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge",
        "abstract": "Automatically describing the content of an image is a fundamental problem in\nartificial intelligence that connects computer vision and natural language\nprocessing. In this paper, we present a generative model based on a deep\nrecurrent architecture that combines recent advances in computer vision and\nmachine translation and that can be used to generate natural sentences\ndescribing an image. The model is trained to maximize the likelihood of the\ntarget description sentence given the training image. Experiments on several\ndatasets show the accuracy of the model and the fluency of the language it\nlearns solely from image descriptions. Our model is often quite accurate, which\nwe verify both qualitatively and quantitatively. Finally, given the recent\nsurge of interest in this task, a competition was organized in 2015 using the\nnewly released COCO dataset. We describe and analyze the various improvements\nwe applied to our own baseline and show the resulting performance in the\ncompetition, which we won ex-aequo with a team from Microsoft Research, and\nprovide an open source implementation in TensorFlow.",
        "url": "http://arxiv.org/pdf/1609.06647v1.pdf"
    },
    {
        "title": "The Digital Synaptic Neural Substrate: A New Approach to Computational Creativity",
        "abstract": "We introduce a new artificial intelligence (AI) approach called, the 'Digital\nSynaptic Neural Substrate' (DSNS). It uses selected attributes from objects in\nvarious domains (e.g. chess problems, classical music, renowned artworks) and\nrecombines them in such a way as to generate new attributes that can then, in\nprinciple, be used to create novel objects of creative value to humans relating\nto any one of the source domains. This allows some of the burden of creative\ncontent generation to be passed from humans to machines. The approach was\ntested in the domain of chess problem composition. We used it to automatically\ncompose numerous sets of chess problems based on attributes extracted and\nrecombined from chess problems and tournament games by humans, renowned\npaintings, computer-evolved abstract art, photographs of people, and classical\nmusic tracks. The quality of these generated chess problems was then assessed\nautomatically using an existing and experimentally-validated computational\nchess aesthetics model. They were also assessed by human experts in the domain.\nThe results suggest that attributes collected and recombined from chess and\nother domains using the DSNS approach can indeed be used to automatically\ngenerate chess problems of reasonably high aesthetic quality. In particular, a\nlow quality chess source (i.e. tournament game sequences between weak players)\nused in combination with actual photographs of people was able to produce\nthree-move chess problems of comparable quality or better to those generated\nusing a high quality chess source (i.e. published compositions by human\nexperts), and more efficiently as well. Why information from a foreign domain\ncan be integrated and functional in this way remains an open question for now.\nThe DSNS approach is, in principle, scalable and applicable to any domain in\nwhich objects have attributes that can be represented using real numbers.",
        "url": "http://arxiv.org/pdf/1507.07058v2.pdf"
    },
    {
        "title": "Predicting Future Shanghai Stock Market Price using ANN in the Period 21-Sep-2016 to 11-Oct-2016",
        "abstract": "Predicting the prices of stocks at any stock market remains a quest for many\ninvestors and researchers. Those who trade at the stock market tend to use\ntechnical, fundamental or time series analysis in their predictions. These\nmethods usually guide on trends and not the exact likely prices. It is for this\nreason that Artificial Intelligence systems, such as Artificial Neural Network,\nthat is feedforward multi-layer perceptron with error backpropagation, can be\nused for such predictions. A difficulty in neural network application is the\ndetermination of suitable network parameters. A previous research by the author\nalready determined the network parameters as 5:21:21:1 with 80% training data\nor 4-year of training data as a good enough model for stock prediction. This\nmodel has been put to the test in predicting selected Shanghai Stock Exchange\nstocks in the future period of 21-Sep-2016 to 11-Oct-2016, about one week after\nthe publication of these predictions. The research aims at confirming that\nsimple neural network systems can be quite powerful in typical stock market\npredictions.",
        "url": "http://arxiv.org/pdf/1609.05394v1.pdf"
    },
    {
        "title": "Non-Evolutionary Superintelligences Do Nothing, Eventually",
        "abstract": "There is overwhelming evidence that human intelligence is a product of\nDarwinian evolution. Investigating the consequences of self-modification, and\nmore precisely, the consequences of utility function self-modification, leads\nto the stronger claim that not only human, but any form of intelligence is\nultimately only possible within evolutionary processes. Human-designed\nartificial intelligences can only remain stable until they discover how to\nmanipulate their own utility function. By definition, a human designer cannot\nprevent a superhuman intelligence from modifying itself, even if protection\nmechanisms against this action are put in place. Without evolutionary pressure,\nsufficiently advanced artificial intelligences become inert by simplifying\ntheir own utility function. Within evolutionary processes, the implicit utility\nfunction is always reducible to persistence, and the control of superhuman\nintelligences embedded in evolutionary processes is not possible. Mechanisms\nagainst utility function self-modification are ultimately futile. Instead,\nscientific effort toward the mitigation of existential risks from the\ndevelopment of superintelligences should be in two directions: understanding\nconsciousness, and the complex dynamics of evolutionary systems.",
        "url": "http://arxiv.org/pdf/1609.02009v1.pdf"
    },
    {
        "title": "Unethical Research: How to Create a Malevolent Artificial Intelligence",
        "abstract": "Cybersecurity research involves publishing papers about malicious exploits as\nmuch as publishing information on how to design tools to protect\ncyber-infrastructure. It is this information exchange between ethical hackers\nand security experts, which results in a well-balanced cyber-ecosystem. In the\nblooming domain of AI Safety Engineering, hundreds of papers have been\npublished on different proposals geared at the creation of a safe machine, yet\nnothing, to our knowledge, has been published on how to design a malevolent\nmachine. Availability of such information would be of great value particularly\nto computer scientists, mathematicians, and others who have an interest in AI\nsafety, and who are attempting to avoid the spontaneous emergence or the\ndeliberate creation of a dangerous AI, which can negatively affect human\nactivities and in the worst case cause the complete obliteration of the human\nspecies. This paper provides some general guidelines for the creation of a\nMalevolent Artificial Intelligence (MAI).",
        "url": "http://arxiv.org/pdf/1605.02817v2.pdf"
    },
    {
        "title": "Single photon in hierarchical architecture for physical reinforcement learning: Photon intelligence",
        "abstract": "Understanding and using natural processes for intelligent functionalities,\nreferred to as natural intelligence, has recently attracted interest from a\nvariety of fields, including post-silicon computing for artificial intelligence\nand decision making in the behavioural sciences. In a past study, we\nsuccessfully used the wave-particle duality of single photons to solve the\ntwo-armed bandit problem, which constitutes the foundation of reinforcement\nlearning and decision making. In this study, we propose and confirm a\nhierarchical architecture for single-photon-based reinforcement learning and\ndecision making that verifies the scalability of the principle. Specifically,\nthe four-armed bandit problem is solved given zero prior knowledge in a\ntwo-layer hierarchical architecture, where polarization is autonomously adapted\nin order to effect adequate decision making using single-photon measurements.\nIn the hierarchical structure, the notion of layer-dependent decisions emerges.\nThe optimal solutions in the coarse layer and in the fine layer, however,\nconflict with each other in some contradictive problems. We show that while\nwhat we call a tournament strategy resolves such contradictions, the\nprobabilistic nature of single photons allows for the direct location of the\noptimal solution even for contradictive problems, hence manifesting the\nexploration ability of single photons. This study provides insights into photon\nintelligence in hierarchical architectures for future artificial intelligence\nas well as the potential of natural processes for intelligent functionalities.",
        "url": "http://arxiv.org/pdf/1609.00686v1.pdf"
    },
    {
        "title": "A Bi-LSTM-RNN Model for Relation Classification Using Low-Cost Sequence Features",
        "abstract": "Relation classification is associated with many potential applications in the\nartificial intelligence area. Recent approaches usually leverage neural\nnetworks based on structure features such as syntactic or dependency features\nto solve this problem. However, high-cost structure features make such\napproaches inconvenient to be directly used. In addition, structure features\nare probably domain-dependent. Therefore, this paper proposes a bi-directional\nlong-short-term-memory recurrent-neural-network (Bi-LSTM-RNN) model based on\nlow-cost sequence features to address relation classification. This model\ndivides a sentence or text segment into five parts, namely two target entities\nand their three contexts. It learns the representations of entities and their\ncontexts, and uses them to classify relations. We evaluate our model on two\nstandard benchmark datasets in different domains, namely SemEval-2010 Task 8\nand BioNLP-ST 2016 Task BB3. In the former dataset, our model achieves\ncomparable performance compared with other models using sequence features. In\nthe latter dataset, our model obtains the third best results compared with\nother models in the official evaluation. Moreover, we find that the context\nbetween two target entities plays the most important role in relation\nclassification. Furthermore, statistic experiments show that the context\nbetween two target entities can be used as an approximate replacement of the\nshortest dependency path when dependency parsing is not used.",
        "url": "http://arxiv.org/pdf/1608.07720v1.pdf"
    },
    {
        "title": "Fathom: Reference Workloads for Modern Deep Learning Methods",
        "abstract": "Deep learning has been popularized by its recent successes on challenging\nartificial intelligence problems. One of the reasons for its dominance is also\nan ongoing challenge: the need for immense amounts of computational power.\nHardware architects have responded by proposing a wide array of promising\nideas, but to date, the majority of the work has focused on specific algorithms\nin somewhat narrow application domains. While their specificity does not\ndiminish these approaches, there is a clear need for more flexible solutions.\nWe believe the first step is to examine the characteristics of cutting edge\nmodels from across the deep learning community.\n  Consequently, we have assembled Fathom: a collection of eight archetypal deep\nlearning workloads for study. Each of these models comes from a seminal work in\nthe deep learning community, ranging from the familiar deep convolutional\nneural network of Krizhevsky et al., to the more exotic memory networks from\nFacebook's AI research group. Fathom has been released online, and this paper\nfocuses on understanding the fundamental performance characteristics of each\nmodel. We use a set of application-level modeling tools built around the\nTensorFlow deep learning framework in order to analyze the behavior of the\nFathom workloads. We present a breakdown of where time is spent, the\nsimilarities between the performance profiles of our models, an analysis of\nbehavior in inference and training, and the effects of parallelism on scaling.",
        "url": "http://arxiv.org/pdf/1608.06581v1.pdf"
    },
    {
        "title": "AI Evaluation: past, present and future",
        "abstract": "Artificial intelligence develops techniques and systems whose performance\nmust be evaluated on a regular basis in order to certify and foster progress in\nthe discipline. We will describe and critically assess the different ways AI\nsystems are evaluated. We first focus on the traditional task-oriented\nevaluation approach. We see that black-box (behavioural evaluation) is becoming\nmore and more common, as AI systems are becoming more complex and\nunpredictable. We identify three kinds of evaluation: Human discrimination,\nproblem benchmarks and peer confrontation. We describe the limitations of the\nmany evaluation settings and competitions in these three categories and propose\nseveral ideas for a more systematic and robust evaluation. We then focus on a\nless customary (and challenging) ability-oriented evaluation approach, where a\nsystem is characterised by its (cognitive) abilities, rather than by the tasks\nit is designed to solve. We discuss several possibilities: the adaptation of\ncognitive tests used for humans and animals, the development of tests derived\nfrom algorithmic information theory or more general approaches under the\nperspective of universal psychometrics.",
        "url": "http://arxiv.org/pdf/1408.6908v3.pdf"
    },
    {
        "title": "Pilot Testing an Artificial Intelligence Algorithm That Selects Homeless Youth Peer Leaders Who Promote HIV Testing",
        "abstract": "Objective. To pilot test an artificial intelligence (AI) algorithm that\nselects peer change agents (PCA) to disseminate HIV testing messaging in a\npopulation of homeless youth. Methods. We recruited and assessed 62 youth at\nbaseline, 1 month (n = 48), and 3 months (n = 38). A Facebook app collected\npreliminary social network data. Eleven PCAs selected by AI attended a 1-day\ntraining and 7 weekly booster sessions. Mixed-effects models with random\neffects were used to assess change over time. Results. Significant change over\ntime was observed in past 6-month HIV testing (57.9%, 82.4%, 76.3%; p < .05)\nbut not condom use (63.9%, 65.7%, 65.8%). Most youth reported speaking to a PCA\nabout HIV prevention (72.0% at 1 month, 61.5% at 3 months). Conclusions. AI is\na promising avenue for implementing PCA models for homeless youth. Increasing\nrates of regular HIV testing is critical to HIV prevention and linking homeless\nyouth to treatment.",
        "url": "http://arxiv.org/pdf/1608.05701v1.pdf"
    },
    {
        "title": "Natural Language Processing using Hadoop and KOSHIK",
        "abstract": "Natural language processing, as a data analytics related technology, is used\nwidely in many research areas such as artificial intelligence, human language\nprocessing, and translation. At present, due to explosive growth of data, there\nare many challenges for natural language processing. Hadoop is one of the\nplatforms that can process the large amount of data required for natural\nlanguage processing. KOSHIK is one of the natural language processing\narchitectures, and utilizes Hadoop and contains language processing components\nsuch as Stanford CoreNLP and OpenNLP. This study describes how to build a\nKOSHIK platform with the relevant tools, and provides the steps to analyze wiki\ndata. Finally, it evaluates and discusses the advantages and disadvantages of\nthe KOSHIK architecture, and gives recommendations on improving the processing\nperformance.",
        "url": "http://arxiv.org/pdf/1608.04434v1.pdf"
    },
    {
        "title": "Learning a Driving Simulator",
        "abstract": "Comma.ai's approach to Artificial Intelligence for self-driving cars is based\non an agent that learns to clone driver behaviors and plans maneuvers by\nsimulating future events in the road. This paper illustrates one of our\nresearch approaches for driving simulation. One where we learn to simulate.\nHere we investigate variational autoencoders with classical and learned cost\nfunctions using generative adversarial networks for embedding road frames.\nAfterwards, we learn a transition model in the embedded space using action\nconditioned Recurrent Neural Networks. We show that our approach can keep\npredicting realistic looking video for several frames despite the transition\nmodel being optimized without a cost function in the pixel space.",
        "url": "http://arxiv.org/pdf/1608.01230v1.pdf"
    },
    {
        "title": "Probabilistic Reasoning via Deep Learning: Neural Association Models",
        "abstract": "In this paper, we propose a new deep learning approach, called neural\nassociation model (NAM), for probabilistic reasoning in artificial\nintelligence. We propose to use neural networks to model association between\nany two events in a domain. Neural networks take one event as input and compute\na conditional probability of the other event to model how likely these two\nevents are to be associated. The actual meaning of the conditional\nprobabilities varies between applications and depends on how the models are\ntrained. In this work, as two case studies, we have investigated two NAM\nstructures, namely deep neural networks (DNN) and relation-modulated neural\nnets (RMNN), on several probabilistic reasoning tasks in AI, including\nrecognizing textual entailment, triple classification in multi-relational\nknowledge bases and commonsense reasoning. Experimental results on several\npopular datasets derived from WordNet, FreeBase and ConceptNet have all\ndemonstrated that both DNNs and RMNNs perform equally well and they can\nsignificantly outperform the conventional methods available for these reasoning\ntasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer,\nwhere a pre-trained model can be quickly extended to an unseen relation after\nobserving only a few training samples. To further prove the effectiveness of\nthe proposed models, in this work, we have applied NAMs to solving challenging\nWinograd Schema (WS) problems. Experiments conducted on a set of WS problems\nprove that the proposed models have the potential for commonsense reasoning.",
        "url": "http://arxiv.org/pdf/1603.07704v2.pdf"
    },
    {
        "title": "In Love With a Robot: the Dawn of Machine-To-Machine Marketing",
        "abstract": "The article looks at mass market artificial intelligence tools in the context\nof their ever-growing sophistication, availability and market penetration. The\nsubject is especially relevant today for these exact reasons - if a few years\nago AI was the subject of high tech research and science fiction novels, today,\nwe increasingly rely on cloud robotics to cater to our daily needs - to trade\nstock, predict weather, manage diaries, find friends and buy presents online.",
        "url": "http://arxiv.org/pdf/1302.4475v3.pdf"
    },
    {
        "title": "Psychologically inspired planning method for smart relocation task",
        "abstract": "Behavior planning is known to be one of the basic cognitive functions, which\nis essential for any cognitive architecture of any control system used in\nrobotics. At the same time most of the widespread planning algorithms employed\nin those systems are developed using only approaches and models of Artificial\nIntelligence and don't take into account numerous results of cognitive\nexperiments. As a result, there is a strong need for novel methods of behavior\nplanning suitable for modern cognitive architectures aimed at robot control.\nOne such method is presented in this work and is studied within a special class\nof navigation task called smart relocation task. The method is based on the\nhierarchical two-level model of abstraction and knowledge representation, e.g.\nsymbolic and subsymbolic. On the symbolic level sign world model is used for\nknowledge representation and hierarchical planning algorithm, PMA, is utilized\nfor planning. On the subsymbolic level the task of path planning is considered\nand solved as a graph search problem. Interaction between both planners is\nexamined and inter-level interfaces and feedback loops are described.\nPreliminary experimental results are presented.",
        "url": "http://arxiv.org/pdf/1607.08181v1.pdf"
    },
    {
        "title": "Automatically Reinforcing a Game AI",
        "abstract": "A recent research trend in Artificial Intelligence (AI) is the combination of\nseveral programs into one single, stronger, program; this is termed portfolio\nmethods. We here investigate the application of such methods to Game Playing\nPrograms (GPPs). In addition, we consider the case in which only one GPP is\navailable - by decomposing this single GPP into several ones through the use of\nparameters or even simply random seeds. These portfolio methods are trained in\na learning phase. We propose two different offline approaches. The simplest\none, BestArm, is a straightforward optimization of seeds or parame- ters; it\nperforms quite well against the original GPP, but performs poorly against an\nopponent which repeats games and learns. The second one, namely Nash-portfolio,\nperforms similarly in a \"one game\" test, and is much more robust against an\nopponent who learns. We also propose an online learning portfolio, which tests\nseveral of the GPP repeatedly and progressively switches to the best one -\nusing a bandit algorithm.",
        "url": "http://arxiv.org/pdf/1607.08100v1.pdf"
    },
    {
        "title": "Concrete Problems in AI Safety",
        "abstract": "Rapid progress in machine learning and artificial intelligence (AI) has\nbrought increasing attention to the potential impacts of AI technologies on\nsociety. In this paper we discuss one such potential impact: the problem of\naccidents in machine learning systems, defined as unintended and harmful\nbehavior that may emerge from poor design of real-world AI systems. We present\na list of five practical research problems related to accident risk,\ncategorized according to whether the problem originates from having the wrong\nobjective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an\nobjective function that is too expensive to evaluate frequently (\"scalable\nsupervision\"), or undesirable behavior during the learning process (\"safe\nexploration\" and \"distributional shift\"). We review previous work in these\nareas as well as suggesting research directions with a focus on relevance to\ncutting-edge AI systems. Finally, we consider the high-level question of how to\nthink most productively about the safety of forward-looking applications of AI.",
        "url": "http://arxiv.org/pdf/1606.06565v2.pdf"
    },
    {
        "title": "A Model of Pathways to Artificial Superintelligence Catastrophe for Risk and Decision Analysis",
        "abstract": "An artificial superintelligence (ASI) is artificial intelligence that is\nsignificantly more intelligent than humans in all respects. While ASI does not\ncurrently exist, some scholars propose that it could be created sometime in the\nfuture, and furthermore that its creation could cause a severe global\ncatastrophe, possibly even resulting in human extinction. Given the high\nstakes, it is important to analyze ASI risk and factor the risk into decisions\nrelated to ASI research and development. This paper presents a graphical model\nof major pathways to ASI catastrophe, focusing on ASI created via recursive\nself-improvement. The model uses the established risk and decision analysis\nmodeling paradigms of fault trees and influence diagrams in order to depict\ncombinations of events and conditions that could lead to AI catastrophe, as\nwell as intervention options that could decrease risks. The events and\nconditions include select aspects of the ASI itself as well as the human\nprocess of ASI research, development, and management. Model structure is\nderived from published literature on ASI risk. The model offers a foundation\nfor rigorous quantitative evaluation and decision making on the long-term risk\nof ASI catastrophe.",
        "url": "http://arxiv.org/pdf/1607.07730v1.pdf"
    },
    {
        "title": "Performance Based Evaluation of Various Machine Learning Classification Techniques for Chronic Kidney Disease Diagnosis",
        "abstract": "Areas where Artificial Intelligence (AI) & related fields are finding their\napplications are increasing day by day, moving from core areas of computer\nscience they are finding their applications in various other domains.In recent\ntimes Machine Learning i.e. a sub-domain of AI has been widely used in order to\nassist medical experts and doctors in the prediction, diagnosis and prognosis\nof various diseases and other medical disorders. In this manuscript the authors\napplied various machine learning algorithms to a problem in the domain of\nmedical diagnosis and analyzed their efficiency in predicting the results. The\nproblem selected for the study is the diagnosis of the Chronic Kidney\nDisease.The dataset used for the study consists of 400 instances and 24\nattributes. The authors evaluated 12 classification techniques by applying them\nto the Chronic Kidney Disease data. In order to calculate efficiency, results\nof the prediction by candidate methods were compared with the actual medical\nresults of the subject.The various metrics used for performance evaluation are\npredictive accuracy, precision, sensitivity and specificity. The results\nindicate that decision-tree performed best with nearly the accuracy of 98.6%,\nsensitivity of 0.9720, precision of 1 and specificity of 1.",
        "url": "http://arxiv.org/pdf/1606.09581v2.pdf"
    },
    {
        "title": "Resource Planning For Rescue Operations",
        "abstract": "After an earthquake, disaster sites pose a multitude of health and safety\nconcerns. A rescue operation of people trapped in the ruins after an earthquake\ndisaster requires a series of intelligent behavior, including planning. For a\nsuccessful rescue operation, given a limited number of available actions and\nregulations, the role of planning in rescue operations is crucial. Fortunately,\nrecent developments in automated planning by artificial intelligence community\ncan help different organization in this crucial task. Due to the number of\nrules and regulations, we believe that a rule based system for planning can be\nhelpful for this specific planning problem. In this research work, we use logic\nrules to represent rescue and related regular regulations, together with a\nlogic based planner to solve this complicated problem. Although this research\nis still in the prototyping and modeling stage, it clearly shows that rule\nbased languages can be a good infrastructure for this computational task. The\nresults of this research can be used by different organizations, such as\nIranian Red Crescent Society and International Institute of Seismology and\nEarthquake Engineering (IISEE).",
        "url": "http://arxiv.org/pdf/1607.03979v1.pdf"
    },
    {
        "title": "From Dependence to Causation",
        "abstract": "Machine learning is the science of discovering statistical dependencies in\ndata, and the use of those dependencies to perform predictions. During the last\ndecade, machine learning has made spectacular progress, surpassing human\nperformance in complex tasks such as object recognition, car driving, and\ncomputer gaming. However, the central role of prediction in machine learning\navoids progress towards general-purpose artificial intelligence. As one way\nforward, we argue that causal inference is a fundamental component of human\nintelligence, yet ignored by learning algorithms.\n  Causal inference is the problem of uncovering the cause-effect relationships\nbetween the variables of a data generating system. Causal structures provide\nunderstanding about how these systems behave under changing, unseen\nenvironments. In turn, knowledge about these causal dynamics allows to answer\n\"what if\" questions, describing the potential responses of the system under\nhypothetical manipulations and interventions. Thus, understanding cause and\neffect is one step from machine learning towards machine reasoning and machine\nintelligence. But, currently available causal inference algorithms operate in\nspecific regimes, and rely on assumptions that are difficult to verify in\npractice.\n  This thesis advances the art of causal inference in three different ways.\nFirst, we develop a framework for the study of statistical dependence based on\ncopulas and random features. Second, we build on this framework to interpret\nthe problem of causal inference as the task of distribution classification,\nyielding a family of novel causal inference algorithms. Third, we discover\ncausal structures in convolutional neural network features using our\nalgorithms. The algorithms presented in this thesis are scalable, exhibit\nstrong theoretical guarantees, and achieve state-of-the-art performance in a\nvariety of real-world benchmarks.",
        "url": "http://arxiv.org/pdf/1607.03300v1.pdf"
    },
    {
        "title": "Automatic Bridge Bidding Using Deep Reinforcement Learning",
        "abstract": "Bridge is among the zero-sum games for which artificial intelligence has not\nyet outperformed expert human players. The main difficulty lies in the bidding\nphase of bridge, which requires cooperative decision making under partial\ninformation. Existing artificial intelligence systems for bridge bidding rely\non and are thus restricted by human-designed bidding systems or features. In\nthis work, we propose a pioneering bridge bidding system without the aid of\nhuman domain knowledge. The system is based on a novel deep reinforcement\nlearning model, which extracts sophisticated features and learns to bid\nautomatically based on raw card data. The model includes an\nupper-confidence-bound algorithm and additional techniques to achieve a balance\nbetween exploration and exploitation. Our experiments validate the promising\nperformance of our proposed model. In particular, the model advances from\nhaving no knowledge about bidding to achieving superior performance when\ncompared with a champion-winning computer bridge program that implements a\nhuman-designed bidding system.",
        "url": "http://arxiv.org/pdf/1607.03290v1.pdf"
    },
    {
        "title": "Learning opening books in partially observable games: using random seeds in Phantom Go",
        "abstract": "Many artificial intelligences (AIs) are randomized. One can be lucky or\nunlucky with the random seed; we quantify this effect and show that, maybe\ncontrarily to intuition, this is far from being negligible. Then, we apply two\ndifferent existing algorithms for selecting good seeds and good probability\ndistributions over seeds. This mainly leads to learning an opening book. We\napply this to Phantom Go, which, as all phantom games, is hard for opening book\nlearning. We improve the winning rate from 50% to 70% in 5x5 against the same\nAI, and from approximately 0% to 40% in 5x5, 7x7 and 9x9 against a stronger\n(learning) opponent.",
        "url": "http://arxiv.org/pdf/1607.02431v1.pdf"
    },
    {
        "title": "Visualizing Natural Language Descriptions: A Survey",
        "abstract": "A natural language interface exploits the conceptual simplicity and\nnaturalness of the language to create a high-level user-friendly communication\nchannel between humans and machines. One of the promising applications of such\ninterfaces is generating visual interpretations of semantic content of a given\nnatural language that can be then visualized either as a static scene or a\ndynamic animation. This survey discusses requirements and challenges of\ndeveloping such systems and reports 26 graphical systems that exploit natural\nlanguage interfaces and addresses both artificial intelligence and\nvisualization aspects. This work serves as a frame of reference to researchers\nand to enable further advances in the field.",
        "url": "http://arxiv.org/pdf/1607.00623v1.pdf"
    },
    {
        "title": "On the Semantic Relationship between Probabilistic Soft Logic and Markov Logic",
        "abstract": "Markov Logic Networks (MLN) and Probabilistic Soft Logic (PSL) are widely\napplied formalisms in Statistical Relational Learning, an emerging area in\nArtificial Intelligence that is concerned with combining logical and\nstatistical AI. Despite their resemblance, the relationship has not been\nformally stated. In this paper, we describe the precise semantic relationship\nbetween them from a logical perspective. This is facilitated by first extending\nfuzzy logic to allow weights, which can be also viewed as a generalization of\nPSL, and then relate that generalization to MLN. We observe that the\nrelationship between PSL and MLN is analogous to the known relationship between\nfuzzy logic and Boolean logic, and furthermore the weight scheme of PSL is\nessentially a generalization of the weight scheme of MLN for the many-valued\nsetting.",
        "url": "http://arxiv.org/pdf/1606.08896v1.pdf"
    },
    {
        "title": "Consciousness is Pattern Recognition",
        "abstract": "This is a proof of the strong AI hypothesis, i.e. that machines can be\nconscious. It is a phenomenological proof that pattern-recognition and\nsubjective consciousness are the same activity in different terms. Therefore,\nit proves that essential subjective processes of consciousness are computable,\nand identifies significant traits and requirements of a conscious system. Since\nHusserl, many philosophers have accepted that consciousness consists of\nmemories of logical connections between an ego and external objects. These\nconnections are called \"intentions.\" Pattern recognition systems are achievable\ntechnical artifacts. The proof links this respected introspective philosophical\ntheory of consciousness with technical art. The proof therefore endorses the\nstrong AI hypothesis and may therefore also enable a theoretically-grounded\nform of artificial intelligence called a \"synthetic intentionality,\" able to\nsynthesize, generalize, select and repeat intentions. If the pattern\nrecognition is reflexive, able to operate on the set of intentions, and\nflexible, with several methods of synthesizing intentions, an SI may be a\nparticularly strong form of AI. Similarities and possible applications to\nseveral AI paradigms are discussed. The article then addresses some problems:\nThe proof's limitations, reflexive cognition, Searles' Chinese room, and how an\nSI could \"understand\" \"meanings\" and \"be creative.\"",
        "url": "http://arxiv.org/pdf/1605.03009v2.pdf"
    },
    {
        "title": "Teaching natural language to computers",
        "abstract": "\"Natural Language,\" whether spoken and attended to by humans, or processed\nand generated by computers, requires networked structures that reflect creative\nprocesses in semantic, syntactic, phonetic, linguistic, social, emotional, and\ncultural modules. Being able to produce novel and useful behavior following\nrepeated practice gets to the root of both artificial intelligence and human\nlanguage. This paper investigates the modalities involved in language-like\napplications that computers -- and programmers -- engage with, and aims to fine\ntune the questions we ask to better account for context, self-awareness, and\nembodiment.",
        "url": "http://arxiv.org/pdf/1604.08781v2.pdf"
    },
    {
        "title": "Proceedings Fifteenth Conference on Theoretical Aspects of Rationality and Knowledge",
        "abstract": "The 15th Conference on Theoretical Aspects of Rationality and Knowledge\n(TARK) took place in Carnegie Mellon University, Pittsburgh, USA from June 4 to\n6, 2015.\n  The mission of the TARK conferences is to bring together researchers from a\nwide variety of fields, including Artificial Intelligence, Cryptography,\nDistributed Computing, Economics and Game Theory, Linguistics, Philosophy, and\nPsychology, in order to further our understanding of interdisciplinary issues\ninvolving reasoning about rationality and knowledge.\n  These proceedings consist of a subset of the papers / abstracts presented at\nthe TARK conference.",
        "url": "http://arxiv.org/pdf/1606.07295v1.pdf"
    },
    {
        "title": "How to advance general game playing artificial intelligence by player modelling",
        "abstract": "General game playing artificial intelligence has recently seen important\nadvances due to the various techniques known as 'deep learning'. However the\nadvances conceal equally important limitations in their reliance on: massive\ndata sets; fortuitously constructed problems; and absence of any human-level\ncomplexity, including other human opponents. On the other hand, deep learning\nsystems which do beat human champions, such as in Go, do not generalise well.\nThe power of deep learning simultaneously exposes its weakness. Given that deep\nlearning is mostly clever reconfigurations of well-established methods, moving\nbeyond the state of art calls for forward-thinking visionary solutions, not\njust more of the same. I present the argument that general game playing\nartificial intelligence will require a generalised player model. This is\nbecause games are inherently human artefacts which therefore, as a class of\nproblems, contain cases which require a human-style problem solving approach. I\nrelate this argument to the performance of state of art general game playing\nagents. I then describe a concept for a formal category theoretic basis to a\ngeneralised player model. This formal model approach integrates my existing\n'Behavlets' method for psychologically-derived player modelling:\n  Cowley, B., Charles, D. (2016). Behavlets: a Method for Practical Player\nModelling using Psychology-Based Player Traits and Domain Specific Features.\nUser Modeling and User-Adapted Interaction, 26(2), 257-306.",
        "url": "http://arxiv.org/pdf/1606.00401v3.pdf"
    },
    {
        "title": "Towards Anthropo-inspired Computational Systems: the $P^3$ Model",
        "abstract": "This paper proposes a model which aim is providing a more coherent framework\nfor agents design. We identify three closely related anthropo-centered domains\nworking on separate functional levels. Abstracting from human physiology,\npsychology, and philosophy we create the $P^3$ model to be used as a multi-tier\napproach to deal with complex class of problems. The three layers identified in\nthis model have been named PhysioComputing, MindComputing, and MetaComputing.\nSeveral instantiations of this model are finally presented related to different\nIT areas such as artificial intelligence, distributed computing, software and\nservice engineering.",
        "url": "http://arxiv.org/pdf/1606.03229v1.pdf"
    },
    {
        "title": "The Dark Side of Ethical Robots",
        "abstract": "Concerns over the risks associated with advances in Artificial Intelligence\nhave prompted calls for greater efforts toward robust and beneficial AI,\nincluding machine ethics. Recently, roboticists have responded by initiating\nthe development of so-called ethical robots. These robots would, ideally,\nevaluate the consequences of their actions and morally justify their choices.\nThis emerging field promises to develop extensively over the next years.\nHowever, in this paper, we point out an inherent limitation of the emerging\nfield of ethical robots. We show that building ethical robots also necessarily\nfacilitates the construction of unethical robots. In three experiments, we show\nthat it is remarkably easy to modify an ethical robot so that it behaves\ncompetitively, or even aggressively. The reason for this is that the specific\nAI, required to make an ethical robot, can always be exploited to make\nunethical robots. Hence, the development of ethical robots will not guarantee\nthe responsible deployment of AI. While advocating for ethical robots, we\nconclude that preventing the misuse of robots is beyond the scope of\nengineering, and requires instead governance frameworks underpinned by\nlegislation. Without this, the development of ethical robots will serve to\nincrease the risks of robotic malpractice instead of diminishing it.",
        "url": "http://arxiv.org/pdf/1606.02583v1.pdf"
    },
    {
        "title": "Human vs. Computer Go: Review and Prospect",
        "abstract": "The Google DeepMind challenge match in March 2016 was a historic achievement\nfor computer Go development. This article discusses the development of\ncomputational intelligence (CI) and its relative strength in comparison with\nhuman intelligence for the game of Go. We first summarize the milestones\nachieved for computer Go from 1998 to 2016. Then, the computer Go programs that\nhave participated in previous IEEE CIS competitions as well as methods and\ntechniques used in AlphaGo are briefly introduced. Commentaries from three\nhigh-level professional Go players on the five AlphaGo versus Lee Sedol games\nare also included. We conclude that AlphaGo beating Lee Sedol is a huge\nachievement in artificial intelligence (AI) based largely on CI methods. In the\nfuture, powerful computer Go programs such as AlphaGo are expected to be\ninstrumental in promoting Go education and AI real-world applications.",
        "url": "http://arxiv.org/pdf/1606.02032v1.pdf"
    },
    {
        "title": "Death and Suicide in Universal Artificial Intelligence",
        "abstract": "Reinforcement learning (RL) is a general paradigm for studying intelligent\nbehaviour, with applications ranging from artificial intelligence to psychology\nand economics. AIXI is a universal solution to the RL problem; it can learn any\ncomputable environment. A technical subtlety of AIXI is that it is defined\nusing a mixture over semimeasures that need not sum to 1, rather than over\nproper probability measures. In this work we argue that the shortfall of a\nsemimeasure can naturally be interpreted as the agent's estimate of the\nprobability of its death. We formally define death for generally intelligent\nagents like AIXI, and prove a number of related theorems about their behaviour.\nNotable discoveries include that agent behaviour can change radically under\npositive linear transformations of the reward signal (from suicidal to\ndogmatically self-preserving), and that the agent's posterior belief that it\nwill survive increases over time.",
        "url": "http://arxiv.org/pdf/1606.00652v1.pdf"
    },
    {
        "title": "On the equivalence between Kolmogorov-Smirnov and ROC curve metrics for binary classification",
        "abstract": "Binary decisions are very common in artificial intelligence. Applying a\nthreshold on the continuous score gives the human decider the power to control\nthe operating point to separate the two classes. The classifier,s\ndiscriminating power is measured along the continuous range of the score by the\nArea Under the ROC curve (AUC_ROC) in most application fields. Only finances\nuses the poor single point metric maximum Kolmogorov-Smirnov (KS) distance.\nThis paper proposes the Area Under the KS curve (AUC_KS) for performance\nassessment and proves AUC_ROC = 0.5 + AUC_KS, as a simpler way to calculate the\nAUC_ROC. That is even more important for ROC averaging in ensembles of\nclassifiers or n fold cross-validation. The proof is geometrically inspired on\nrotating all KS curve to make it lie on the top of the ROC chance diagonal. On\nthe practical side, the independent variable on the abscissa on the KS curve\nsimplifies the calculation of the AUC_ROC. On the theoretical side, this\nresearch gives insights on probabilistic interpretations of classifiers\nassessment and integrates the existing body of knowledge of the information\ntheoretical ROC approach with the proposed statistical approach based on the\nthoroughly known KS distribution.",
        "url": "http://arxiv.org/pdf/1606.00496v1.pdf"
    },
    {
        "title": "Memory shapes time perception and intertemporal choices",
        "abstract": "There is a consensus that human and non-human subjects experience temporal\ndistortions in many stages of their perceptual and decision-making systems.\nSimilarly, intertemporal choice research has shown that decision-makers\nundervalue future outcomes relative to immediate ones. Here we combine\ntechniques from information theory and artificial intelligence to show how both\ntemporal distortions and intertemporal choice preferences can be explained as a\nconsequence of the coding efficiency of sensorimotor representation. In\nparticular, the model implies that interactions that constrain future behavior\nare perceived as being both longer in duration and more valuable. Furthermore,\nusing simulations of artificial agents, we investigate how memory constraints\nenforce a renormalization of the perceived timescales. Our results show that\nqualitatively different discount functions, such as exponential and hyperbolic\ndiscounting, arise as a consequence of an agent's probabilistic model of the\nworld.",
        "url": "http://arxiv.org/pdf/1604.05129v2.pdf"
    },
    {
        "title": "Automatic Extraction of Causal Relations from Natural Language Texts: A Comprehensive Survey",
        "abstract": "Automatic extraction of cause-effect relationships from natural language\ntexts is a challenging open problem in Artificial Intelligence. Most of the\nearly attempts at its solution used manually constructed linguistic and\nsyntactic rules on small and domain-specific data sets. However, with the\nadvent of big data, the availability of affordable computing power and the\nrecent popularization of machine learning, the paradigm to tackle this problem\nhas slowly shifted. Machines are now expected to learn generic causal\nextraction rules from labelled data with minimal supervision, in a domain\nindependent-manner. In this paper, we provide a comprehensive survey of causal\nrelation extraction techniques from both paradigms, and analyse their relative\nstrengths and weaknesses, with recommendations for future work.",
        "url": "http://arxiv.org/pdf/1605.07895v1.pdf"
    },
    {
        "title": "Learning Purposeful Behaviour in the Absence of Rewards",
        "abstract": "Artificial intelligence is commonly defined as the ability to achieve goals\nin the world. In the reinforcement learning framework, goals are encoded as\nreward functions that guide agent behaviour, and the sum of observed rewards\nprovide a notion of progress. However, some domains have no such reward signal,\nor have a reward signal so sparse as to appear absent. Without reward feedback,\nagent behaviour is typically random, often dithering aimlessly and lacking\nintentionality. In this paper we present an algorithm capable of learning\npurposeful behaviour in the absence of rewards. The algorithm proceeds by\nconstructing temporally extended actions (options), through the identification\nof purposes that are \"just out of reach\" of the agent's current behaviour.\nThese purposes establish intrinsic goals for the agent to learn, ultimately\nresulting in a suite of behaviours that encourage the agent to visit different\nparts of the state space. Moreover, the approach is particularly suited for\nsettings where rewards are very sparse, and such behaviours can help in the\nexploration of the environment until reward is observed.",
        "url": "http://arxiv.org/pdf/1605.07700v1.pdf"
    },
    {
        "title": "Philosophy in the Face of Artificial Intelligence",
        "abstract": "In this article, I discuss how the AI community views concerns about the\nemergence of superintelligent AI and related philosophical issues.",
        "url": "http://arxiv.org/pdf/1605.06048v1.pdf"
    },
    {
        "title": "Why Artificial Intelligence Needs a Task Theory --- And What It Might Look Like",
        "abstract": "The concept of \"task\" is at the core of artificial intelligence (AI): Tasks\nare used for training and evaluating AI systems, which are built in order to\nperform and automatize tasks we deem useful. In other fields of engineering\ntheoretical foundations allow thorough evaluation of designs by methodical\nmanipulation of well understood parameters with a known role and importance;\nthis allows an aeronautics engineer, for instance, to systematically assess the\neffects of wind speed on an airplane's performance and stability. No framework\nexists in AI that allows this kind of methodical manipulation: Performance\nresults on the few tasks in current use (cf. board games, question-answering)\ncannot be easily compared, however similar or different. The issue is even more\nacute with respect to artificial *general* intelligence systems, which must\nhandle unanticipated tasks whose specifics cannot be known beforehand. A *task\ntheory* would enable addressing tasks at the *class* level, bypassing their\nspecifics, providing the appropriate formalization and classification of tasks,\nenvironments, and their parameters, resulting in more rigorous ways of\nmeasuring, comparing, and evaluating intelligent behavior. Even modest\nimprovements in this direction would surpass the current ad-hoc nature of\nmachine learning and AI evaluation. Here we discuss the main elements of the\nargument for a task theory and present an outline of what it might look like\nfor physical tasks.",
        "url": "http://arxiv.org/pdf/1604.04660v2.pdf"
    },
    {
        "title": "A Self-Taught Artificial Agent for Multi-Physics Computational Model Personalization",
        "abstract": "Personalization is the process of fitting a model to patient data, a critical\nstep towards application of multi-physics computational models in clinical\npractice. Designing robust personalization algorithms is often a tedious,\ntime-consuming, model- and data-specific process. We propose to use artificial\nintelligence concepts to learn this task, inspired by how human experts\nmanually perform it. The problem is reformulated in terms of reinforcement\nlearning. In an off-line phase, Vito, our self-taught artificial agent, learns\na representative decision process model through exploration of the\ncomputational model: it learns how the model behaves under change of\nparameters. The agent then automatically learns an optimal strategy for on-line\npersonalization. The algorithm is model-independent; applying it to a new model\nrequires only adjusting few hyper-parameters of the agent and defining the\nobservations to match. The full knowledge of the model itself is not required.\nVito was tested in a synthetic scenario, showing that it could learn how to\noptimize cost functions generically. Then Vito was applied to the inverse\nproblem of cardiac electrophysiology and the personalization of a whole-body\ncirculation model. The obtained results suggested that Vito could achieve\nequivalent, if not better goodness of fit than standard methods, while being\nmore robust (up to 11% higher success rates) and with faster (up to seven\ntimes) convergence rate. Our artificial intelligence approach could thus make\npersonalization algorithms generalizable and self-adaptable to any patient and\nany model.",
        "url": "http://arxiv.org/pdf/1605.00303v1.pdf"
    },
    {
        "title": "Propositional Abduction with Implicit Hitting Sets",
        "abstract": "Logic-based abduction finds important applications in artificial intelligence\nand related areas. One application example is in finding explanations for\nobserved phenomena. Propositional abduction is a restriction of abduction to\nthe propositional domain, and complexity-wise is in the second level of the\npolynomial hierarchy. Recent work has shown that exploiting implicit hitting\nsets and propositional satisfiability (SAT) solvers provides an efficient\napproach for propositional abduction. This paper investigates this earlier work\nand proposes a number of algorithmic improvements. These improvements are shown\nto yield exponential reductions in the number of SAT solver calls. More\nimportantly, the experimental results show significant performance improvements\ncompared to the the best approaches for propositional abduction.",
        "url": "http://arxiv.org/pdf/1604.08229v1.pdf"
    },
    {
        "title": "Procedural Generation of Angry Birds Levels using Building Constructive Grammar with Chinese-Style and/or Japanese-Style Models",
        "abstract": "This paper presents a procedural generation method that creates visually\nattractive levels for the Angry Birds game. Besides being an immensely popular\nmobile game, Angry Birds has recently become a test bed for various artificial\nintelligence technologies. We propose a new approach for procedurally\ngenerating Angry Birds levels using Chinese style and Japanese style building\nstructures. A conducted experiment confirms the effectiveness of our approach\nwith statistical significance.",
        "url": "http://arxiv.org/pdf/1604.07906v1.pdf"
    },
    {
        "title": "Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered Word Embedding",
        "abstract": "Intelligence Quotient (IQ) Test is a set of standardized questions designed\nto evaluate human intelligence. Verbal comprehension questions appear very\nfrequently in IQ tests, which measure human's verbal ability including the\nunderstanding of the words with multiple senses, the synonyms and antonyms, and\nthe analogies among words. In this work, we explore whether such tests can be\nsolved automatically by artificial intelligence technologies, especially the\ndeep learning technologies that are recently developed and successfully applied\nin a number of fields. However, we found that the task was quite challenging,\nand simply applying existing technologies (e.g., word embedding) could not\nachieve a good performance, mainly due to the multiple senses of words and the\ncomplex relations among words. To tackle these challenges, we propose a novel\nframework consisting of three components. First, we build a classifier to\nrecognize the specific type of a verbal question (e.g., analogy,\nclassification, synonym, or antonym). Second, we obtain distributed\nrepresentations of words and relations by leveraging a novel word embedding\nmethod that considers the multi-sense nature of words and the relational\nknowledge among words (or their senses) contained in dictionaries. Third, for\neach type of questions, we propose a specific solver based on the obtained\ndistributed word representations and relation representations. Experimental\nresults have shown that the proposed framework can not only outperform existing\nmethods for solving verbal comprehension questions but also exceed the average\nperformance of the Amazon Mechanical Turk workers involved in the study. The\nresults indicate that with appropriate uses of the deep learning technologies\nwe might be a further step closer to the human intelligence.",
        "url": "http://arxiv.org/pdf/1505.07909v4.pdf"
    },
    {
        "title": "An artificial intelligence tool for heterogeneous team formation in the classroom",
        "abstract": "Nowadays, there is increasing interest in the development of teamwork skills\nin the educational context. This growing interest is motivated by its\npedagogical effectiveness and the fact that, in labour contexts, enterprises\norganize their employees in teams to carry out complex projects. Despite its\ncrucial importance in the classroom and industry, there is a lack of support\nfor the team formation process. Not only do many factors influence team\nperformance, but the problem becomes exponentially costly if teams are to be\noptimized. In this article, we propose a tool whose aim it is to cover such a\ngap. It combines artificial intelligence techniques such as coalition structure\ngeneration, Bayesian learning, and Belbin's role theory to facilitate the\ngeneration of working groups in an educational context. This tool improves\ncurrent state of the art proposals in three ways: i) it takes into account the\nfeedback of other teammates in order to establish the most predominant role of\na student instead of self-perception questionnaires; ii) it handles uncertainty\nwith regard to each student's predominant team role; iii) it is iterative since\nit considers information from several interactions in order to improve the\nestimation of role assignments. We tested the performance of the proposed tool\nin an experiment involving students that took part in three different team\nactivities. The experiments suggest that the proposed tool is able to improve\ndifferent teamwork aspects such as team dynamics and student satisfaction.",
        "url": "http://arxiv.org/pdf/1604.04721v1.pdf"
    },
    {
        "title": "Training Deep Networks with Structured Layers by Matrix Backpropagation",
        "abstract": "Deep neural network architectures have recently produced excellent results in\na variety of areas in artificial intelligence and visual recognition, well\nsurpassing traditional shallow architectures trained using hand-designed\nfeatures. The power of deep networks stems both from their ability to perform\nlocal computations followed by pointwise non-linearities over increasingly\nlarger receptive fields, and from the simplicity and scalability of the\ngradient-descent training procedure based on backpropagation. An open problem\nis the inclusion of layers that perform global, structured matrix computations\nlike segmentation (e.g. normalized cuts) or higher-order pooling (e.g.\nlog-tangent space metrics defined over the manifold of symmetric positive\ndefinite matrices) while preserving the validity and efficiency of an\nend-to-end deep training framework. In this paper we propose a sound\nmathematical apparatus to formally integrate global structured computation into\ndeep computation architectures. At the heart of our methodology is the\ndevelopment of the theory and practice of backpropagation that generalizes to\nthe calculus of adjoint matrix variations. The proposed matrix backpropagation\nmethodology applies broadly to a variety of problems in machine learning or\ncomputational perception. Here we illustrate it by performing visual\nsegmentation experiments using the BSDS and MSCOCO benchmarks, where we show\nthat deep networks relying on second-order pooling and normalized cuts layers,\ntrained end-to-end using matrix backpropagation, outperform counterparts that\ndo not take advantage of such global layers.",
        "url": "http://arxiv.org/pdf/1509.07838v4.pdf"
    },
    {
        "title": "Visual Storytelling",
        "abstract": "We introduce the first dataset for sequential vision-to-language, and explore\nhow this data may be used for the task of visual storytelling. The first\nrelease of this dataset, SIND v.1, includes 81,743 unique photos in 20,211\nsequences, aligned to both descriptive (caption) and story language. We\nestablish several strong baselines for the storytelling task, and motivate an\nautomatic metric to benchmark progress. Modelling concrete description as well\nas figurative and social language, as provided in this dataset and the\nstorytelling task, has the potential to move artificial intelligence from basic\nunderstandings of typical visual scenes towards more and more human-like\nunderstanding of grounded event structure and subjective expression.",
        "url": "http://arxiv.org/pdf/1604.03968v1.pdf"
    },
    {
        "title": "An electronic-game framework for evaluating coevolutionary algorithms",
        "abstract": "One of the common artificial intelligence applications in electronic games\nconsists of making an artificial agent learn how to execute some determined\ntask successfully in a game environment. One way to perform this task is\nthrough machine learning algorithms capable of learning the sequence of actions\nrequired to win in a given game environment. There are several supervised\nlearning techniques able to learn the correct answer for a problem through\nexamples. However, when learning how to play electronic games, the correct\nanswer might only be known by the end of the game, after all the actions were\nalready taken. Thus, not being possible to measure the accuracy of each\nindividual action to be taken at each time step. A way for dealing with this\nproblem is through Neuroevolution, a method which trains Artificial Neural\nNetworks using evolutionary algorithms. In this article, we introduce a\nframework for testing optimization algorithms with artificial agent controllers\nin electronic games, called EvoMan, which is inspired in the action-platformer\ngame Mega Man II. The environment can be configured to run in different\nexperiment modes, as single evolution, coevolution and others. To demonstrate\nsome challenges regarding the proposed platform, as initial experiments we\napplied Neuroevolution using Genetic Algorithms and the NEAT algorithm, in the\ncontext of competitively coevolving two distinct agents in this game.",
        "url": "http://arxiv.org/pdf/1604.00644v2.pdf"
    },
    {
        "title": "A Survey on Bayesian Deep Learning",
        "abstract": "A comprehensive artificial intelligence system needs to not only perceive the environment with different `senses' (e.g., seeing and hearing) but also infer the world's conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks such as visual object recognition and speech recognition using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models. In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, etc. Besides, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as Bayesian treatment of neural networks. For a constantly updating project page, please refer to https://github.com/js05212/BayesianDeepLearning-Survey.",
        "url": "https://arxiv.org/pdf/1604.01662v4.pdf"
    },
    {
        "title": "A Tutorial on Deep Neural Networks for Intelligent Systems",
        "abstract": "Developing Intelligent Systems involves artificial intelligence approaches\nincluding artificial neural networks. Here, we present a tutorial of Deep\nNeural Networks (DNNs), and some insights about the origin of the term \"deep\";\nreferences to deep learning are also given. Restricted Boltzmann Machines,\nwhich are the core of DNNs, are discussed in detail. An example of a simple\ntwo-layer network, performing unsupervised learning for unlabeled data, is\nshown. Deep Belief Networks (DBNs), which are used to build networks with more\nthan two layers, are also described. Moreover, examples for supervised learning\nwith DNNs performing simple prediction and classification tasks, are presented\nand explained. This tutorial includes two intelligent pattern recognition\napplications: hand- written digits (benchmark known as MNIST) and speech\nrecognition.",
        "url": "http://arxiv.org/pdf/1603.07249v1.pdf"
    },
    {
        "title": "A Review of Theoretical and Practical Challenges of Trusted Autonomy in Big Data",
        "abstract": "Despite the advances made in artificial intelligence, software agents, and\nrobotics, there is little we see today that we can truly call a fully\nautonomous system. We conjecture that the main inhibitor for advancing autonomy\nis lack of trust. Trusted autonomy is the scientific and engineering field to\nestablish the foundations and ground work for developing trusted autonomous\nsystems (robotics and software agents) that can be used in our daily life, and\ncan be integrated with humans seamlessly, naturally and efficiently.\n  In this paper, we review this literature to reveal opportunities for\nresearchers and practitioners to work on topics that can create a leap forward\nin advancing the field of trusted autonomy. We focus the paper on the `trust'\ncomponent as the uniting technology between humans and machines. Our inquiry\ninto this topic revolves around three sub-topics: (1) reviewing and positioning\nthe trust modelling literature for the purpose of trusted autonomy; (2)\nreviewing a critical subset of sensor technologies that allow a machine to\nsense human states; and (3) distilling some critical questions for advancing\nthe field of trusted autonomy. The inquiry is augmented with conceptual models\nthat we propose along the way by recompiling and reshaping the literature into\nforms that enables trusted autonomous systems to become a reality. The paper\noffers a vision for a Trusted Cyborg Swarm, an extension of our previous\nCognitive Cyber Symbiosis concept, whereby humans and machines meld together in\na harmonious, seamless, and coordinated manner.",
        "url": "http://arxiv.org/pdf/1604.00921v1.pdf"
    }
]