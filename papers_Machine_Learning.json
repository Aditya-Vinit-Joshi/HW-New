[
    {
        "title": "Automated Bridge Component Recognition using Video Data",
        "abstract": "This paper investigates the automated recognition of structural bridge\ncomponents using video data. Although understanding video data for structural\ninspections is straightforward for human inspectors, the implementation of the\nsame task using machine learning methods has not been fully realized. In\nparticular, single-frame image processing techniques, such as convolutional\nneural networks (CNNs), are not expected to identify structural components\naccurately when the image is a close-up view, lacking contextual information\nregarding where on the structure the image originates. Inspired by the\nsignificant progress in video processing techniques, this study investigates\nautomated bridge component recognition using video data, where the information\nfrom the past frames is used to augment the understanding of the current frame.\nA new simulated video dataset is created to train the machine learning\nalgorithms. Then, convolutional Neural Networks (CNNs) with recurrent\narchitectures are designed and applied to implement the automated bridge\ncomponent recognition task. Results are presented for simulated video data, as\nwell as video collected in the field.",
        "url": "http://arxiv.org/pdf/1806.06820v2.pdf"
    },
    {
        "title": "Robust inference on the average treatment effect using the outcome highly adaptive lasso",
        "abstract": "Many estimators of the average effect of a treatment on an outcome require estimation of the propensity score, the outcome regression, or both. It is often beneficial to utilize flexible techniques such as semiparametric regression or machine learning to estimate these quantities. However, optimal estimation of these regressions does not necessarily lead to optimal estimation of the average treatment effect, particularly in settings with strong instrumental variables. A recent proposal addressed these issues via the outcome-adaptive lasso, a penalized regression technique for estimating the propensity score that seeks to minimize the impact of instrumental variables on treatment effect estimators. However, a notable limitation of this approach is that its application is restricted to parametric models. We propose a more flexible alternative that we call the outcome highly adaptive lasso. We discuss large sample theory for this estimator and propose closed form confidence intervals based on the proposed estimator. We show via simulation that our method offers benefits over several popular approaches.",
        "url": "https://arxiv.org/pdf/1806.06784v3.pdf"
    },
    {
        "title": "Consistent Individualized Feature Attribution for Tree Ensembles",
        "abstract": "A unified approach to explain the output of any machine learning model.",
        "url": "http://arxiv.org/pdf/1802.03888v3.pdf"
    },
    {
        "title": "On Enhancing Speech Emotion Recognition using Generative Adversarial Networks",
        "abstract": "Generative Adversarial Networks (GANs) have gained a lot of attention from\nmachine learning community due to their ability to learn and mimic an input\ndata distribution. GANs consist of a discriminator and a generator working in\ntandem playing a min-max game to learn a target underlying data distribution;\nwhen fed with data-points sampled from a simpler distribution (like uniform or\nGaussian distribution). Once trained, they allow synthetic generation of\nexamples sampled from the target distribution. We investigate the application\nof GANs to generate synthetic feature vectors used for speech emotion\nrecognition. Specifically, we investigate two set ups: (i) a vanilla GAN that\nlearns the distribution of a lower dimensional representation of the actual\nhigher dimensional feature vector and, (ii) a conditional GAN that learns the\ndistribution of the higher dimensional feature vectors conditioned on the\nlabels or the emotional class to which it belongs. As a potential practical\napplication of these synthetically generated samples, we measure any\nimprovement in a classifier's performance when the synthetic data is used along\nwith real data for training. We perform cross-validation analyses followed by a\ncross-corpus study.",
        "url": "http://arxiv.org/pdf/1806.06626v1.pdf"
    },
    {
        "title": "Evaluating and Characterizing Incremental Learning from Non-Stationary Data",
        "abstract": "Incremental learning from non-stationary data poses special challenges to the\nfield of machine learning. Although new algorithms have been developed for\nthis, assessment of results and comparison of behaviors are still open\nproblems, mainly because evaluation metrics, adapted from more traditional\ntasks, can be ineffective in this context. Overall, there is a lack of common\ntesting practices. This paper thus presents a testbed for incremental\nnon-stationary learning algorithms, based on specially designed synthetic\ndatasets. Also, test results are reported for some well-known algorithms to\nshow that the proposed methodology is effective at characterizing their\nstrengths and weaknesses. It is expected that this methodology will provide a\ncommon basis for evaluating future contributions in the field.",
        "url": "http://arxiv.org/pdf/1806.06610v1.pdf"
    },
    {
        "title": "Snap ML: A Hierarchical Framework for Machine Learning",
        "abstract": "We describe a new software framework for fast training of generalized linear\nmodels. The framework, named Snap Machine Learning (Snap ML), combines recent\nadvances in machine learning systems and algorithms in a nested manner to\nreflect the hierarchical architecture of modern computing systems. We prove\ntheoretically that such a hierarchical system can accelerate training in\ndistributed environments where intra-node communication is cheaper than\ninter-node communication. Additionally, we provide a review of the\nimplementation of Snap ML in terms of GPU acceleration, pipelining,\ncommunication patterns and software architecture, highlighting aspects that\nwere critical for achieving high performance. We evaluate the performance of\nSnap ML in both single-node and multi-node environments, quantifying the\nbenefit of the hierarchical scheme and the data streaming functionality, and\ncomparing with other widely-used machine learning software frameworks. Finally,\nwe present a logistic regression benchmark on the Criteo Terabyte Click Logs\ndataset and show that Snap ML achieves the same test loss an order of magnitude\nfaster than any of the previously reported results, including those obtained\nusing TensorFlow and scikit-learn.",
        "url": "http://arxiv.org/pdf/1803.06333v3.pdf"
    },
    {
        "title": "Computational Theories of Curiosity-Driven Learning",
        "abstract": "What are the functions of curiosity? What are the mechanisms of\ncuriosity-driven learning? We approach these questions about the living using\nconcepts and tools from machine learning and developmental robotics. We argue\nthat curiosity-driven learning enables organisms to make discoveries to solve\ncomplex problems with rare or deceptive rewards. By fostering exploration and\ndiscovery of a diversity of behavioural skills, and ignoring these rewards,\ncuriosity can be efficient to bootstrap learning when there is no information,\nor deceptive information, about local improvement towards these problems. We\nalso explain the key role of curiosity for efficient learning of world models.\nWe review both normative and heuristic computational frameworks used to\nunderstand the mechanisms of curiosity in humans, conceptualizing the child as\na sense-making organism. These frameworks enable us to discuss the\nbi-directional causal links between curiosity and learning, and to provide new\nhypotheses about the fundamental role of curiosity in self-organizing\ndevelopmental structures through curriculum learning. We present various\ndevelopmental robotics experiments that study these mechanisms in action, both\nsupporting these hypotheses to understand better curiosity in humans and\nopening new research avenues in machine learning and artificial intelligence.\nFinally, we discuss challenges for the design of experimental paradigms for\nstudying curiosity in psychology and cognitive neuroscience.\n  Keywords: Curiosity, intrinsic motivation, lifelong learning, predictions,\nworld model, rewards, free-energy principle, learning progress, machine\nlearning, AI, developmental robotics, development, curriculum learning,\nself-organization.",
        "url": "http://arxiv.org/pdf/1802.10546v2.pdf"
    },
    {
        "title": "Distributed learning with compressed gradients",
        "abstract": "Asynchronous computation and gradient compression have emerged as two key\ntechniques for achieving scalability in distributed optimization for\nlarge-scale machine learning. This paper presents a unified analysis framework\nfor distributed gradient methods operating with staled and compressed\ngradients. Non-asymptotic bounds on convergence rates and information exchange\nare derived for several optimization algorithms. These bounds give explicit\nexpressions for step-sizes and characterize how the amount of asynchrony and\nthe compression accuracy affect iteration and communication complexity\nguarantees. Numerical results highlight convergence properties of different\ngradient compression algorithms and confirm that fast convergence under limited\ninformation exchange is indeed possible.",
        "url": "http://arxiv.org/pdf/1806.06573v2.pdf"
    },
    {
        "title": "Women also Snowboard: Overcoming Bias in Captioning Models",
        "abstract": "Most machine learning methods are known to capture and exploit biases of the\ntraining data. While some biases are beneficial for learning, others are\nharmful. Specifically, image captioning models tend to exaggerate biases\npresent in training data (e.g., if a word is present in 60% of training\nsentences, it might be predicted in 70% of sentences at test time). This can\nlead to incorrect captions in domains where unbiased captions are desired, or\nrequired, due to over-reliance on the learned prior and image context. In this\nwork we investigate generation of gender-specific caption words (e.g. man,\nwoman) based on the person's appearance or the image context. We introduce a\nnew Equalizer model that ensures equal gender probability when gender evidence\nis occluded in a scene and confident predictions when gender evidence is\npresent. The resulting model is forced to look at a person rather than use\ncontextual cues to make a gender-specific predictions. The losses that comprise\nour model, the Appearance Confusion Loss and the Confident Loss, are general,\nand can be added to any description model in order to mitigate impacts of\nunwanted bias in a description dataset. Our proposed model has lower error than\nprior work when describing images with people and mentioning their gender and\nmore closely matches the ground truth ratio of sentences including women to\nsentences including men. We also show that unlike other approaches, our model\nis indeed more often looking at people when predicting their gender.",
        "url": "http://arxiv.org/pdf/1803.09797v4.pdf"
    },
    {
        "title": "A Novel Hybrid Machine Learning Model for Auto-Classification of Retinal Diseases",
        "abstract": "Automatic clinical diagnosis of retinal diseases has emerged as a promising\napproach to facilitate discovery in areas with limited access to specialists.\nWe propose a novel visual-assisted diagnosis hybrid model based on the support\nvector machine (SVM) and deep neural networks (DNNs). The model incorporates\ncomplementary strengths of DNNs and SVM. Furthermore, we present a new clinical\nretina label collection for ophthalmology incorporating 32 retina diseases\nclasses. Using EyeNet, our model achieves 89.73% diagnosis accuracy and the\nmodel performance is comparable to the professional ophthalmologists.",
        "url": "http://arxiv.org/pdf/1806.06423v1.pdf"
    },
    {
        "title": "Feature Learning and Classification in Neuroimaging: Predicting Cognitive Impairment from Magnetic Resonance Imaging",
        "abstract": "Due to the rapid innovation of technology and the desire to find and employ\nbiomarkers for neurodegenerative disease, high-dimensional data classification\nproblems are routinely encountered in neuroimaging studies. To avoid\nover-fitting and to explore relationships between disease and potential\nbiomarkers, feature learning and selection plays an important role in\nclassifier construction and is an important area in machine learning. In this\narticle, we review several important feature learning and selection techniques\nincluding lasso-based methods, PCA, the two-sample t-test, and stacked\nauto-encoders. We compare these approaches using a numerical study involving\nthe prediction of Alzheimer's disease from Magnetic Resonance Imaging.",
        "url": "http://arxiv.org/pdf/1806.06415v1.pdf"
    },
    {
        "title": "Measuring Semantic Coherence of a Conversation",
        "abstract": "Conversational systems have become increasingly popular as a way for humans\nto interact with computers. To be able to provide intelligent responses,\nconversational systems must correctly model the structure and semantics of a\nconversation. We introduce the task of measuring semantic (in)coherence in a\nconversation with respect to background knowledge, which relies on the\nidentification of semantic relations between concepts introduced during a\nconversation. We propose and evaluate graph-based and machine learning-based\napproaches for measuring semantic coherence using knowledge graphs, their\nvector space embeddings and word embedding models, as sources of background\nknowledge. We demonstrate how these approaches are able to uncover different\ncoherence patterns in conversations on the Ubuntu Dialogue Corpus.",
        "url": "http://arxiv.org/pdf/1806.06411v1.pdf"
    },
    {
        "title": "Scraping and Preprocessing Commercial Auction Data for Fraud Classification",
        "abstract": "In the last three decades, we have seen a significant increase in trading\ngoods and services through online auctions. However, this business created an\nattractive environment for malicious moneymakers who can commit different types\nof fraud activities, such as Shill Bidding (SB). The latter is predominant\nacross many auctions but this type of fraud is difficult to detect due to its\nsimilarity to normal bidding behaviour. The unavailability of SB datasets makes\nthe development of SB detection and classification models burdensome.\nFurthermore, to implement efficient SB detection models, we should produce SB\ndata from actual auctions of commercial sites. In this study, we first scraped\na large number of eBay auctions of a popular product. After preprocessing the\nraw auction data, we build a high-quality SB dataset based on the most reliable\nSB strategies. The aim of our research is to share the preprocessed auction\ndataset as well as the SB training (unlabelled) dataset, thereby researchers\ncan apply various machine learning techniques by using authentic data of\nauctions and fraud.",
        "url": "http://arxiv.org/pdf/1806.00656v2.pdf"
    },
    {
        "title": "Laplacian Smoothing Gradient Descent",
        "abstract": "We propose a class of very simple modifications of gradient descent and\nstochastic gradient descent. We show that when applied to a large variety of\nmachine learning problems, ranging from logistic regression to deep neural\nnets, the proposed surrogates can dramatically reduce the variance, allow to\ntake a larger step size, and improve the generalization accuracy. The methods\nonly involve multiplying the usual (stochastic) gradient by the inverse of a\npositive definitive matrix (which can be computed efficiently by FFT) with a\nlow condition number coming from a one-dimensional discrete Laplacian or its\nhigh order generalizations. It also preserves the mean and increases the\nsmallest component and decreases the largest component. The theory of\nHamilton-Jacobi partial differential equations demonstrates that the implicit\nversion of the new algorithm is almost the same as doing gradient descent on a\nnew function which (i) has the same global minima as the original function and\n(ii) is ``more convex\". Moreover, we show that optimization algorithms with\nthese surrogates converge uniformly in the discrete Sobolev $H_\\sigma^p$ sense\nand reduce the optimality gap for convex optimization problems. The code is\navailable at:\n\\url{https://github.com/BaoWangMath/LaplacianSmoothing-GradientDescent}",
        "url": "http://arxiv.org/pdf/1806.06317v5.pdf"
    },
    {
        "title": "Stable Prediction across Unknown Environments",
        "abstract": "In many important machine learning applications, the training distribution\nused to learn a probabilistic classifier differs from the testing distribution\non which the classifier will be used to make predictions. Traditional methods\ncorrect the distribution shift by reweighting the training data with the ratio\nof the density between test and training data. In many applications training\ntakes place without prior knowledge of the testing distribution on which the\nalgorithm will be applied in the future. Recently, methods have been proposed\nto address the shift by learning causal structure, but those methods rely on\nthe diversity of multiple training data to a good performance, and have\ncomplexity limitations in high dimensions. In this paper, we propose a novel\nDeep Global Balancing Regression (DGBR) algorithm to jointly optimize a deep\nauto-encoder model for feature selection and a global balancing model for\nstable prediction across unknown environments. The global balancing model\nconstructs balancing weights that facilitate estimating of partial effects of\nfeatures (holding fixed all other features), a problem that is challenging in\nhigh dimensions, and thus helps to identify stable, causal relationships\nbetween features and outcomes. The deep auto-encoder model is designed to\nreduce the dimensionality of the feature space, thus making global balancing\neasier. We show, both theoretically and with empirical experiments, that our\nalgorithm can make stable predictions across unknown environments. Our\nexperiments on both synthetic and real world datasets demonstrate that our DGBR\nalgorithm outperforms the state-of-the-art methods for stable prediction across\nunknown environments.",
        "url": "http://arxiv.org/pdf/1806.06270v2.pdf"
    },
    {
        "title": "Binary Classification in Unstructured Space With Hypergraph Case-Based Reasoning",
        "abstract": "Binary classification is one of the most common problem in machine learning.\nIt consists in predicting whether a given element belongs to a particular\nclass. In this paper, a new algorithm for binary classification is proposed\nusing a hypergraph representation. The method is agnostic to data\nrepresentation, can work with multiple data sources or in non-metric spaces,\nand accommodates with missing values. As a result, it drastically reduces the\nneed for data preprocessing or feature engineering. Each element to be\nclassified is partitioned according to its interactions with the training set.\nFor each class, a seminorm over the training set partition is learnt to\nrepresent the distribution of evidence supporting this class.\n  Empirical validation demonstrates its high potential on a wide range of\nwell-known datasets and the results are compared to the state-of-the-art. The\ntime complexity is given and empirically validated. Its robustness with regard\nto hyperparameter sensitivity is studied and compared to standard\nclassification methods. Finally, the limitation of the model space is\ndiscussed, and some potential solutions proposed.",
        "url": "http://arxiv.org/pdf/1806.06232v3.pdf"
    },
    {
        "title": "EARL: Joint Entity and Relation Linking for Question Answering over Knowledge Graphs",
        "abstract": "Many question answering systems over knowledge graphs rely on entity and\nrelation linking components in order to connect the natural language input to\nthe underlying knowledge graph. Traditionally, entity linking and relation\nlinking have been performed either as dependent sequential tasks or as\nindependent parallel tasks. In this paper, we propose a framework called EARL,\nwhich performs entity linking and relation linking as a joint task. EARL\nimplements two different solution strategies for which we provide a comparative\nanalysis in this paper: The first strategy is a formalisation of the joint\nentity and relation linking tasks as an instance of the Generalised Travelling\nSalesman Problem (GTSP). In order to be computationally feasible, we employ\napproximate GTSP solvers. The second strategy uses machine learning in order to\nexploit the connection density between nodes in the knowledge graph. It relies\non three base features and re-ranking steps in order to predict entities and\nrelations. We compare the strategies and evaluate them on a dataset with 5000\nquestions. Both strategies significantly outperform the current\nstate-of-the-art approaches for entity and relation linking.",
        "url": "http://arxiv.org/pdf/1801.03825v4.pdf"
    },
    {
        "title": "Orthogonal Machine Learning: Power and Limitations",
        "abstract": "Double machine learning provides $\\sqrt{n}$-consistent estimates of\nparameters of interest even when high-dimensional or nonparametric nuisance\nparameters are estimated at an $n^{-1/4}$ rate. The key is to employ\nNeyman-orthogonal moment equations which are first-order insensitive to\nperturbations in the nuisance parameters. We show that the $n^{-1/4}$\nrequirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order\nnotion of orthogonality that grants robustness to more complex or\nhigher-dimensional nuisance parameters. In the partially linear regression\nsetting popular in causal inference, we show that we can construct second-order\northogonal moments if and only if the treatment residual is not normally\ndistributed. Our proof relies on Stein's lemma and may be of independent\ninterest. We conclude by demonstrating the robustness benefits of an explicit\ndoubly-orthogonal estimation procedure for treatment effect.",
        "url": "http://arxiv.org/pdf/1711.00342v6.pdf"
    },
    {
        "title": "Random Forest for Label Ranking",
        "abstract": "Label ranking aims to learn a mapping from instances to rankings over a\nfinite number of predefined labels. Random forest is a powerful and one of the\nmost successful general-purpose machine learning algorithms of modern times. In\nthis paper, we present a powerful random forest label ranking method which uses\nrandom decision trees to retrieve nearest neighbors. We have developed a novel\ntwo-step rank aggregation strategy to effectively aggregate neighboring\nrankings discovered by the random forest into a final predicted ranking.\nCompared with existing methods, the new random forest method has many\nadvantages including its intrinsically scalable tree data structure, highly\nparallel-able computational architecture and much superior performance. We\npresent extensive experimental results to demonstrate that our new method\nachieves the highly competitive performance compared with state-of-the-art\nmethods for datasets with complete ranking and datasets with only partial\nranking information.",
        "url": "http://arxiv.org/pdf/1608.07710v3.pdf"
    },
    {
        "title": "Kernel machines that adapt to GPUs for effective large batch training",
        "abstract": "Modern machine learning models are typically trained using Stochastic\nGradient Descent (SGD) on massively parallel computing resources such as GPUs.\nIncreasing mini-batch size is a simple and direct way to utilize the parallel\ncomputing capacity. For small batch an increase in batch size results in the\nproportional reduction in the training time, a phenomenon known as linear\nscaling. However, increasing batch size beyond a certain value leads to no\nfurther improvement in training time. In this paper we develop the first\nanalytical framework that extends linear scaling to match the parallel\ncomputing capacity of a resource. The framework is designed for a class of\nclassical kernel machines. It automatically modifies a standard kernel machine\nto output a mathematically equivalent prediction function, yet allowing for\nextended linear scaling, i.e., higher effective parallelization and faster\ntraining time on given hardware.\n  The resulting algorithms are accurate, principled and very fast. For example,\nusing a single Titan Xp GPU, training on ImageNet with $1.3\\times 10^6$ data\npoints and $1000$ labels takes under an hour, while smaller datasets, such as\nMNIST, take seconds. As the parameters are chosen analytically, based on the\ntheoretical bounds, little tuning beyond selecting the kernel and the kernel\nparameter is needed, further facilitating the practical use of these methods.",
        "url": "http://arxiv.org/pdf/1806.06144v3.pdf"
    },
    {
        "title": "Fairness Under Composition",
        "abstract": "Algorithmic fairness, and in particular the fairness of scoring and\nclassification algorithms, has become a topic of increasing social concern and\nhas recently witnessed an explosion of research in theoretical computer\nscience, machine learning, statistics, the social sciences, and law. Much of\nthe literature considers the case of a single classifier (or scoring function)\nused once, in isolation. In this work, we initiate the study of the fairness\nproperties of systems composed of algorithms that are fair in isolation; that\nis, we study fairness under composition. We identify pitfalls of naive\ncomposition and give general constructions for fair composition, demonstrating\nboth that classifiers that are fair in isolation do not necessarily compose\ninto fair systems and also that seemingly unfair components may be carefully\ncombined to construct fair systems. We focus primarily on the individual\nfairness setting proposed in [Dwork, Hardt, Pitassi, Reingold, Zemel, 2011],\nbut also extend our results to a large class of group fairness definitions\npopular in the recent literature, exhibiting several cases in which group\nfairness definitions give misleading signals under composition.",
        "url": "http://arxiv.org/pdf/1806.06122v2.pdf"
    },
    {
        "title": "The Limits of Post-Selection Generalization",
        "abstract": "While statistics and machine learning offers numerous methods for ensuring\ngeneralization, these methods often fail in the presence of adaptivity---the\ncommon practice in which the choice of analysis depends on previous\ninteractions with the same dataset. A recent line of work has introduced\npowerful, general purpose algorithms that ensure post hoc generalization (also\ncalled robust or post-selection generalization), which says that, given the\noutput of the algorithm, it is hard to find any statistic for which the data\ndiffers significantly from the population it came from.\n  In this work we show several limitations on the power of algorithms\nsatisfying post hoc generalization. First, we show a tight lower bound on the\nerror of any algorithm that satisfies post hoc generalization and answers\nadaptively chosen statistical queries, showing a strong barrier to progress in\npost selection data analysis. Second, we show that post hoc generalization is\nnot closed under composition, despite many examples of such algorithms\nexhibiting strong composition properties.",
        "url": "http://arxiv.org/pdf/1806.06100v1.pdf"
    },
    {
        "title": "Homonym Detection in Curated Bibliographies: Learning from dblp's Experience (full version)",
        "abstract": "Identifying (and fixing) homonymous and synonymous author profiles is one of\nthe major tasks of curating personalized bibliographic metadata repositories\nlike the dblp computer science bibliography. In this paper, we present and\nevaluate a machine learning approach to identify homonymous author\nbibliographies using a simple multilayer perceptron setup. We train our model\non a novel gold-standard data set derived from the past years of active, manual\ncuration at the dblp computer science bibliography.",
        "url": "http://arxiv.org/pdf/1806.06017v1.pdf"
    },
    {
        "title": "On Machine Learning and Structure for Mobile Robots",
        "abstract": "Due to recent advances - compute, data, models - the role of learning in\nautonomous systems has expanded significantly, rendering new applications\npossible for the first time. While some of the most significant benefits are\nobtained in the perception modules of the software stack, other aspects\ncontinue to rely on known manual procedures based on prior knowledge on\ngeometry, dynamics, kinematics etc. Nonetheless, learning gains relevance in\nthese modules when data collection and curation become easier than manual rule\ndesign. Building on this coarse and broad survey of current research, the final\nsections aim to provide insights into future potentials and challenges as well\nas the necessity of structure in current practical applications.",
        "url": "http://arxiv.org/pdf/1806.06003v1.pdf"
    },
    {
        "title": "From probabilistic graphical models to generalized tensor networks for supervised learning",
        "abstract": "Tensor networks have found a wide use in a variety of applications in physics and computer science, recently leading to both theoretical insights as well as practical algorithms in machine learning. In this work we explore the connection between tensor networks and probabilistic graphical models, and show that it motivates the definition of generalized tensor networks where information from a tensor can be copied and reused in other parts of the network. We discuss the relationship between generalized tensor network architectures used in quantum physics, such as string-bond states, and architectures commonly used in machine learning. We provide an algorithm to train these networks in a supervised-learning context and show that they overcome the limitations of regular tensor networks in higher dimensions, while keeping the computation efficient. A method to combine neural networks and tensor networks as part of a common deep learning architecture is also introduced. We benchmark our algorithm for several generalized tensor network architectures on the task of classifying images and sounds, and show that they outperform previously introduced tensor-network algorithms. The models we consider also have a natural implementation on a quantum computer and may guide the development of near-term quantum machine learning architectures.",
        "url": "https://arxiv.org/pdf/1806.05964v2.pdf"
    },
    {
        "title": "MAGIX: Model Agnostic Globally Interpretable Explanations",
        "abstract": "Explaining the behavior of a black box machine learning model at the instance\nlevel is useful for building trust. However, it is also important to understand\nhow the model behaves globally. Such an understanding provides insight into\nboth the data on which the model was trained and the patterns that it learned.\nWe present here an approach that learns if-then rules to globally explain the\nbehavior of black box machine learning models that have been used to solve\nclassification problems. The approach works by first extracting conditions that\nwere important at the instance level and then evolving rules through a genetic\nalgorithm with an appropriate fitness function. Collectively, these rules\nrepresent the patterns followed by the model for decisioning and are useful for\nunderstanding its behavior. We demonstrate the validity and usefulness of the\napproach by interpreting black box models created using publicly available data\nsets as well as a private digital marketing data set.",
        "url": "http://arxiv.org/pdf/1706.07160v3.pdf"
    },
    {
        "title": "Automated Image Data Preprocessing with Deep Reinforcement Learning",
        "abstract": "Data preparation, i.e. the process of transforming raw data into a format that can be used for training effective machine learning models, is a tedious and time-consuming task. For image data, preprocessing typically involves a sequence of basic transformations such as cropping, filtering, rotating or flipping images. Currently, data scientists decide manually based on their experience which transformations to apply in which particular order to a given image data set. Besides constituting a bottleneck in real-world data science projects, manual image data preprocessing may yield suboptimal results as data scientists need to rely on intuition or trial-and-error approaches when exploring the space of possible image transformations and thus might not be able to discover the most effective ones. To mitigate the inefficiency and potential ineffectiveness of manual data preprocessing, this paper proposes a deep reinforcement learning framework to automatically discover the optimal data preprocessing steps for training an image classifier. The framework takes as input sets of labeled images and predefined preprocessing transformations. It jointly learns the classifier and the optimal preprocessing transformations for individual images. Experimental results show that the proposed approach not only improves the accuracy of image classifiers, but also makes them substantially more robust to noisy inputs at test time.",
        "url": "https://arxiv.org/pdf/1806.05886v2.pdf"
    },
    {
        "title": "Financial Risk and Returns Prediction with Modular Networked Learning",
        "abstract": "An artificial agent for financial risk and returns' prediction is built with\na modular cognitive system comprised of interconnected recurrent neural\nnetworks, such that the agent learns to predict the financial returns, and\nlearns to predict the squared deviation around these predicted returns. These\ntwo expectations are used to build a volatility-sensitive interval prediction\nfor financial returns, which is evaluated on three major financial indices and\nshown to be able to predict financial returns with higher than 80% success rate\nin interval prediction in both training and testing, raising into question the\nEfficient Market Hypothesis. The agent is introduced as an example of a class\nof artificial intelligent systems that are equipped with a Modular Networked\nLearning cognitive system, defined as an integrated networked system of machine\nlearning modules, where each module constitutes a functional unit that is\ntrained for a given specific task that solves a subproblem of a complex main\nproblem expressed as a network of linked subproblems. In the case of neural\nnetworks, these systems function as a form of an \"artificial brain\", where each\nmodule is like a specialized brain region comprised of a neural network with a\nspecific architecture.",
        "url": "http://arxiv.org/pdf/1806.05876v1.pdf"
    },
    {
        "title": "Deep Learning with Convolutional Neural Network for Objective Skill Evaluation in Robot-assisted Surgery",
        "abstract": "With the advent of robot-assisted surgery, the role of data-driven approaches\nto integrate statistics and machine learning is growing rapidly with prominent\ninterests in objective surgical skill assessment. However, most existing work\nrequires translating robot motion kinematics into intermediate features or\ngesture segments that are expensive to extract, lack efficiency, and require\nsignificant domain-specific knowledge. We propose an analytical deep learning\nframework for skill assessment in surgical training. A deep convolutional\nneural network is implemented to map multivariate time series data of the\nmotion kinematics to individual skill levels. We perform experiments on the\npublic minimally invasive surgical robotic dataset, JHU-ISI Gesture and Skill\nAssessment Working Set (JIGSAWS). Our proposed learning model achieved a\ncompetitive accuracy of 92.5%, 95.4%, and 91.3%, in the standard training\ntasks: Suturing, Needle-passing, and Knot-tying, respectively. Without the need\nof engineered features or carefully-tuned gesture segmentation, our model can\nsuccessfully decode skill information from raw motion profiles via end-to-end\nlearning. Meanwhile, the proposed model is able to reliably interpret skills\nwithin 1-3 second window, without needing an observation of entire training\ntrial. This study highlights the potentials of deep architectures for an\nproficient online skill assessment in modern surgical training.",
        "url": "http://arxiv.org/pdf/1806.05796v2.pdf"
    },
    {
        "title": "Hardware Trojan Attacks on Neural Networks",
        "abstract": "With the rising popularity of machine learning and the ever increasing demand\nfor computational power, there is a growing need for hardware optimized\nimplementations of neural networks and other machine learning models. As the\ntechnology evolves, it is also plausible that machine learning or artificial\nintelligence will soon become consumer electronic products and military\nequipment, in the form of well-trained models. Unfortunately, the modern\nfabless business model of manufacturing hardware, while economic, leads to\ndeficiencies in security through the supply chain. In this paper, we illuminate\nthese security issues by introducing hardware Trojan attacks on neural\nnetworks, expanding the current taxonomy of neural network security to\nincorporate attacks of this nature. To aid in this, we develop a novel\nframework for inserting malicious hardware Trojans in the implementation of a\nneural network classifier. We evaluate the capabilities of the adversary in\nthis setting by implementing the attack algorithm on convolutional neural\nnetworks while controlling a variety of parameters available to the adversary.\nOur experimental results show that the proposed algorithm could effectively\nclassify a selected input trigger as a specified class on the MNIST dataset by\ninjecting hardware Trojans into $0.03\\%$, on average, of neurons in the 5th\nhidden layer of arbitrary 7-layer convolutional neural networks, while\nundetectable under the test data. Finally, we discuss the potential defenses to\nprotect neural networks against hardware Trojan attacks.",
        "url": "http://arxiv.org/pdf/1806.05768v1.pdf"
    },
    {
        "title": "PAC-Bayes Control: Learning Policies that Provably Generalize to Novel Environments",
        "abstract": "Our goal is to learn control policies for robots that provably generalize well to novel environments given a dataset of example environments. The key technical idea behind our approach is to leverage tools from generalization theory in machine learning by exploiting a precise analogy (which we present in the form of a reduction) between generalization of control policies to novel environments and generalization of hypotheses in the supervised learning setting. In particular, we utilize the Probably Approximately Correct (PAC)-Bayes framework, which allows us to obtain upper bounds that hold with high probability on the expected cost of (stochastic) control policies across novel environments. We propose policy learning algorithms that explicitly seek to minimize this upper bound. The corresponding optimization problem can be solved using convex optimization (Relative Entropy Programming in particular) in the setting where we are optimizing over a finite policy space. In the more general setting of continuously parameterized policies (e.g., neural network policies), we minimize this upper bound using stochastic gradient descent. We present simulated results of our approach applied to learning (1) reactive obstacle avoidance policies and (2) neural network-based grasping policies. We also present hardware results for the Parrot Swing drone navigating through different obstacle environments. Our examples demonstrate the potential of our approach to provide strong generalization guarantees for robotic systems with continuous state and action spaces, complicated (e.g., nonlinear) dynamics, rich sensory inputs (e.g., depth images), and neural network-based policies.",
        "url": "https://arxiv.org/pdf/1806.04225v5.pdf"
    },
    {
        "title": "Natural Language Processing for EHR-Based Computational Phenotyping",
        "abstract": "This article reviews recent advances in applying natural language processing\n(NLP) to Electronic Health Records (EHRs) for computational phenotyping.\nNLP-based computational phenotyping has numerous applications including\ndiagnosis categorization, novel phenotype discovery, clinical trial screening,\npharmacogenomics, drug-drug interaction (DDI) and adverse drug event (ADE)\ndetection, as well as genome-wide and phenome-wide association studies.\nSignificant progress has been made in algorithm development and resource\nconstruction for computational phenotyping. Among the surveyed methods,\nwell-designed keyword search and rule-based systems often achieve good\nperformance. However, the construction of keyword and rule lists requires\nsignificant manual effort, which is difficult to scale. Supervised machine\nlearning models have been favored because they are capable of acquiring both\nclassification patterns and structures from data. Recently, deep learning and\nunsupervised learning have received growing attention, with the former favored\nfor its performance and the latter for its ability to find novel phenotypes.\nIntegrating heterogeneous data sources have become increasingly important and\nhave shown promise in improving model performance. Often better performance is\nachieved by combining multiple modalities of information. Despite these many\nadvances, challenges and opportunities remain for NLP-based computational\nphenotyping, including better model interpretability and generalizability, and\nproper characterization of feature relations in clinical narratives",
        "url": "http://arxiv.org/pdf/1806.04820v2.pdf"
    },
    {
        "title": "Differentiable Submodular Maximization",
        "abstract": "We consider learning of submodular functions from data. These functions are\nimportant in machine learning and have a wide range of applications, e.g. data\nsummarization, feature selection and active learning. Despite their\ncombinatorial nature, submodular functions can be maximized approximately with\nstrong theoretical guarantees in polynomial time. Typically, learning the\nsubmodular function and optimization of that function are treated separately,\ni.e. the function is first learned using a proxy objective and subsequently\nmaximized. In contrast, we show how to perform learning and optimization\njointly. By interpreting the output of greedy maximization algorithms as\ndistributions over sequences of items and smoothening these distributions, we\nobtain a differentiable objective. In this way, we can differentiate through\nthe maximization algorithms and optimize the model to work well with the\noptimization algorithm. We theoretically characterize the error made by our\napproach, yielding insights into the tradeoff of smoothness and accuracy. We\ndemonstrate the effectiveness of our approach for jointly learning and\noptimizing on synthetic maximum cut data, and on real world applications such\nas product recommendation and image collection summarization.",
        "url": "http://arxiv.org/pdf/1803.01785v2.pdf"
    },
    {
        "title": "Towards Distributed Energy Services: Decentralizing Optimal Power Flow with Machine Learning",
        "abstract": "The implementation of optimal power flow (OPF) methods to perform voltage and power flow regulation in electric networks is generally believed to require extensive communication. We consider distribution systems with multiple controllable Distributed Energy Resources (DERs) and present a data-driven approach to learn control policies for each DER to reconstruct and mimic the solution to a centralized OPF problem from solely locally available information. Collectively, all local controllers closely match the centralized OPF solution, providing near optimal performance and satisfaction of system constraints. A rate distortion framework enables the analysis of how well the resulting fully decentralized control policies are able to reconstruct the OPF solution. The methodology provides a natural extension to decide what nodes a DER should communicate with to improve the reconstruction of its individual policy. The method is applied on both single- and three-phase test feeder networks using data from real loads and distributed generators, focusing on DERs that do not exhibit inter-temporal dependencies. It provides a framework for Distribution System Operators to efficiently plan and operate the contributions of DERs to achieve Distributed Energy Services in distribution networks.",
        "url": "https://arxiv.org/pdf/1806.06790v3.pdf"
    },
    {
        "title": "Multilevel Artificial Neural Network Training for Spatially Correlated Learning",
        "abstract": "Multigrid modeling algorithms are a technique used to accelerate relaxation models running on a hierarchy of similar graphlike structures. We introduce and demonstrate a new method for training neural networks which uses multilevel methods. Using an objective function derived from a graph-distance metric, we perform orthogonally-constrained optimization to find optimal prolongation and restriction maps between graphs. We compare and contrast several methods for performing this numerical optimization, and additionally present some new theoretical results on upper bounds of this type of objective function. Once calculated, these optimal maps between graphs form the core of Multiscale Artificial Neural Network (MsANN) training, a new procedure we present which simultaneously trains a hierarchy of neural network models of varying spatial resolution. Parameter information is passed between members of this hierarchy according to standard coarsening and refinement schedules from the multiscale modelling literature. In our machine learning experiments, these models are able to learn faster than default training, achieving a comparable level of error in an order of magnitude fewer training examples.",
        "url": "https://arxiv.org/pdf/1806.05703v3.pdf"
    },
    {
        "title": "The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning",
        "abstract": "In this paper we aim to formally explain the phenomenon of fast convergence\nof SGD observed in modern machine learning. The key observation is that most\nmodern learning architectures are over-parametrized and are trained to\ninterpolate the data by driving the empirical loss (classification and\nregression) close to zero. While it is still unclear why these interpolated\nsolutions perform well on test data, we show that these regimes allow for fast\nconvergence of SGD, comparable in number of iterations to full gradient\ndescent.\n  For convex loss functions we obtain an exponential convergence bound for {\\it\nmini-batch} SGD parallel to that for full gradient descent. We show that there\nis a critical batch size $m^*$ such that: (a) SGD iteration with mini-batch\nsize $m\\leq m^*$ is nearly equivalent to $m$ iterations of mini-batch size $1$\n(\\emph{linear scaling regime}). (b) SGD iteration with mini-batch $m> m^*$ is\nnearly equivalent to a full gradient descent iteration (\\emph{saturation\nregime}).\n  Moreover, for the quadratic loss, we derive explicit expressions for the\noptimal mini-batch and step size and explicitly characterize the two regimes\nabove. The critical mini-batch size can be viewed as the limit for effective\nmini-batch parallelization. It is also nearly independent of the data size,\nimplying $O(n)$ acceleration over GD per unit of computation. We give\nexperimental evidence on real data which closely follows our theoretical\nanalyses.\n  Finally, we show how our results fit in the recent developments in training\ndeep neural networks and discuss connections to adaptive rates for SGD and\nvariance reduction.",
        "url": "http://arxiv.org/pdf/1712.06559v3.pdf"
    },
    {
        "title": "Interactive Classification for Deep Learning Interpretation",
        "abstract": "We present an interactive system enabling users to manipulate images to\nexplore the robustness and sensitivity of deep learning image classifiers.\nUsing modern web technologies to run in-browser inference, users can remove\nimage features using inpainting algorithms and obtain new classifications in\nreal time, which allows them to ask a variety of \"what if\" questions by\nexperimentally modifying images and seeing how the model reacts. Our system\nallows users to compare and contrast what image regions humans and machine\nlearning models use for classification, revealing a wide range of surprising\nresults ranging from spectacular failures (e.g., a \"water bottle\" image becomes\na \"concert\" when removing a person) to impressive resilience (e.g., a \"baseball\nplayer\" image remains correctly classified even without a glove or base). We\ndemonstrate our system at The 2018 Conference on Computer Vision and Pattern\nRecognition (CVPR) for the audience to try it live. Our system is open-sourced\nat https://github.com/poloclub/interactive-classification. A video demo is\navailable at https://youtu.be/llub5GcOF6w.",
        "url": "http://arxiv.org/pdf/1806.05660v2.pdf"
    },
    {
        "title": "Gender Prediction in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System",
        "abstract": "The rapid expansion in the usage of social media networking sites leads to a\nhuge amount of unprocessed user generated data which can be used for text\nmining. Author profiling is the problem of automatically determining profiling\naspects like the author's gender and age group through a text is gaining much\npopularity in computational linguistics. Most of the past research in author\nprofiling is concentrated on English texts \\cite{1,2}. However many users often\nchange the language while posting on social media which is called code-mixing,\nand it develops some challenges in the field of text classification and author\nprofiling like variations in spelling, non-grammatical structure and\ntransliteration \\cite{3}. There are very few English-Hindi code-mixed annotated\ndatasets of social media content present online \\cite{4}. In this paper, we\nanalyze the task of author's gender prediction in code-mixed content and\npresent a corpus of English-Hindi texts collected from Twitter which is\nannotated with author's gender. We also explore language identification of\nevery word in this corpus. We present a supervised classification baseline\nsystem which uses various machine learning algorithms to identify the gender of\nan author using a text, based on character and word level features.",
        "url": "http://arxiv.org/pdf/1806.05600v1.pdf"
    },
    {
        "title": "The Exact Equivalence of Distance and Kernel Methods for Hypothesis Testing",
        "abstract": "Distance-based tests, also called \"energy statistics\", are leading methods for two-sample and independence tests from the statistics community. Kernel-based tests, developed from \"kernel mean embeddings\", are leading methods for two-sample and independence tests from the machine learning community. A fixed-point transformation was previously proposed to connect the distance methods and kernel methods for the population statistics. In this paper, we propose a new bijective transformation between metrics and kernels. It simplifies the fixed-point transformation, inherits similar theoretical properties, allows distance methods to be exactly the same as kernel methods for sample statistics and p-value, and better preserves the data structure upon transformation. Our results further advance the understanding in distance and kernel-based tests, streamline the code base for implementing these tests, and enable a rich literature of distance-based and kernel-based methodologies to directly communicate with each other.",
        "url": "https://arxiv.org/pdf/1806.05514v5.pdf"
    },
    {
        "title": "ServeNet: A Deep Neural Network for Web Services Classification",
        "abstract": "Automated service classification plays a crucial role in service discovery, selection, and composition. Machine learning has been widely used for service classification in recent years. However, the performance of conventional machine learning methods highly depends on the quality of manual feature engineering. In this paper, we present a novel deep neural network to automatically abstract low-level representation of both service name and service description to high-level merged features without feature engineering and the length limitation, and then predict service classification on 50 service categories. To demonstrate the effectiveness of our approach, we conduct a comprehensive experimental study by comparing 10 machine learning methods on 10,000 real-world web services. The result shows that the proposed deep neural network can achieve higher accuracy in classification and more robust than other machine learning methods.",
        "url": "https://arxiv.org/pdf/1806.05437v3.pdf"
    },
    {
        "title": "Aggregating Predictions on Multiple Non-disclosed Datasets using Conformal Prediction",
        "abstract": "Conformal Prediction is a machine learning methodology that produces valid\nprediction regions under mild conditions. In this paper, we explore the\napplication of making predictions over multiple data sources of different sizes\nwithout disclosing data between the sources. We propose that each data source\napplies a transductive conformal predictor independently using the local data,\nand that the individual predictions are then aggregated to form a combined\nprediction region. We demonstrate the method on several data sets, and show\nthat the proposed method produces conservatively valid predictions and reduces\nthe variance in the aggregated predictions. We also study the effect that the\nnumber of data sources and size of each source has on aggregated predictions,\nas compared with equally sized sources and pooled data.",
        "url": "http://arxiv.org/pdf/1806.04000v2.pdf"
    },
    {
        "title": "A Fast Proximal Point Method for Computing Exact Wasserstein Distance",
        "abstract": "Wasserstein distance plays increasingly important roles in machine learning, stochastic programming and image processing. Major efforts have been under way to address its high computational complexity, some leading to approximate or regularized variations such as Sinkhorn distance. However, as we will demonstrate, regularized variations with large regularization parameter will degradate the performance in several important machine learning applications, and small regularization parameter will fail due to numerical stability issues with existing algorithms. We address this challenge by developing an Inexact Proximal point method for exact Optimal Transport problem (IPOT) with the proximal operator approximately evaluated at each iteration using projections to the probability simplex. The algorithm (a) converges to exact Wasserstein distance with theoretical guarantee and robust regularization parameter selection, (b) alleviates numerical stability issue, (c) has similar computational complexity to Sinkhorn, and (d) avoids the shrinking problem when apply to generative models. Furthermore, a new algorithm is proposed based on IPOT to obtain sharper Wasserstein barycenter.",
        "url": "https://arxiv.org/pdf/1802.04307v3.pdf"
    },
    {
        "title": "Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity",
        "abstract": "Online optimization has been a successful framework for solving large-scale\nproblems under computational constraints and partial information. Current\nmethods for online convex optimization require either a projection or exact\ngradient computation at each step, both of which can be prohibitively expensive\nfor large-scale applications. At the same time, there is a growing trend of\nnon-convex optimization in machine learning community and a need for online\nmethods. Continuous DR-submodular functions, which exhibit a natural\ndiminishing returns condition, have recently been proposed as a broad class of\nnon-convex functions which may be efficiently optimized. Although online\nmethods have been introduced, they suffer from similar problems. In this work,\nwe propose Meta-Frank-Wolfe, the first online projection-free algorithm that\nuses stochastic gradient estimates. The algorithm relies on a careful sampling\nof gradients in each round and achieves the optimal $O( \\sqrt{T})$ adversarial\nregret bounds for convex and continuous submodular optimization. We also\npropose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single\nstochastic gradient estimate in each round and achieves an $O(T^{2/3})$\nstochastic regret bound for convex and continuous submodular optimization. We\napply our methods to develop a novel \"lifting\" framework for the online\ndiscrete submodular maximization and also see that they outperform current\nstate-of-the-art techniques on various experiments.",
        "url": "http://arxiv.org/pdf/1802.08183v4.pdf"
    },
    {
        "title": "Improving Robustness of ML Classifiers against Realizable Evasion Attacks Using Conserved Features",
        "abstract": "Machine learning (ML) techniques are increasingly common in security applications, such as malware and intrusion detection. However, ML models are often susceptible to evasion attacks, in which an adversary makes changes to the input (such as malware) in order to avoid being detected. A conventional approach to evaluate ML robustness to such attacks, as well as to design robust ML, is by considering simplified feature-space models of attacks, where the attacker changes ML features directly to effect evasion, while minimizing or constraining the magnitude of this change. We investigate the effectiveness of this approach to designing robust ML in the face of attacks that can be realized in actual malware (realizable attacks). We demonstrate that in the context of structure-based PDF malware detection, such techniques appear to have limited effectiveness, but they are effective with content-based detectors. In either case, we show that augmenting the feature space models with conserved features (those that cannot be unilaterally modified without compromising malicious functionality) significantly improves performance. Finally, we show that feature space models enable generalized robustness when faced with a variety of realizable attacks, as compared to classifiers which are tuned to be robust to a specific realizable attack.",
        "url": "https://arxiv.org/pdf/1708.08327v5.pdf"
    },
    {
        "title": "A comparison of methods for model selection when estimating individual treatment effects",
        "abstract": "Practitioners in medicine, business, political science, and other fields are\nincreasingly aware that decisions should be personalized to each patient,\ncustomer, or voter. A given treatment (e.g. a drug or advertisement) should be\nadministered only to those who will respond most positively, and certainly not\nto those who will be harmed by it. Individual-level treatment effects can be\nestimated with tools adapted from machine learning, but different models can\nyield contradictory estimates. Unlike risk prediction models, however,\ntreatment effect models cannot be easily evaluated against each other using a\nheld-out test set because the true treatment effect itself is never directly\nobserved. Besides outcome prediction accuracy, several metrics that can\nleverage held-out data to evaluate treatment effects models have been proposed,\nbut they are not widely used. We provide a didactic framework that elucidates\nthe relationships between the different approaches and compare them all using a\nvariety of simulations of both randomized and observational data. Our results\nshow that researchers estimating heterogenous treatment effects need not limit\nthemselves to a single model-fitting algorithm. Instead of relying on a single\nmethod, multiple models fit by a diverse set of algorithms should be evaluated\nagainst each other using an objective function learned from the validation set.\nThe model minimizing that objective should be used for estimating the\nindividual treatment effect for future individuals.",
        "url": "http://arxiv.org/pdf/1804.05146v2.pdf"
    },
    {
        "title": "What About Applied Fairness?",
        "abstract": "Machine learning practitioners are often ambivalent about the ethical aspects\nof their products. We believe anything that gets us from that current state to\none in which our systems are achieving some degree of fairness is an\nimprovement that should be welcomed. This is true even when that progress does\nnot get us 100% of the way to the goal of \"complete\" fairness or perfectly\nalign with our personal belief on which measure of fairness is used. Some\nmeasure of fairness being built would still put us in a better position than\nthe status quo. Impediments to getting fairness and ethical concerns applied in\nreal applications, whether they are abstruse philosophical debates or technical\noverhead such as the introduction of ever more hyper-parameters, should be\navoided. In this paper we further elaborate on our argument for this viewpoint\nand its importance.",
        "url": "http://arxiv.org/pdf/1806.05250v1.pdf"
    },
    {
        "title": "Interpretable Machine Learning for Privacy-Preserving Pervasive Systems",
        "abstract": "Our everyday interactions with pervasive systems generate traces that capture various aspects of human behavior and enable machine learning algorithms to extract latent information about users. In this paper, we propose a machine learning interpretability framework that enables users to understand how these generated traces violate their privacy.",
        "url": "https://arxiv.org/pdf/1710.08464v6.pdf"
    },
    {
        "title": "Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate",
        "abstract": "Many modern machine learning models are trained to achieve zero or near-zero\ntraining error in order to obtain near-optimal (but non-zero) test error. This\nphenomenon of strong generalization performance for \"overfitted\" / interpolated\nclassifiers appears to be ubiquitous in high-dimensional data, having been\nobserved in deep networks, kernel machines, boosting and random forests. Their\nperformance is consistently robust even when the data contain large amounts of\nlabel noise.\n  Very little theory is available to explain these observations. The vast\nmajority of theoretical analyses of generalization allows for interpolation\nonly when there is little or no label noise. This paper takes a step toward a\ntheoretical foundation for interpolated classifiers by analyzing local\ninterpolating schemes, including geometric simplicial interpolation algorithm\nand singularly weighted $k$-nearest neighbor schemes. Consistency or\nnear-consistency is proved for these schemes in classification and regression\nproblems. Moreover, the nearest neighbor schemes exhibit optimal rates under\nsome standard statistical assumptions.\n  Finally, this paper suggests a way to explain the phenomenon of adversarial\nexamples, which are seemingly ubiquitous in modern machine learning, and also\ndiscusses some connections to kernel machines and random forests in the\ninterpolated regime.",
        "url": "http://arxiv.org/pdf/1806.05161v3.pdf"
    },
    {
        "title": "On Landscape of Lagrangian Functions and Stochastic Search for Constrained Nonconvex Optimization",
        "abstract": "We study constrained nonconvex optimization problems in machine learning, signal processing, and stochastic control. It is well-known that these problems can be rewritten to a minimax problem in a Lagrangian form. However, due to the lack of convexity, their landscape is not well understood and how to find the stable equilibria of the Lagrangian function is still unknown. To bridge the gap, we study the landscape of the Lagrangian function. Further, we define a special class of Lagrangian functions. They enjoy two properties: 1.Equilibria are either stable or unstable (Formal definition in Section 2); 2.Stable equilibria correspond to the global optima of the original problem. We show that a generalized eigenvalue (GEV) problem, including canonical correlation analysis and other problems, belongs to the class. Specifically, we characterize its stable and unstable equilibria by leveraging an invariant group and symmetric property (more details in Section 3). Motivated by these neat geometric structures, we propose a simple, efficient, and stochastic primal-dual algorithm solving the online GEV problem. Theoretically, we provide sufficient conditions, based on which we establish an asymptotic convergence rate and obtain the first sample complexity result for the online GEV problem by diffusion approximations, which are widely used in applied probability and stochastic control. Numerical results are provided to support our theory.",
        "url": "https://arxiv.org/pdf/1806.05151v3.pdf"
    },
    {
        "title": "3D Convolutional Neural Networks for Classification of Functional Connectomes",
        "abstract": "Resting-state functional MRI (rs-fMRI) scans hold the potential to serve as a\ndiagnostic or prognostic tool for a wide variety of conditions, such as autism,\nAlzheimer's disease, and stroke. While a growing number of studies have\ndemonstrated the promise of machine learning algorithms for rs-fMRI based\nclinical or behavioral prediction, most prior models have been limited in their\ncapacity to exploit the richness of the data. For example, classification\ntechniques applied to rs-fMRI often rely on region-based summary statistics\nand/or linear models. In this work, we propose a novel volumetric Convolutional\nNeural Network (CNN) framework that takes advantage of the full-resolution 3D\nspatial structure of rs-fMRI data and fits non-linear predictive models. We\nshowcase our approach on a challenging large-scale dataset (ABIDE, with N >\n2,000) and report state-of-the-art accuracy results on rs-fMRI-based\ndiscrimination of autism patients and healthy controls.",
        "url": "http://arxiv.org/pdf/1806.04209v2.pdf"
    },
    {
        "title": "Lagrange Coded Computing: Optimal Design for Resiliency, Security and Privacy",
        "abstract": "We consider a scenario involving computations over a massive dataset stored\ndistributedly across multiple workers, which is at the core of distributed\nlearning algorithms. We propose Lagrange Coded Computing (LCC), a new framework\nto simultaneously provide (1) resiliency against stragglers that may prolong\ncomputations; (2) security against Byzantine (or malicious) workers that\ndeliberately modify the computation for their benefit; and (3)\n(information-theoretic) privacy of the dataset amidst possible collusion of\nworkers. LCC, which leverages the well-known Lagrange polynomial to create\ncomputation redundancy in a novel coded form across workers, can be applied to\nany computation scenario in which the function of interest is an arbitrary\nmultivariate polynomial of the input dataset, hence covering many computations\nof interest in machine learning. LCC significantly generalizes prior works to\ngo beyond linear computations. It also enables secure and private computing in\ndistributed settings, improving the computation and communication efficiency of\nthe state-of-the-art. Furthermore, we prove the optimality of LCC by showing\nthat it achieves the optimal tradeoff between resiliency, security, and\nprivacy, i.e., in terms of tolerating the maximum number of stragglers and\nadversaries, and providing data privacy against the maximum number of colluding\nworkers. Finally, we show via experiments on Amazon EC2 that LCC speeds up the\nconventional uncoded implementation of distributed least-squares linear\nregression by up to $13.43\\times$, and also achieves a\n$2.36\\times$-$12.65\\times$ speedup over the state-of-the-art straggler\nmitigation strategies.",
        "url": "http://arxiv.org/pdf/1806.00939v4.pdf"
    },
    {
        "title": "Comparing Fairness Criteria Based on Social Outcome",
        "abstract": "Fairness in algorithmic decision-making processes is attracting increasing\nconcern. When an algorithm is applied to human-related decision-making an\nestimator solely optimizing its predictive power can learn biases on the\nexisting data, which motivates us the notion of fairness in machine learning.\nwhile several different notions are studied in the literature, little studies\nare done on how these notions affect the individuals. We demonstrate such a\ncomparison between several policies induced by well-known fairness criteria,\nincluding the color-blind (CB), the demographic parity (DP), and the equalized\nodds (EO). We show that the EO is the only criterion among them that removes\ngroup-level disparity. Empirical studies on the social welfare and disparity of\nthese policies are conducted.",
        "url": "http://arxiv.org/pdf/1806.05112v1.pdf"
    },
    {
        "title": "Informative Gene Selection for Microarray Classification via Adaptive Elastic Net with Conditional Mutual Information",
        "abstract": "Due to the advantage of achieving a better performance under weak\nregularization, elastic net has attracted wide attention in statistics, machine\nlearning, bioinformatics, and other fields. In particular, a variation of the\nelastic net, adaptive elastic net (AEN), integrates the adaptive grouping\neffect. In this paper, we aim to develop a new algorithm: Adaptive Elastic Net\nwith Conditional Mutual Information (AEN-CMI) that further improves AEN by\nincorporating conditional mutual information into the gene selection process.\nWe apply this new algorithm to screen significant genes for two kinds of\ncancers: colon cancer and leukemia. Compared with other algorithms including\nSupport Vector Machine, Classic Elastic Net and Adaptive Elastic Net, the\nproposed algorithm, AEN-CMI, obtains the best classification performance using\nthe least number of genes.",
        "url": "http://arxiv.org/pdf/1806.01466v3.pdf"
    },
    {
        "title": "Hyperdrive: A Multi-Chip Systolically Scalable Binary-Weight CNN Inference Engine",
        "abstract": "Deep neural networks have achieved impressive results in computer vision and\nmachine learning. Unfortunately, state-of-the-art networks are extremely\ncompute and memory intensive which makes them unsuitable for mW-devices such as\nIoT end-nodes. Aggressive quantization of these networks dramatically reduces\nthe computation and memory footprint. Binary-weight neural networks (BWNs)\nfollow this trend, pushing weight quantization to the limit. Hardware\naccelerators for BWNs presented up to now have focused on core efficiency,\ndisregarding I/O bandwidth and system-level efficiency that are crucial for\ndeployment of accelerators in ultra-low power devices. We present Hyperdrive: a\nBWN accelerator dramatically reducing the I/O bandwidth exploiting a novel\nbinary-weight streaming approach, which can be used for arbitrarily sized\nconvolutional neural network architecture and input resolution by exploiting\nthe natural scalability of the compute units both at chip-level and\nsystem-level by arranging Hyperdrive chips systolically in a 2D mesh while\nprocessing the entire feature map together in parallel. Hyperdrive achieves 4.3\nTOp/s/W system-level efficiency (i.e., including I/Os)---3.1x higher than\nstate-of-the-art BWN accelerators, even if its core uses resource-intensive\nFP16 arithmetic for increased robustness.",
        "url": "http://arxiv.org/pdf/1804.00623v3.pdf"
    },
    {
        "title": "Safe learning-based optimal motion planning for automated driving",
        "abstract": "This paper presents preliminary work on learning the search heuristic for the\noptimal motion planning for automated driving in urban traffic. Previous work\nconsidered search-based optimal motion planning framework (SBOMP) that utilized\nnumerical or model-based heuristics that did not consider dynamic obstacles.\nOptimal solution was still guaranteed since dynamic obstacles can only increase\nthe cost. However, significant variations in the search efficiency are observed\ndepending whether dynamic obstacles are present or not. This paper introduces\nmachine learning (ML) based heuristic that takes into account dynamic\nobstacles, thus adding to the performance consistency for achieving real-time\nimplementation.",
        "url": "http://arxiv.org/pdf/1805.09994v2.pdf"
    },
    {
        "title": "Crowd-Powered Data Mining",
        "abstract": "Many data mining tasks cannot be completely addressed by auto- mated\nprocesses, such as sentiment analysis and image classification. Crowdsourcing\nis an effective way to harness the human cognitive ability to process these\nmachine-hard tasks. Thanks to public crowdsourcing platforms, e.g., Amazon\nMechanical Turk and Crowd- Flower, we can easily involve hundreds of thousands\nof ordinary workers (i.e., the crowd) to address these machine-hard tasks. In\nthis tutorial, we will survey and synthesize a wide spectrum of existing\nstudies on crowd-powered data mining. We first give an overview of\ncrowdsourcing, and then summarize the fundamental techniques, including quality\ncontrol, cost control, and latency control, which must be considered in\ncrowdsourced data mining. Next we review crowd-powered data mining operations,\nincluding classification, clustering, pattern mining, machine learning using\nthe crowd (including deep learning, transfer learning and semi-supervised\nlearning) and knowledge discovery. Finally, we provide the emerging challenges\nin crowdsourced data mining.",
        "url": "http://arxiv.org/pdf/1806.04968v2.pdf"
    },
    {
        "title": "Towards Semantically Enhanced Data Understanding",
        "abstract": "In the field of machine learning, data understanding is the practice of\ngetting initial insights in unknown datasets. Such knowledge-intensive tasks\nrequire a lot of documentation, which is necessary for data scientists to grasp\nthe meaning of the data. Usually, documentation is separate from the data in\nvarious external documents, diagrams, spreadsheets and tools which causes\nconsiderable look up overhead. Moreover, other supporting applications are not\nable to consume and utilize such unstructured data. That is why we propose a\nmethodology that uses a single semantic model that interlinks data with its\ndocumentation. Hence, data scientists are able to directly look up the\nconnected information about the data by simply following links. Equally, they\ncan browse the documentation which always refers to the data. Furthermore, the\nmodel can be used by other approaches providing additional support, like\nsearching, comparing, integrating or visualizing data. To showcase our approach\nwe also demonstrate an early prototype.",
        "url": "http://arxiv.org/pdf/1806.04952v1.pdf"
    },
    {
        "title": "A Machine-Learning Item Recommendation System for Video Games",
        "abstract": "Video-game players generate huge amounts of data, as everything they do\nwithin a game is recorded. In particular, among all the stored actions and\nbehaviors, there is information on the in-game purchases of virtual products.\nSuch information is of critical importance in modern free-to-play titles, where\ngamers can select or buy a profusion of items during the game in order to\nprogress and fully enjoy their experience.\n  To try to maximize these kind of purchases, one can use a recommendation\nsystem so as to present players with items that might be interesting for them.\nSuch systems can better achieve their goal by employing machine learning\nalgorithms that are able to predict the rating of an item or product by a\nparticular user. In this paper we evaluate and compare two of these algorithms,\nan ensemble-based model (extremely randomized trees) and a deep neural network,\nboth of which are promising candidates for operational video-game recommender\nengines.\n  Item recommenders can help developers improve the game. But, more\nimportantly, it should be possible to integrate them into the game, so that\nusers automatically get personalized recommendations while playing. The\npresented models are not only able to meet this challenge, providing accurate\npredictions of the items that a particular player will find attractive, but\nalso sufficiently fast and robust to be used in operational settings.",
        "url": "http://arxiv.org/pdf/1806.04900v2.pdf"
    },
    {
        "title": "mQAPViz: A divide-and-conquer multi-objective optimization algorithm to compute large data visualizations",
        "abstract": "Algorithms for data visualizations are essential tools for transforming data\ninto useful narratives. Unfortunately, very few visualization algorithms can\nhandle the large datasets of many real-world scenarios. In this study, we\naddress the visualization of these datasets as a Multi-Objective Optimization\nProblem. We propose mQAPViz, a divide-and-conquer multi-objective optimization\nalgorithm to compute large-scale data visualizations. Our method employs the\nMulti-Objective Quadratic Assignment Problem (mQAP) as the mathematical\nfoundation to solve the visualization task at hand. The algorithm applies\nadvanced sampling techniques originating from the field of machine learning and\nefficient data structures to scale to millions of data objects. The algorithm\nallocates objects onto a 2D grid layout. Experimental results on real-world and\nlarge datasets demonstrate that mQAPViz is a competitive alternative to\nexisting techniques.",
        "url": "http://arxiv.org/pdf/1804.00656v3.pdf"
    },
    {
        "title": "Cell Identity Codes: Understanding Cell Identity from Gene Expression Profiles using Deep Neural Networks",
        "abstract": "Understanding cell identity is an important task in many biomedical areas.\nExpression patterns of specific marker genes have been used to characterize\nsome limited cell types, but exclusive markers are not available for many cell\ntypes. A second approach is to use machine learning to discriminate cell types\nbased on the whole gene expression profiles (GEPs). The accuracies of simple\nclassification algorithms such as linear discriminators or support vector\nmachines are limited due to the complexity of biological systems. We used deep\nneural networks to analyze 1040 GEPs from 16 different human tissues and cell\ntypes. After comparing different architectures, we identified a specific\nstructure of deep autoencoders that can encode a GEP into a vector of 30\nnumeric values, which we call the cell identity code (CIC). The original GEP\ncan be reproduced from the CIC with an accuracy comparable to technical\nreplicates of the same experiment. Although we use an unsupervised approach to\ntrain the autoencoder, we show different values of the CIC are connected to\ndifferent biological aspects of the cell, such as different pathways or\nbiological processes. This network can use CIC to reproduce the GEP of the cell\ntypes it has never seen during the training. It also can resist some noise in\nthe measurement of the GEP. Furthermore, we introduce classifier autoencoder,\nan architecture that can accurately identify cell type based on the GEP or the\nCIC.",
        "url": "http://arxiv.org/pdf/1806.04863v1.pdf"
    },
    {
        "title": "Learning Structural Node Embeddings Via Diffusion Wavelets",
        "abstract": "Nodes residing in different parts of a graph can have similar structural\nroles within their local network topology. The identification of such roles\nprovides key insight into the organization of networks and can be used for a\nvariety of machine learning tasks. However, learning structural representations\nof nodes is a challenging problem, and it has typically involved manually\nspecifying and tailoring topological features for each node. In this paper, we\ndevelop GraphWave, a method that represents each node's network neighborhood\nvia a low-dimensional embedding by leveraging heat wavelet diffusion patterns.\nInstead of training on hand-selected features, GraphWave learns these\nembeddings in an unsupervised way. We mathematically prove that nodes with\nsimilar network neighborhoods will have similar GraphWave embeddings even\nthough these nodes may reside in very different parts of the network, and our\nmethod scales linearly with the number of edges. Experiments in a variety of\ndifferent settings demonstrate GraphWave's real-world potential for capturing\nstructural roles in networks, and our approach outperforms existing\nstate-of-the-art baselines in every experiment, by as much as 137%.",
        "url": "http://arxiv.org/pdf/1710.10321v4.pdf"
    },
    {
        "title": "Learning from Mutants: Using Code Mutation to Learn and Monitor Invariants of a Cyber-Physical System",
        "abstract": "Cyber-physical systems (CPS) consist of sensors, actuators, and controllers\nall communicating over a network; if any subset becomes compromised, an\nattacker could cause significant damage. With access to data logs and a model\nof the CPS, the physical effects of an attack could potentially be detected\nbefore any damage is done. Manually building a model that is accurate enough in\npractice, however, is extremely difficult. In this paper, we propose a novel\napproach for constructing models of CPS automatically, by applying supervised\nmachine learning to data traces obtained after systematically seeding their\nsoftware components with faults (\"mutants\"). We demonstrate the efficacy of\nthis approach on the simulator of a real-world water purification plant,\npresenting a framework that automatically generates mutants, collects data\ntraces, and learns an SVM-based model. Using cross-validation and statistical\nmodel checking, we show that the learnt model characterises an invariant\nphysical property of the system. Furthermore, we demonstrate the usefulness of\nthe invariant by subjecting the system to 55 network and code-modification\nattacks, and showing that it can detect 85% of them from the data logs\ngenerated at runtime.",
        "url": "http://arxiv.org/pdf/1801.00903v2.pdf"
    },
    {
        "title": "Regularized Orthogonal Machine Learning for Nonlinear Semiparametric Models",
        "abstract": "This paper proposes a Lasso-type estimator for a high-dimensional sparse parameter identified by a single index conditional moment restriction (CMR). In addition to this parameter, the moment function can also depend on a nuisance function, such as the propensity score or the conditional choice probability, which we estimate by modern machine learning tools. We first adjust the moment function so that the gradient of the future loss function is insensitive (formally, Neyman-orthogonal) with respect to the first-stage regularization bias, preserving the single index property. We then take the loss function to be an indefinite integral of the adjusted moment function with respect to the single index. The proposed Lasso estimator converges at the oracle rate, where the oracle knows the nuisance function and solves only the parametric problem. We demonstrate our method by estimating the short-term heterogeneous impact of Connecticut's Jobs First welfare reform experiment on women's welfare participation decision.",
        "url": "https://arxiv.org/pdf/1806.04823v8.pdf"
    },
    {
        "title": "Deep Learning for Forecasting Stock Returns in the Cross-Section",
        "abstract": "Many studies have been undertaken by using machine learning techniques,\nincluding neural networks, to predict stock returns. Recently, a method known\nas deep learning, which achieves high performance mainly in image recognition\nand speech recognition, has attracted attention in the machine learning field.\nThis paper implements deep learning to predict one-month-ahead stock returns in\nthe cross-section in the Japanese stock market and investigates the performance\nof the method. Our results show that deep neural networks generally outperform\nshallow neural networks, and the best networks also outperform representative\nmachine learning models. These results indicate that deep learning shows\npromise as a skillful machine learning method to predict stock returns in the\ncross-section.",
        "url": "http://arxiv.org/pdf/1801.01777v4.pdf"
    },
    {
        "title": "Static Malware Detection & Subterfuge: Quantifying the Robustness of Machine Learning and Current Anti-Virus",
        "abstract": "As machine-learning (ML) based systems for malware detection become more\nprevalent, it becomes necessary to quantify the benefits compared to the more\ntraditional anti-virus (AV) systems widely used today. It is not practical to\nbuild an agreed upon test set to benchmark malware detection systems on pure\nclassification performance. Instead we tackle the problem by creating a new\ntesting methodology, where we evaluate the change in performance on a set of\nknown benign & malicious files as adversarial modifications are performed. The\nchange in performance combined with the evasion techniques then quantifies a\nsystem's robustness against that approach. Through these experiments we are\nable to show in a quantifiable way how purely ML based systems can be more\nrobust than AV products at detecting malware that attempts evasion through\nmodification, but may be slower to adapt in the face of significantly novel\nattacks.",
        "url": "http://arxiv.org/pdf/1806.04773v1.pdf"
    },
    {
        "title": "Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons",
        "abstract": "Deep feed-forward convolutional neural networks (CNNs) have become ubiquitous\nin virtually all machine learning and computer vision challenges; however,\nadvancements in CNNs have arguably reached an engineering saturation point\nwhere incremental novelty results in minor performance gains. Although there is\nevidence that object classification has reached human levels on narrowly\ndefined tasks, for general applications, the biological visual system is far\nsuperior to that of any computer. Research reveals there are numerous missing\ncomponents in feed-forward deep neural networks that are critical in mammalian\nvision. The brain does not work solely in a feed-forward fashion, but rather\nall of the neurons are in competition with each other; neurons are integrating\ninformation in a bottom up and top down fashion and incorporating expectation\nand feedback in the modeling process. Furthermore, our visual cortex is working\nin tandem with our parietal lobe, integrating sensory information from various\nmodalities.\n  In our work, we sought to improve upon the standard feed-forward deep\nlearning model by augmenting them with biologically inspired concepts of\nsparsity, top-down feedback, and lateral inhibition. We define our model as a\nsparse coding problem using hierarchical layers. We solve the sparse coding\nproblem with an additional top-down feedback error driving the dynamics of the\nneural network. While building and observing the behavior of our model, we were\nfascinated that multimodal, invariant neurons naturally emerged that mimicked,\n\"Halle Berry neurons\" found in the human brain. Furthermore, our sparse\nrepresentation of multimodal signals demonstrates qualitative and quantitative\nsuperiority to the standard feed-forward joint embedding in common vision and\nmachine learning tasks.",
        "url": "http://arxiv.org/pdf/1711.07998v2.pdf"
    },
    {
        "title": "Fully Convolutional Network for Melanoma Diagnostics",
        "abstract": "This work seeks to determine how modern machine learning techniques may be\napplied to the previously unexplored topic of melanoma diagnostics using\ndigital pathology. We curated a new dataset of 50 patient cases of cutaneous\nmelanoma using digital pathology. We provide gold standard annotations for\nthree tissue types (tumour, epidermis, and dermis) which are important for the\nprognostic measurements known as Breslow thickness and Clark level. Then, we\ndevised a novel multi-stride fully convolutional network (FCN) architecture\nthat outperformed other networks trained and evaluated using the same data\naccording to standard metrics. Finally, we trained a model to detect and\nlocalize the target tissue types. When processing previously unseen cases, our\nmodel's output is qualitatively very similar to the gold standard. In addition\nto the standard metrics computed as a baseline for our approach, we asked three\nadditional pathologists to measure the Breslow thickness on the network's\noutput. Their responses were diagnostically equivalent to the ground truth\nmeasurements, and when removing cases where a measurement was not appropriate,\ninter-rater reliability (IRR) between the four pathologists was 75.0%. Given\nthe qualitative and quantitative results, it is possible to overcome the\ndiscriminative challenges of the skin and tumour anatomy for segmentation using\nmodern machine learning techniques, though more work is required to improve the\nnetwork's performance on dermis segmentation. Further, we show that it is\npossible to achieve a level of accuracy required to manually perform the\nBreslow thickness measurement.",
        "url": "http://arxiv.org/pdf/1806.04765v1.pdf"
    },
    {
        "title": "Adversarial Attacks on Variational Autoencoders",
        "abstract": "Adversarial attacks are malicious inputs that derail machine-learning models.\nWe propose a scheme to attack autoencoders, as well as a quantitative\nevaluation framework that correlates well with the qualitative assessment of\nthe attacks. We assess --- with statistically validated experiments --- the\nresistance to attacks of three variational autoencoders (simple, convolutional,\nand DRAW) in three datasets (MNIST, SVHN, CelebA), showing that both DRAW's\nrecurrence and attention mechanism lead to better resistance. As autoencoders\nare proposed for compressing data --- a scenario in which their safety is\nparamount --- we expect more attention will be given to adversarial attacks on\nthem.",
        "url": "http://arxiv.org/pdf/1806.04646v1.pdf"
    },
    {
        "title": "Streaming PCA and Subspace Tracking: The Missing Data Case",
        "abstract": "For many modern applications in science and engineering, data are collected\nin a streaming fashion carrying time-varying information, and practitioners\nneed to process them with a limited amount of memory and computational\nresources in a timely manner for decision making. This often is coupled with\nthe missing data problem, such that only a small fraction of data attributes\nare observed. These complications impose significant, and unconventional,\nconstraints on the problem of streaming Principal Component Analysis (PCA) and\nsubspace tracking, which is an essential building block for many inference\ntasks in signal processing and machine learning. This survey article reviews a\nvariety of classical and recent algorithms for solving this problem with low\ncomputational and memory complexities, particularly those applicable in the big\ndata regime with missing data. We illustrate that streaming PCA and subspace\ntracking algorithms can be understood through algebraic and geometric\nperspectives, and they need to be adjusted carefully to handle missing data.\nBoth asymptotic and non-asymptotic convergence guarantees are reviewed.\nFinally, we benchmark the performance of several competitive algorithms in the\npresence of missing data for both well-conditioned and ill-conditioned systems.",
        "url": "http://arxiv.org/pdf/1806.04609v1.pdf"
    },
    {
        "title": "Logistic Ensemble Models",
        "abstract": "Predictive models that are developed in a regulated industry or a regulated\napplication, like determination of credit worthiness, must be interpretable and\nrational (e.g., meaningful improvements in basic credit behavior must result in\nimproved credit worthiness scores). Machine Learning technologies provide very\ngood performance with minimal analyst intervention, making them well suited to\na high volume analytic environment, but the majority are black box tools that\nprovide very limited insight or interpretability into key drivers of model\nperformance or predicted model output values. This paper presents a methodology\nthat blends one of the most popular predictive statistical modeling methods for\nbinary classification with a core model enhancement strategy found in machine\nlearning. The resulting prediction methodology provides solid performance, from\nminimal analyst effort, while providing the interpretability and rationality\nrequired in regulated industries, as well as in other environments where\ninterpretation of model parameters is required (e.g. businesses that require\ninterpretation of models, to take action on them).",
        "url": "http://arxiv.org/pdf/1806.04555v1.pdf"
    },
    {
        "title": "Bitcoin Volatility Forecasting with a Glimpse into Buy and Sell Orders",
        "abstract": "In this paper, we study the ability to make the short-term prediction of the\nexchange price fluctuations towards the United States dollar for the Bitcoin\nmarket. We use the data of realized volatility collected from one of the\nlargest Bitcoin digital trading offices in 2016 and 2017 as well as order\ninformation. Experiments are performed to evaluate a variety of statistical and\nmachine learning approaches.",
        "url": "http://arxiv.org/pdf/1802.04065v3.pdf"
    },
    {
        "title": "Adversarial Attacks on Neural Networks for Graph Data",
        "abstract": "Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.",
        "url": "https://arxiv.org/pdf/1805.07984v4.pdf"
    },
    {
        "title": "Learning to Automatically Generate Fill-In-The-Blank Quizzes",
        "abstract": "In this paper we formalize the problem automatic fill-in-the-blank question\ngeneration using two standard NLP machine learning schemes, proposing concrete\ndeep learning models for each. We present an empirical study based on data\nobtained from a language learning platform showing that both of our proposed\nsettings offer promising results.",
        "url": "http://arxiv.org/pdf/1806.04524v1.pdf"
    },
    {
        "title": "Examining the Use of Neural Networks for Feature Extraction: A Comparative Analysis using Deep Learning, Support Vector Machines, and K-Nearest Neighbor Classifiers",
        "abstract": "Neural networks in many varieties are touted as very powerful machine\nlearning tools because of their ability to distill large amounts of information\nfrom different forms of data, extracting complex features and enabling powerful\nclassification abilities. In this study, we use neural networks to extract\nfeatures from both images and numeric data and use these extracted features as\ninputs for other machine learning models, namely support vector machines (SVMs)\nand k-nearest neighbor classifiers (KNNs), in order to see if\nneural-network-extracted features enhance the capabilities of these models. We\ntested 7 different neural network architectures in this manner, 4 for images\nand 3 for numeric data, training each for varying lengths of time and then\ncomparing the results of the neural network independently to those of an SVM\nand KNN on the data, and finally comparing these results to models of SVM and\nKNN trained using features extracted via the neural network architecture. This\nprocess was repeated on 3 different image datasets and 2 different numeric\ndatasets. The results show that, in many cases, the features extracted using\nthe neural network significantly improve the capabilities of SVMs and KNNs\ncompared to running these algorithms on the raw features, and in some cases\nalso surpass the performance of the neural network alone. This in turn suggests\nthat it may be a reasonable practice to use neural networks as a means to\nextract features for classification by other machine learning models for some\ndatasets.",
        "url": "http://arxiv.org/pdf/1805.02294v2.pdf"
    },
    {
        "title": "Two Use Cases of Machine Learning for SDN-Enabled IP/Optical Networks: Traffic Matrix Prediction and Optical Path Performance Prediction",
        "abstract": "We describe two applications of machine learning in the context of IP/Optical\nnetworks. The first one allows agile management of resources at a core\nIP/Optical network by using machine learning for short-term and long-term\nprediction of traffic flows and joint global optimization of IP and optical\nlayers using colorless/directionless (CD) flexible ROADMs. Multilayer\ncoordination allows for significant cost savings, flexible new services to meet\ndynamic capacity needs, and improved robustness by being able to proactively\nadapt to new traffic patterns and network conditions. The second application is\nimportant as we migrate our metro networks to Open ROADM networks, to allow\nphysical routing without the need for detailed knowledge of optical parameters.\nWe discuss a proof-of-concept study, where detailed performance data for\nwavelengths on a current flexible ROADM network is used for machine learning to\npredict the optical performance of each wavelength. Both applications can be\nefficiently implemented by using a SDN (Software Defined Network) controller.",
        "url": "http://arxiv.org/pdf/1804.07433v2.pdf"
    },
    {
        "title": "Training Medical Image Analysis Systems like Radiologists",
        "abstract": "The training of medical image analysis systems using machine learning\napproaches follows a common script: collect and annotate a large dataset, train\nthe classifier on the training set, and test it on a hold-out test set. This\nprocess bears no direct resemblance with radiologist training, which is based\non solving a series of tasks of increasing difficulty, where each task involves\nthe use of significantly smaller datasets than those used in machine learning.\nIn this paper, we propose a novel training approach inspired by how\nradiologists are trained. In particular, we explore the use of meta-training\nthat models a classifier based on a series of tasks. Tasks are selected using\nteacher-student curriculum learning, where each task consists of simple\nclassification problems containing small training sets. We hypothesize that our\nproposed meta-training approach can be used to pre-train medical image analysis\nmodels. This hypothesis is tested on the automatic breast screening\nclassification from DCE-MRI trained with weakly labeled datasets. The\nclassification performance achieved by our approach is shown to be the best in\nthe field for that application, compared to state of art baseline approaches:\nDenseNet, multiple instance learning and multi-task learning.",
        "url": "http://arxiv.org/pdf/1805.10884v3.pdf"
    },
    {
        "title": "ToxicBlend: Virtual Screening of Toxic Compounds with Ensemble Predictors",
        "abstract": "Timely assessment of compound toxicity is one of the biggest challenges\nfacing the pharmaceutical industry today. A significant proportion of compounds\nidentified as potential leads are ultimately discarded due to the toxicity they\ninduce. In this paper, we propose a novel machine learning approach for the\nprediction of molecular activity on ToxCast targets. We combine extreme\ngradient boosting with fully-connected and graph-convolutional neural network\narchitectures trained on QSAR physical molecular property descriptors, PubChem\nmolecular fingerprints, and SMILES sequences. Our ensemble predictor leverages\nthe strengths of each individual technique, significantly outperforming\nexisting state-of-the art models on the ToxCast and Tox21 toxicity-prediction\ndatasets. We provide free access to molecule toxicity prediction using our\nmodel at http://www.owkin.com/toxicblend.",
        "url": "http://arxiv.org/pdf/1806.04449v1.pdf"
    },
    {
        "title": "ML + FV = $\\heartsuit$? A Survey on the Application of Machine Learning to Formal Verification",
        "abstract": "Formal Verification (FV) and Machine Learning (ML) can seem incompatible due\nto their opposite mathematical foundations and their use in real-life problems:\nFV mostly relies on discrete mathematics and aims at ensuring correctness; ML\noften relies on probabilistic models and consists of learning patterns from\ntraining data. In this paper, we postulate that they are complementary in\npractice, and explore how ML helps FV in its classical approaches: static\nanalysis, model-checking, theorem-proving, and SAT solving. We draw a landscape\nof the current practice and catalog some of the most prominent uses of ML\ninside FV tools, thus offering a new perspective on FV techniques that can help\nresearchers and practitioners to better locate the possible synergies. We\ndiscuss lessons learned from our work, point to possible improvements and offer\nvisions for the future of the domain in the light of the science of software\nand systems modeling.",
        "url": "http://arxiv.org/pdf/1806.03600v2.pdf"
    },
    {
        "title": "Quantum classification of the MNIST dataset with Slow Feature Analysis",
        "abstract": "Quantum machine learning carries the promise to revolutionize information and communication technologies. While a number of quantum algorithms with potential exponential speedups have been proposed already, it is quite difficult to provide convincing evidence that quantum computers with quantum memories will be in fact useful to solve real-world problems. Our work makes considerable progress towards this goal. We design quantum techniques for Dimensionality Reduction and for Classification, and combine them to provide an efficient and high accuracy quantum classifier that we test on the MNIST dataset. More precisely, we propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality reduction technique that maps the dataset in a lower dimensional space where we can apply a novel quantum classification procedure, the Quantum Frobenius Distance (QFD). We simulate the quantum classifier (including errors) and show that it can provide classification of the MNIST handwritten digit dataset, a widely used dataset for benchmarking classification algorithms, with $98.5\\%$ accuracy, similar to the classical case. The running time of the quantum classifier is polylogarithmic in the dimension and number of data points. We also provide evidence that the other parameters on which the running time depends (condition number, Frobenius norm, error threshold, etc.) scale favorably in practice, thus ascertaining the efficiency of our algorithm.",
        "url": "https://arxiv.org/pdf/1805.08837v3.pdf"
    },
    {
        "title": "Support Vector Machine Application for Multiphase Flow Pattern Prediction",
        "abstract": "In this paper a data analytical approach featuring support vector machines\n(SVM) is employed to train a predictive model over an experimentaldataset,\nwhich consists of the most relevant studies for two-phase flow pattern\nprediction. The database for this study consists of flow patterns or flow\nregimes in gas-liquid two-phase flow. The term flow pattern refers to the\ngeometrical configuration of the gas and liquid phases in the pipe. When gas\nand liquid flow simultaneously in a pipe, the two phases can distribute\nthemselves in a variety of flow configurations. Gas-liquid two-phase flow\noccurs ubiquitously in various major industrial fields: petroleum, chemical,\nnuclear, and geothermal industries. The flow configurations differ from each\nother in the spatial distribution of the interface, resulting in different flow\ncharacteristics. Experimental results obtained by applying the presented\nmethodology to different combinations of flow patterns demonstrate that the\nproposed approach is state-of-the-art alternatives by achieving 97% correct\nclassification. The results suggest machine learning could be used as an\neffective tool for automatic detection and classification of gas-liquid flow\npatterns.",
        "url": "http://arxiv.org/pdf/1806.05054v1.pdf"
    },
    {
        "title": "MISSION: Ultra Large-Scale Feature Selection using Count-Sketches",
        "abstract": "Feature selection is an important challenge in machine learning. It plays a\ncrucial role in the explainability of machine-driven decisions that are rapidly\npermeating throughout modern society. Unfortunately, the explosion in the size\nand dimensionality of real-world datasets poses a severe challenge to standard\nfeature selection algorithms. Today, it is not uncommon for datasets to have\nbillions of dimensions. At such scale, even storing the feature vector is\nimpossible, causing most existing feature selection methods to fail.\nWorkarounds like feature hashing, a standard approach to large-scale machine\nlearning, helps with the computational feasibility, but at the cost of losing\nthe interpretability of features. In this paper, we present MISSION, a novel\nframework for ultra large-scale feature selection that performs stochastic\ngradient descent while maintaining an efficient representation of the features\nin memory using a Count-Sketch data structure. MISSION retains the simplicity\nof feature hashing without sacrificing the interpretability of the features\nwhile using only O(log^2(p)) working memory. We demonstrate that MISSION\naccurately and efficiently performs feature selection on real-world,\nlarge-scale datasets with billions of dimensions.",
        "url": "http://arxiv.org/pdf/1806.04310v1.pdf"
    },
    {
        "title": "Model-Free Information Extraction in Enriched Nonlinear Phase-Space",
        "abstract": "Detecting anomalies and discovering driving signals is an essential component\nof scientific research and industrial practice. Often the underlying mechanism\nis highly complex, involving hidden evolving nonlinear dynamics and noise\ncontamination. When representative physical models and large labeled data sets\nare unavailable, as is the case with most real-world applications,\nmodel-dependent Bayesian approaches would yield misleading results, and most\nsupervised learning machines would also fail to reliably resolve the\nintricately evolving systems. Here, we propose an unsupervised machine-learning\napproach that operates in a well-constructed function space, whereby the\nevolving nonlinear dynamics are captured through a linear functional\nrepresentation determined by the Koopman operator. This breakthrough leverages\non the time-feature embedding and the ensuing reconstruction of a phase-space\nrepresentation of the dynamics, thereby permitting the reliable identification\nof critical global signatures from the whole trajectory. This dramatically\nimproves over commonly used static local features, which are vulnerable to\nunknown transitions or noise. Thanks to its data-driven nature, our method\nexcludes any prior models and training corpus. We benchmark the astonishing\naccuracy of our method on three diverse and challenging problems in: biology,\nmedicine, and engineering. In all cases, it outperforms existing\nstate-of-the-art methods. As a new unsupervised information processing\nparadigm, it is suitable for ubiquitous nonlinear dynamical systems or\nend-users with little expertise, which permits an unbiased excavation of\nunderlying working principles or intrinsic correlations submerged in unlabeled\ndata flows.",
        "url": "http://arxiv.org/pdf/1804.05170v2.pdf"
    },
    {
        "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\u0141ojasiewicz Condition",
        "abstract": "In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the \\L{}ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-\\L{}ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of randomized and greedy coordinate descent methods, sign-based gradient descent methods, and stochastic gradient methods in the classic setting (with decreasing or constant step-sizes) as well as the variance-reduced setting. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence of these methods. Along the way, we give simple convergence results for a wide variety of problems in machine learning: least squares, logistic regression, boosting, resilient backpropagation, L1-regularization, support vector machines, stochastic dual coordinate ascent, and stochastic variance-reduced gradient methods.",
        "url": "https://arxiv.org/pdf/1608.04636v4.pdf"
    },
    {
        "title": "Can machine learning identify interesting mathematics? An exploration using empirically observed laws",
        "abstract": "We explore the possibility of using machine learning to identify interesting\nmathematical structures by using certain quantities that serve as fingerprints.\nIn particular, we extract features from integer sequences using two empirical\nlaws: Benford's law and Taylor's law and experiment with various classifiers to\nidentify whether a sequence is, for example, nice, important, multiplicative,\neasy to compute or related to primes or palindromes.",
        "url": "http://arxiv.org/pdf/1805.07431v3.pdf"
    },
    {
        "title": "How Curiosity can be modeled for a Clickbait Detector",
        "abstract": "The impact of continually evolving digital technologies and the proliferation\nof communications and content has now been widely acknowledged to be central to\nunderstanding our world. What is less acknowledged is that this is based on the\nsuccessful arousing of curiosity both at the collective and individual levels.\nAdvertisers, communication professionals and news editors are in constant\ncompetition to capture attention of the digital population perennially shifty\nand distracted. This paper, tries to understand how curiosity works in the\ndigital world by attempting the first ever work done on quantifying human\ncuriosity, basing itself on various theories drawn from humanities and social\nsciences. Curious communication pushes people to spot, read and click the\nmessage from their social feed or any other form of online presentation. Our\napproach focuses on measuring the strength of the stimulus to generate reader\ncuriosity by using unsupervised and supervised machine learning algorithms, but\nis also informed by philosophical, psychological, neural and cognitive studies\non this topic. Manually annotated news headlines - clickbaits - have been\nselected for the study, which are known to have drawn huge reader response. A\nbinary classifier was developed based on human curiosity (unlike the work done\nso far using words and other linguistic features). Our classifier shows an\naccuracy of 97% . This work is part of the research in computational humanities\non digital politics quantifying the emotions of curiosity and outrage on\ndigital media.",
        "url": "http://arxiv.org/pdf/1806.04212v1.pdf"
    },
    {
        "title": "Baselines and a datasheet for the Cerema AWP dataset",
        "abstract": "This paper presents the recently published Cerema AWP (Adverse Weather\nPedestrian) dataset for various machine learning tasks and its exports in\nmachine learning friendly format. We explain why this dataset can be\ninteresting (mainly because it is a greatly controlled and fully annotated\nimage dataset) and present baseline results for various tasks. Moreover, we\ndecided to follow the very recent suggestions of datasheets for dataset, trying\nto standardize all the available information of the dataset, with a\ntransparency objective.",
        "url": "http://arxiv.org/pdf/1806.04016v1.pdf"
    },
    {
        "title": "Second-Order Asymptotically Optimal Statistical Classification",
        "abstract": "Motivated by real-world machine learning applications, we analyze\napproximations to the non-asymptotic fundamental limits of statistical\nclassification. In the binary version of this problem, given two training\nsequences generated according to two {\\em unknown} distributions $P_1$ and\n$P_2$, one is tasked to classify a test sequence which is known to be generated\naccording to either $P_1$ or $P_2$. This problem can be thought of as an\nanalogue of the binary hypothesis testing problem but in the present setting,\nthe generating distributions are unknown. Due to finite sample considerations,\nwe consider the second-order asymptotics (or dispersion-type) tradeoff between\ntype-I and type-II error probabilities for tests which ensure that (i) the\ntype-I error probability for {\\em all} pairs of distributions decays\nexponentially fast and (ii) the type-II error probability for a {\\em\nparticular} pair of distributions is non-vanishing. We generalize our results\nto classification of multiple hypotheses with the rejection option.",
        "url": "http://arxiv.org/pdf/1806.00739v3.pdf"
    },
    {
        "title": "Low-Rank Inducing Norms with Optimality Interpretations",
        "abstract": "Optimization problems with rank constraints appear in many diverse fields\nsuch as control, machine learning and image analysis. Since the rank constraint\nis non-convex, these problems are often approximately solved via convex\nrelaxations. Nuclear norm regularization is the prevailing convexifying\ntechnique for dealing with these types of problem. This paper introduces a\nfamily of low-rank inducing norms and regularizers which includes the nuclear\nnorm as a special case. A posteriori guarantees on solving an underlying rank\nconstrained optimization problem with these convex relaxations are provided. We\nevaluate the performance of the low-rank inducing norms on three matrix\ncompletion problems. In all examples, the nuclear norm heuristic is\noutperformed by convex relaxations based on other low-rank inducing norms. For\ntwo of the problems there exist low-rank inducing norms that succeed in\nrecovering the partially unknown matrix, while the nuclear norm fails. These\nlow-rank inducing norms are shown to be representable as semi-definite\nprograms. Moreover, these norms have cheaply computable proximal mappings,\nwhich makes it possible to also solve problems of large size using first-order\nmethods.",
        "url": "http://arxiv.org/pdf/1612.03186v2.pdf"
    },
    {
        "title": "Multi-task learning of daily work and study round-trips from survey data",
        "abstract": "In this study, we present a machine learning approach to infer the worker and\nstudent mobility flows on daily basis from static censuses. The rapid\nurbanization has made the estimation of the human mobility flows a critical\ntask for transportation and urban planners. The primary objective of this paper\nis to complete individuals' census data with working and studying trips,\nallowing its merging with other mobility data to better estimate the complete\norigin-destination matrices. Worker and student mobility flows are among the\nmost weekly regular displacements and consequently generate road congestion\nproblems. Estimating their round-trips eases the decision-making processes for\nlocal authorities. Worker and student censuses often contain home location,\nwork places and educational institutions. We thus propose a neural network\nmodel that learns the temporal distribution of displacements from other\nmobility sources and tries to predict them on new censuses data. The inclusion\nof multi-task learning in our neural network results in a significant error\nrate control in comparison to single task learning.",
        "url": "http://arxiv.org/pdf/1806.03903v1.pdf"
    },
    {
        "title": "Deep Learning for Classification Tasks on Geospatial Vector Polygons",
        "abstract": "In this paper, we evaluate the accuracy of deep learning approaches on geospatial vector geometry classification tasks. The purpose of this evaluation is to investigate the ability of deep learning models to learn from geometry coordinates directly. Previous machine learning research applied to geospatial polygon data did not use geometries directly, but derived properties thereof. These are produced by way of extracting geometry properties such as Fourier descriptors. Instead, our introduced deep neural net architectures are able to learn on sequences of coordinates mapped directly from polygons. In three classification tasks we show that the deep learning architectures are competitive with common learning algorithms that require extracted features.",
        "url": "https://arxiv.org/pdf/1806.03857v2.pdf"
    },
    {
        "title": "Data augmentation instead of explicit regularization",
        "abstract": "Contrary to most machine learning models, modern deep artificial neural networks typically include multiple components that contribute to regularization. Despite the fact that some (explicit) regularization techniques, such as weight decay and dropout, require costly fine-tuning of sensitive hyperparameters, the interplay between them and other elements that provide implicit regularization is not well understood yet. Shedding light upon these interactions is key to efficiently using computational resources and may contribute to solving the puzzle of generalization in deep learning. Here, we first provide formal definitions of explicit and implicit regularization that help understand essential differences between techniques. Second, we contrast data augmentation with weight decay and dropout. Our results show that visual object categorization models trained with data augmentation alone achieve the same performance or higher than models trained also with weight decay and dropout, as is common practice. We conclude that the contribution on generalization of weight decay and dropout is not only superfluous when sufficient implicit regularization is provided, but also such techniques can dramatically deteriorate the performance if the hyperparameters are not carefully tuned for the architecture and data set. In contrast, data augmentation systematically provides large generalization gains and does not require hyperparameter re-tuning. In view of our results, we suggest to optimize neural networks without weight decay and dropout to save computational resources, hence carbon emissions, and focus more on data augmentation and other inductive biases to improve performance and robustness.",
        "url": "https://arxiv.org/pdf/1806.03852v5.pdf"
    },
    {
        "title": "Adaptive MCMC via Combining Local Samplers",
        "abstract": "Markov chain Monte Carlo (MCMC) methods are widely used in machine learning. One of the major problems with MCMC is the question of how to design chains that mix fast over the whole state space; in particular, how to select the parameters of an MCMC algorithm. Here we take a different approach and, similarly to parallel MCMC methods, instead of trying to find a single chain that samples from the whole distribution, we combine samples from several chains run in parallel, each exploring only parts of the state space (e.g., a few modes only). The chains are prioritized based on kernel Stein discrepancy, which provides a good measure of performance locally. The samples from the independent chains are combined using a novel technique for estimating the probability of different regions of the sample space. Experimental results demonstrate that the proposed algorithm may provide significant speedups in different sampling problems. Most importantly, when combined with the state-of-the-art NUTS algorithm as the base MCMC sampler, our method remained competitive with NUTS on sampling from unimodal distributions, while significantly outperforming state-of-the-art competitors on synthetic multimodal problems as well as on a challenging sensor localization task.",
        "url": "https://arxiv.org/pdf/1806.03816v6.pdf"
    },
    {
        "title": "P\u00f3lya Urn Latent Dirichlet Allocation: a doubly sparse massively parallel sampler",
        "abstract": "Latent Dirichlet Allocation (LDA) is a topic model widely used in natural language processing and machine learning. Most approaches to training the model rely on iterative algorithms, which makes it difficult to run LDA on big corpora that are best analyzed in parallel and distributed computational environments. Indeed, current approaches to parallel inference either don't converge to the correct posterior or require storage of large dense matrices in memory. We present a novel sampler that overcomes both problems, and we show that this sampler is faster, both empirically and theoretically, than previous Gibbs samplers for LDA. We do so by employing a novel P\\'olya-urn-based approximation in the sparse partially collapsed sampler for LDA. We prove that the approximation error vanishes with data size, making our algorithm asymptotically exact, a property of importance for large-scale topic models. In addition, we show, via an explicit example, that - contrary to popular belief in the topic modeling literature - partially collapsed samplers can be more efficient than fully collapsed samplers. We conclude by comparing the performance of our algorithm with that of other approaches on well-known corpora.",
        "url": "https://arxiv.org/pdf/1704.03581v7.pdf"
    },
    {
        "title": "LexNLP: Natural language processing and information extraction for legal and regulatory texts",
        "abstract": "LexNLP is an open source Python package focused on natural language\nprocessing and machine learning for legal and regulatory text. The package\nincludes functionality to (i) segment documents, (ii) identify key text such as\ntitles and section headings, (iii) extract over eighteen types of structured\ninformation like distances and dates, (iv) extract named entities such as\ncompanies and geopolitical entities, (v) transform text into features for model\ntraining, and (vi) build unsupervised and supervised models such as word\nembedding or tagging models. LexNLP includes pre-trained models based on\nthousands of unit tests drawn from real documents available from the SEC EDGAR\ndatabase as well as various judicial and regulatory proceedings. LexNLP is\ndesigned for use in both academic research and industrial applications, and is\ndistributed at https://github.com/LexPredict/lexpredict-lexnlp.",
        "url": "http://arxiv.org/pdf/1806.03688v1.pdf"
    },
    {
        "title": "Neural Architecture Search with Bayesian Optimisation and Optimal Transport",
        "abstract": "Bayesian Optimisation (BO) refers to a class of methods for global\noptimisation of a function $f$ which is only accessible via point evaluations.\nIt is typically used in settings where $f$ is expensive to evaluate. A common\nuse case for BO in machine learning is model selection, where it is not\npossible to analytically model the generalisation performance of a statistical\nmodel, and we resort to noisy and expensive training and validation procedures\nto choose the best model. Conventional BO methods have focused on Euclidean and\ncategorical domains, which, in the context of model selection, only permits\ntuning scalar hyper-parameters of machine learning algorithms. However, with\nthe surge of interest in deep learning, there is an increasing demand to tune\nneural network \\emph{architectures}. In this work, we develop NASBOT, a\nGaussian process based BO framework for neural architecture search. To\naccomplish this, we develop a distance metric in the space of neural network\narchitectures which can be computed efficiently via an optimal transport\nprogram. This distance might be of independent interest to the deep learning\ncommunity as it may find applications outside of BO. We demonstrate that NASBOT\noutperforms other alternatives for architecture search in several cross\nvalidation based model selection tasks on multi-layer perceptrons and\nconvolutional neural networks.",
        "url": "http://arxiv.org/pdf/1802.07191v3.pdf"
    },
    {
        "title": "Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization",
        "abstract": "Asynchronous momentum stochastic gradient descent algorithms (Async-MSGD) is one of the most popular algorithms in distributed machine learning. However, its convergence properties for these complicated nonconvex problems is still largely unknown, because of the current technical limit. Therefore, in this paper, we propose to analyze the algorithm through a simpler but nontrivial nonconvex problem - streaming PCA, which helps us to understand Aync-MSGD better even for more general problems. Specifically, we establish the asymptotic rate of convergence of Async-MSGD for streaming PCA by diffusion approximation. Our results indicate a fundamental tradeoff between asynchrony and momentum: To ensure convergence and acceleration through asynchrony, we have to reduce the momentum (compared with Sync-MSGD). To the best of our knowledge, this is the first theoretical attempt on understanding Async-MSGD for distributed nonconvex stochastic optimization. Numerical experiments on both streaming PCA and training deep neural networks are provided to support our findings for Async-MSGD.",
        "url": "https://arxiv.org/pdf/1806.01660v6.pdf"
    },
    {
        "title": "Capacity Releasing Diffusion for Speed and Locality",
        "abstract": "Diffusions and related random walk procedures are of central importance in\nmany areas of machine learning, data analysis, and applied mathematics. Because\nthey spread mass agnostically at each step in an iterative manner, they can\nsometimes spread mass \"too aggressively,\" thereby failing to find the \"right\"\nclusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process,\nwhich is both faster and stays more local than the classical spectral diffusion\nprocess. As an application, we use our CRD Process to develop an improved local\nalgorithm for graph clustering. Our local graph clustering method can find\nlocal clusters in a model of clustering where one begins the CRD Process in a\ncluster whose vertices are connected better internally than externally by an\n$O(\\log^2 n)$ factor, where $n$ is the number of nodes in the cluster. Thus,\nour CRD Process is the first local graph clustering algorithm that is not\nsubject to the well-known quadratic Cheeger barrier. Our result requires a\ncertain smoothness condition, which we expect to be an artifact of our\nanalysis. Our empirical evaluation demonstrates improved results, in particular\nfor realistic social graphs where there are moderately good---but not very\ngood---clusters.",
        "url": "http://arxiv.org/pdf/1706.05826v2.pdf"
    },
    {
        "title": "Genesis of Basic and Multi-Layer Echo State Network Recurrent Autoencoders for Efficient Data Representations",
        "abstract": "It is a widely accepted fact that data representations intervene noticeably\nin machine learning tools. The more they are well defined the better the\nperformance results are. Feature extraction-based methods such as autoencoders\nare conceived for finding more accurate data representations from the original\nones. They efficiently perform on a specific task in terms of 1) high accuracy,\n2) large short term memory and 3) low execution time. Echo State Network (ESN)\nis a recent specific kind of Recurrent Neural Network which presents very rich\ndynamics thanks to its reservoir-based hidden layer. It is widely used in\ndealing with complex non-linear problems and it has outperformed classical\napproaches in a number of tasks including regression, classification, etc. In\nthis paper, the noticeable dynamism and the large memory provided by ESN and\nthe strength of Autoencoders in feature extraction are gathered within an ESN\nRecurrent Autoencoder (ESN-RAE). In order to bring up sturdier alternative to\nconventional reservoir-based networks, not only single layer basic ESN is used\nas an autoencoder, but also Multi-Layer ESN (ML-ESN-RAE). The new features,\nonce extracted from ESN's hidden layer, are applied to classification tasks.\nThe classification rates rise considerably compared to those obtained when\napplying the original data features. An accuracy-based comparison is performed\nbetween the proposed recurrent AEs and two variants of an ELM feed-forward AEs\n(Basic and ML) in both of noise free and noisy environments. The empirical\nstudy reveals the main contribution of recurrent connections in improving the\nclassification performance results.",
        "url": "http://arxiv.org/pdf/1804.08996v2.pdf"
    },
    {
        "title": "A Taxonomy of Network Threats and the Effect of Current Datasets on Intrusion Detection Systems",
        "abstract": "As the world moves towards being increasingly dependent on computers and automation, building secure applications, systems and networks are some of the main challenges faced in the current decade. The number of threats that individuals and businesses face is rising exponentially due to the increasing complexity of networks and services of modern networks. To alleviate the impact of these threats, researchers have proposed numerous solutions for anomaly detection; however, current tools often fail to adapt to ever-changing architectures, associated threats and zero-day attacks. This manuscript aims to pinpoint research gaps and shortcomings of current datasets, their impact on building Network Intrusion Detection Systems (NIDS) and the growing number of sophisticated threats. To this end, this manuscript provides researchers with two key pieces of information; a survey of prominent datasets, analyzing their use and impact on the development of the past decade's Intrusion Detection Systems (IDS) and a taxonomy of network threats and associated tools to carry out these attacks. The manuscript highlights that current IDS research covers only 33.3% of our threat taxonomy. Current datasets demonstrate a clear lack of real-network threats, attack representation and include a large number of deprecated threats, which together limit the detection accuracy of current machine learning IDS approaches. The unique combination of the taxonomy and the analysis of the datasets provided in this manuscript aims to improve the creation of datasets and the collection of real-world data. As a result, this will improve the efficiency of the next generation IDS and reflect network threats more accurately within new datasets.",
        "url": "https://arxiv.org/pdf/1806.03517v2.pdf"
    },
    {
        "title": "TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service",
        "abstract": "Machine learning methods are widely used for a variety of prediction\nproblems. \\emph{Prediction as a service} is a paradigm in which service\nproviders with technological expertise and computational resources may perform\npredictions for clients. However, data privacy severely restricts the\napplicability of such services, unless measures to keep client data private\n(even from the service provider) are designed. Equally important is to minimize\nthe amount of computation and communication required between client and server.\nFully homomorphic encryption offers a possible way out, whereby clients may\nencrypt their data, and on which the server may perform arithmetic\ncomputations. The main drawback of using fully homomorphic encryption is the\namount of time required to evaluate large machine learning models on encrypted\ndata. We combine ideas from the machine learning literature, particularly work\non binarization and sparsification of neural networks, together with\nalgorithmic tools to speed-up and parallelize computation using encrypted data.",
        "url": "http://arxiv.org/pdf/1806.03461v1.pdf"
    },
    {
        "title": "A hybrid econometric-machine learning approach for relative importance analysis: Prioritizing food policy",
        "abstract": "A measure of relative importance of variables is often desired by researchers when the explanatory aspects of econometric methods are of interest. To this end, the author briefly reviews the limitations of conventional econometrics in constructing a reliable measure of variable importance. The author highlights the relative stature of explanatory and predictive analysis in economics and the emergence of fruitful collaborations between econometrics and computer science. Learning lessons from both, the author proposes a hybrid approach based on conventional econometrics and advanced machine learning (ML) algorithms, which are otherwise, used in predictive analytics. The purpose of this article is two-fold, to propose a hybrid approach to assess relative importance and demonstrate its applicability in addressing policy priority issues with an example of food inflation in India, followed by a broader aim to introduce the possibility of conflation of ML and conventional econometrics to an audience of researchers in economics and social sciences, in general.",
        "url": "https://arxiv.org/pdf/1806.04517v3.pdf"
    },
    {
        "title": "Hierarchical Clustering with Prior Knowledge",
        "abstract": "Hierarchical clustering is a class of algorithms that seeks to build a\nhierarchy of clusters. It has been the dominant approach to constructing\nembedded classification schemes since it outputs dendrograms, which capture the\nhierarchical relationship among members at all levels of granularity,\nsimultaneously. Being greedy in the algorithmic sense, a hierarchical\nclustering partitions data at every step solely based on a similarity /\ndissimilarity measure. The clustering results oftentimes depend on not only the\ndistribution of the underlying data, but also the choice of dissimilarity\nmeasure and the clustering algorithm. In this paper, we propose a method to\nincorporate prior domain knowledge about entity relationship into the\nhierarchical clustering. Specifically, we use a distance function in\nultrametric space to encode the external ontological information. We show that\npopular linkage-based algorithms can faithfully recover the encoded structure.\nSimilar to some regularized machine learning techniques, we add this distance\nas a penalty term to the original pairwise distance to regulate the final\nstructure of the dendrogram. As a case study, we applied this method on real\ndata in the building of a customer behavior based product taxonomy for an\nAmazon service, leveraging the information from a larger Amazon-wide browse\nstructure. The method is useful when one wants to leverage the relational\ninformation from external sources, or the data used to generate the distance\nmatrix is noisy and sparse. Our work falls in the category of semi-supervised\nor constrained clustering.",
        "url": "http://arxiv.org/pdf/1806.03432v3.pdf"
    },
    {
        "title": "Efficient Optimization Algorithms for Robust Principal Component Analysis and Its Variants",
        "abstract": "Robust PCA has drawn significant attention in the last decade due to its\nsuccess in numerous application domains, ranging from bio-informatics,\nstatistics, and machine learning to image and video processing in computer\nvision. Robust PCA and its variants such as sparse PCA and stable PCA can be\nformulated as optimization problems with exploitable special structures. Many\nspecialized efficient optimization methods have been proposed to solve robust\nPCA and related problems. In this paper we review existing optimization methods\nfor solving convex and nonconvex relaxations/variants of robust PCA, discuss\ntheir advantages and disadvantages, and elaborate on their convergence\nbehaviors. We also provide some insights for possible future research\ndirections including new algorithmic frameworks that might be suitable for\nimplementing on multi-processor setting to handle large-scale problems.",
        "url": "http://arxiv.org/pdf/1806.03430v1.pdf"
    },
    {
        "title": "Going Deeper in Spiking Neural Networks: VGG and Residual Architectures",
        "abstract": "Over the past few years, Spiking Neural Networks (SNNs) have become popular\nas a possible pathway to enable low-power event-driven neuromorphic hardware.\nHowever, their application in machine learning have largely been limited to\nvery shallow neural network architectures for simple problems. In this paper,\nwe propose a novel algorithmic technique for generating an SNN with a deep\narchitecture, and demonstrate its effectiveness on complex visual recognition\nproblems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and\nResidual network architectures, with significantly better accuracy than the\nstate-of-the-art. Finally, we present analysis of the sparse event-driven\ncomputations to demonstrate reduced hardware overhead when operating in the\nspiking domain.",
        "url": "http://arxiv.org/pdf/1802.02627v4.pdf"
    },
    {
        "title": "Discovering Signals from Web Sources to Predict Cyber Attacks",
        "abstract": "Cyber attacks are growing in frequency and severity. Over the past year alone\nwe have witnessed massive data breaches that stole personal information of\nmillions of people and wide-scale ransomware attacks that paralyzed critical\ninfrastructure of several countries. Combating the rising cyber threat calls\nfor a multi-pronged strategy, which includes predicting when these attacks will\noccur. The intuition driving our approach is this: during the planning and\npreparation stages, hackers leave digital traces of their activities on both\nthe surface web and dark web in the form of discussions on platforms like\nhacker forums, social media, blogs and the like. These data provide predictive\nsignals that allow anticipating cyber attacks. In this paper, we describe\nmachine learning techniques based on deep neural networks and autoregressive\ntime series models that leverage external signals from publicly available Web\nsources to forecast cyber attacks. Performance of our framework across ground\ntruth data over real-world forecasting tasks shows that our methods yield a\nsignificant lift or increase of F1 for the top signals on predicted cyber\nattacks. Our results suggest that, when deployed, our system will be able to\nprovide an effective line of defense against various types of targeted cyber\nattacks.",
        "url": "http://arxiv.org/pdf/1806.03342v1.pdf"
    },
    {
        "title": "Stein Points",
        "abstract": "An important task in computational statistics and machine learning is to\napproximate a posterior distribution $p(x)$ with an empirical measure supported\non a set of representative points $\\{x_i\\}_{i=1}^n$. This paper focuses on\nmethods where the selection of points is essentially deterministic, with an\nemphasis on achieving accurate approximation when $n$ is small. To this end, we\npresent `Stein Points'. The idea is to exploit either a greedy or a conditional\ngradient method to iteratively minimise a kernel Stein discrepancy between the\nempirical measure and $p(x)$. Our empirical results demonstrate that Stein\nPoints enable accurate approximation of the posterior at modest computational\ncost. In addition, theoretical results are provided to establish convergence of\nthe method.",
        "url": "http://arxiv.org/pdf/1803.10161v4.pdf"
    },
    {
        "title": "Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware",
        "abstract": "As Machine Learning (ML) gets applied to security-critical or sensitive\ndomains, there is a growing need for integrity and privacy for outsourced ML\ncomputations. A pragmatic solution comes from Trusted Execution Environments\n(TEEs), which use hardware and software protections to isolate sensitive\ncomputations from the untrusted software stack. However, these isolation\nguarantees come at a price in performance, compared to untrusted alternatives.\nThis paper initiates the study of high performance execution of Deep Neural\nNetworks (DNNs) in TEEs by efficiently partitioning DNN computations between\ntrusted and untrusted devices. Building upon an efficient outsourcing scheme\nfor matrix multiplication, we propose Slalom, a framework that securely\ndelegates execution of all linear layers in a DNN from a TEE (e.g., Intel SGX\nor Sanctum) to a faster, yet untrusted, co-located processor. We evaluate\nSlalom by running DNNs in an Intel SGX enclave, which selectively delegates\nwork to an untrusted GPU. For canonical DNNs (VGG16, MobileNet and ResNet\nvariants) we obtain 6x to 20x increases in throughput for verifiable inference,\nand 4x to 11x for verifiable and private inference.",
        "url": "http://arxiv.org/pdf/1806.03287v2.pdf"
    },
    {
        "title": "Blind Justice: Fairness with Encrypted Sensitive Attributes",
        "abstract": "Recent work has explored how to train machine learning models which do not\ndiscriminate against any subgroup of the population as determined by sensitive\nattributes such as gender or race. To avoid disparate treatment, sensitive\nattributes should not be considered. On the other hand, in order to avoid\ndisparate impact, sensitive attributes must be examined, e.g., in order to\nlearn a fair model, or to check if a given model is fair. We introduce methods\nfrom secure multi-party computation which allow us to avoid both. By encrypting\nsensitive attributes, we show how an outcome-based fair model may be learned,\nchecked, or have its outputs verified and held to account, without users\nrevealing their sensitive attributes.",
        "url": "http://arxiv.org/pdf/1806.03281v1.pdf"
    },
    {
        "title": "Data-driven model for the identification of the rock type at a drilling bit",
        "abstract": "Directional oil well drilling requires high precision of the wellbore\npositioning inside the productive area. However, due to specifics of\nengineering design, sensors that explicitly determine the type of the drilled\nrock are located farther than 15m from the drilling bit. As a result, the\ntarget area runaways can be detected only after this distance, which in turn,\nleads to a loss in well productivity and the risk of the need for an expensive\nre-boring operation.\n  We present a novel approach for identifying rock type at the drilling bit\nbased on machine learning classification methods and data mining on sensors\nreadings. We compare various machine-learning algorithms, examine extra\nfeatures coming from mathematical modeling of drilling mechanics, and show that\nthe real-time rock type classification error can be reduced from 13.5 % to 9 %.\nThe approach is applicable for precise directional drilling in relatively thin\ntarget intervals of complex shapes and generalizes appropriately to new wells\nthat are different from the ones used for training the machine learning model.",
        "url": "http://arxiv.org/pdf/1806.03218v3.pdf"
    },
    {
        "title": "Machine Learning CICY Threefolds",
        "abstract": "The latest techniques from Neural Networks and Support Vector Machines (SVM)\nare used to investigate geometric properties of Complete Intersection\nCalabi-Yau (CICY) threefolds, a class of manifolds that facilitate string model\nbuilding. An advanced neural network classifier and SVM are employed to (1)\nlearn Hodge numbers and report a remarkable improvement over previous efforts,\n(2) query for favourability, and (3) predict discrete symmetries, a highly\nimbalanced problem to which both Synthetic Minority Oversampling Technique\n(SMOTE) and permutations of the CICY matrix are used to decrease the class\nimbalance and improve performance. In each case study, we employ a genetic\nalgorithm to optimise the hyperparameters of the neural network. We demonstrate\nthat our approach provides quick diagnostic tools capable of shortlisting\nquasi-realistic string models based on compactification over smooth CICYs and\nfurther supports the paradigm that classes of problems in algebraic geometry\ncan be machine learned.",
        "url": "http://arxiv.org/pdf/1806.03121v3.pdf"
    },
    {
        "title": "Rotation Equivariant CNNs for Digital Pathology",
        "abstract": "We propose a new model for digital pathology segmentation, based on the\nobservation that histopathology images are inherently symmetric under rotation\nand reflection. Utilizing recent findings on rotation equivariant CNNs, the\nproposed model leverages these symmetries in a principled manner. We present a\nvisual analysis showing improved stability on predictions, and demonstrate that\nexploiting rotation equivariance significantly improves tumor detection\nperformance on a challenging lymph node metastases dataset. We further present\na novel derived dataset to enable principled comparison of machine learning\nmodels, in combination with an initial benchmark. Through this dataset, the\ntask of histopathology diagnosis becomes accessible as a challenging benchmark\nfor fundamental machine learning research.",
        "url": "http://arxiv.org/pdf/1806.03962v1.pdf"
    },
    {
        "title": "Machine learning-based colon deformation estimation method for colonoscope tracking",
        "abstract": "This paper presents a colon deformation estimation method, which can be used\nto estimate colon deformations during colonoscope insertions. Colonoscope\ntracking or navigation system that navigates a physician to polyp positions\nduring a colonoscope insertion is required to reduce complications such as\ncolon perforation. A previous colonoscope tracking method obtains a colonoscope\nposition in the colon by registering a colonoscope shape and a colon shape. The\ncolonoscope shape is obtained using an electromagnetic sensor, and the colon\nshape is obtained from a CT volume. However, large tracking errors were\nobserved due to colon deformations occurred during colonoscope insertions. Such\ndeformations make the registration difficult. Because the colon deformation is\ncaused by a colonoscope, there is a strong relationship between the colon\ndeformation and the colonoscope shape. An estimation method of colon\ndeformations occur during colonoscope insertions is necessary to reduce\ntracking errors. We propose a colon deformation estimation method. This method\nis used to estimate a deformed colon shape from a colonoscope shape. We use the\nregression forests algorithm to estimate a deformed colon shape. The regression\nforests algorithm is trained using pairs of colon and colonoscope shapes, which\ncontains deformations occur during colonoscope insertions. As a preliminary\nstudy, we utilized the method to estimate deformations of a colon phantom. In\nour experiments, the proposed method correctly estimated deformed colon phantom\nshapes.",
        "url": "http://arxiv.org/pdf/1806.03014v1.pdf"
    },
    {
        "title": "Domain Adaptive Generation of Aircraft on Satellite Imagery via Simulated and Unsupervised Learning",
        "abstract": "Object detection and classification for aircraft are the most important tasks\nin the satellite image analysis. The success of modern detection and\nclassification methods has been based on machine learning and deep learning.\nOne of the key requirements for those learning processes is huge data to train.\nHowever, there is an insufficient portion of aircraft since the targets are on\nmilitary action and oper- ation. Considering the characteristics of satellite\nimagery, this paper attempts to provide a framework of the simulated and\nunsupervised methodology without any additional su- pervision or physical\nassumptions. Finally, the qualitative and quantitative analysis revealed a\npotential to replenish insufficient data for machine learning platform for\nsatellite image analysis.",
        "url": "http://arxiv.org/pdf/1806.03002v1.pdf"
    },
    {
        "title": "q-Space Novelty Detection with Variational Autoencoders",
        "abstract": "In machine learning, novelty detection is the task of identifying novel\nunseen data. During training, only samples from the normal class are available.\nTest samples are classified as normal or abnormal by assignment of a novelty\nscore. Here we propose novelty detection methods based on training variational\nautoencoders (VAEs) on normal data. Since abnormal samples are not used during\ntraining, we define novelty metrics based on the (partially complementary)\nassumptions that the VAE is less capable of reconstructing abnormal samples\nwell; that abnormal samples more strongly violate the VAE regularizer; and that\nabnormal samples differ from normal samples not only in input-feature space,\nbut also in the VAE latent space and VAE output. These approaches, combined\nwith various possibilities of using (e.g. sampling) the probabilistic VAE to\nobtain scalar novelty scores, yield a large family of methods. We apply these\nmethods to magnetic resonance imaging, namely to the detection of\ndiffusion-space (q-space) abnormalities in diffusion MRI scans of multiple\nsclerosis patients, i.e. to detect multiple sclerosis lesions without using any\nlesion labels for training. Many of our methods outperform previously proposed\nq-space novelty detection methods. We also evaluate the proposed methods on the\nMNIST handwritten digits dataset and show that many of them are able to\noutperform the state of the art.",
        "url": "http://arxiv.org/pdf/1806.02997v2.pdf"
    },
    {
        "title": "SGD and Hogwild! Convergence Without the Bounded Gradients Assumption",
        "abstract": "Stochastic gradient descent (SGD) is the optimization algorithm of choice in\nmany machine learning applications such as regularized empirical risk\nminimization and training deep neural networks. The classical convergence\nanalysis of SGD is carried out under the assumption that the norm of the\nstochastic gradient is uniformly bounded. While this might hold for some loss\nfunctions, it is always violated for cases where the objective function is\nstrongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD\nis performed under the assumption that stochastic gradients are bounded with\nrespect to the true gradient norm. Here we show that for stochastic problems\narising in machine learning such bound always holds; and we also propose an\nalternative convergence analysis of SGD with diminishing learning rate regime,\nwhich results in more relaxed conditions than those in (Bottou et al.,2016). We\nthen move on the asynchronous parallel setting, and prove convergence of\nHogwild! algorithm in the same regime, obtaining the first convergence results\nfor this method in the case of diminished learning rate.",
        "url": "http://arxiv.org/pdf/1802.03801v2.pdf"
    },
    {
        "title": "Efficient Full-Matrix Adaptive Regularization",
        "abstract": "Adaptive regularization methods pre-multiply a descent direction by a preconditioning matrix. Due to the large number of parameters of machine learning problems, full-matrix preconditioning methods are prohibitively expensive. We show how to modify full-matrix adaptive regularization in order to make it practical and effective. We also provide a novel theoretical analysis for adaptive regularization in non-convex optimization settings. The core of our algorithm, termed GGT, consists of the efficient computation of the inverse square root of a low-rank matrix. Our preliminary experiments show improved iteration-wise convergence rates across synthetic tasks and standard deep learning benchmarks, and that the more carefully-preconditioned steps sometimes lead to a better solution.",
        "url": "https://arxiv.org/pdf/1806.02958v2.pdf"
    },
    {
        "title": "Program Synthesis Through Reinforcement Learning Guided Tree Search",
        "abstract": "Program Synthesis is the task of generating a program from a provided\nspecification. Traditionally, this has been treated as a search problem by the\nprogramming languages (PL) community and more recently as a supervised learning\nproblem by the machine learning community. Here, we propose a third approach,\nrepresenting the task of synthesizing a given program as a Markov decision\nprocess solvable via reinforcement learning(RL). From observations about the\nstates of partial programs, we attempt to find a program that is optimal over a\nprovided reward metric on pairs of programs and states. We instantiate this\napproach on a subset of the RISC-V assembly language operating on floating\npoint numbers, and as an optimization inspired by search-based techniques from\nthe PL community, we combine RL with a priority search tree. We evaluate this\ninstantiation and demonstrate the effectiveness of our combined method compared\nto a variety of baselines, including a pure RL ablation and a state of the art\nMarkov chain Monte Carlo search method on this task.",
        "url": "http://arxiv.org/pdf/1806.02932v1.pdf"
    },
    {
        "title": "Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care",
        "abstract": "Patients in the intensive care unit (ICU) require constant and close\nsupervision. To assist clinical staff in this task, hospitals use monitoring\nsystems that trigger audiovisual alarms if their algorithms indicate that a\npatient's condition may be worsening. However, current monitoring systems are\nextremely sensitive to movement artefacts and technical errors. As a result,\nthey typically trigger hundreds to thousands of false alarms per patient per\nday - drowning the important alarms in noise and adding to the exhaustion of\nclinical staff. In this setting, data is abundantly available, but obtaining\ntrustworthy annotations by experts is laborious and expensive. We frame the\nproblem of false alarm reduction from multivariate time series as a\nmachine-learning task and address it with a novel multitask network\narchitecture that utilises distant supervision through multiple related\nauxiliary tasks in order to reduce the number of expensive labels required for\ntraining. We show that our approach leads to significant improvements over\nseveral state-of-the-art baselines on real-world ICU data and provide new\ninsights on the importance of task selection and architectural choices in\ndistantly supervised multitask learning.",
        "url": "http://arxiv.org/pdf/1802.05027v2.pdf"
    },
    {
        "title": "Feature selection in functional data classification with recursive maxima hunting",
        "abstract": "Dimensionality reduction is one of the key issues in the design of effective\nmachine learning methods for automatic induction. In this work, we introduce\nrecursive maxima hunting (RMH) for variable selection in classification\nproblems with functional data. In this context, variable selection techniques\nare especially attractive because they reduce the dimensionality, facilitate\nthe interpretation and can improve the accuracy of the predictive models. The\nmethod, which is a recursive extension of maxima hunting (MH), performs\nvariable selection by identifying the maxima of a relevance function, which\nmeasures the strength of the correlation of the predictor functional variable\nwith the class label. At each stage, the information associated with the\nselected variable is removed by subtracting the conditional expectation of the\nprocess. The results of an extensive empirical evaluation are used to\nillustrate that, in the problems investigated, RMH has comparable or higher\npredictive accuracy than the standard dimensionality reduction techniques, such\nas PCA and PLS, and state-of-the-art feature selection methods for functional\ndata, such as maxima hunting.",
        "url": "http://arxiv.org/pdf/1806.02922v1.pdf"
    },
    {
        "title": "RadialGAN: Leveraging multiple datasets to improve target-specific predictive models using Generative Adversarial Networks",
        "abstract": "Training complex machine learning models for prediction often requires a\nlarge amount of data that is not always readily available. Leveraging these\nexternal datasets from related but different sources is therefore an important\ntask if good predictive models are to be built for deployment in settings where\ndata can be rare. In this paper we propose a novel approach to the problem in\nwhich we use multiple GAN architectures to learn to translate from one dataset\nto another, thereby allowing us to effectively enlarge the target dataset, and\ntherefore learn better predictive models than if we simply used the target\ndataset. We show the utility of such an approach, demonstrating that our method\nimproves the prediction performance on the target domain over using just the\ntarget dataset and also show that our framework outperforms several other\nbenchmarks on a collection of real-world medical datasets.",
        "url": "http://arxiv.org/pdf/1802.06403v2.pdf"
    },
    {
        "title": "CapsGAN: Using Dynamic Routing for Generative Adversarial Networks",
        "abstract": "In this paper, we propose a novel technique for generating images in the 3D\ndomain from images with high degree of geometrical transformations. By\ncoalescing two popular concurrent methods that have seen rapid ascension to the\nmachine learning zeitgeist in recent years: GANs (Goodfellow et. al.) and\nCapsule networks (Sabour, Hinton et. al.) - we present: \\textbf{CapsGAN}. We\nshow that CapsGAN performs better than or equal to traditional CNN based GANs\nin generating images with high geometric transformations using rotated MNIST.\nIn the process, we also show the efficacy of using capsules architecture in the\nGANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the\nperformance control and training stability by experimenting with using\nWasserstein distance (gradient clipping, penalty) and Spectral Normalization.\nThe experimental findings of this paper should propel the application of\ncapsules and GANs in the still exciting and nascent domain of 3D image\ngeneration, and plausibly video (frame) generation.",
        "url": "http://arxiv.org/pdf/1806.03968v1.pdf"
    },
    {
        "title": "Adversarial Time-to-Event Modeling",
        "abstract": "Modern health data science applications leverage abundant molecular and\nelectronic health data, providing opportunities for machine learning to build\nstatistical models to support clinical practice. Time-to-event analysis, also\ncalled survival analysis, stands as one of the most representative examples of\nsuch statistical models. We present a deep-network-based approach that\nleverages adversarial learning to address a key challenge in modern\ntime-to-event modeling: nonparametric estimation of event-time distributions.\nWe also introduce a principled cost function to exploit information from\ncensored events (events that occur subsequent to the observation window).\nUnlike most time-to-event models, we focus on the estimation of time-to-event\ndistributions, rather than time ordering. We validate our model on both\nbenchmark and real datasets, demonstrating that the proposed formulation yields\nsignificant performance gains relative to a parametric alternative, which we\nalso propose.",
        "url": "http://arxiv.org/pdf/1804.03184v2.pdf"
    },
    {
        "title": "Generating Artificial Data for Private Deep Learning",
        "abstract": "In this paper, we propose generating artificial data that retain statistical\nproperties of real data as the means of providing privacy with respect to the\noriginal dataset. We use generative adversarial network to draw\nprivacy-preserving artificial data samples and derive an empirical method to\nassess the risk of information disclosure in a differential-privacy-like way.\nOur experiments show that we are able to generate artificial data of high\nquality and successfully train and validate machine learning models on this\ndata while limiting potential privacy loss.",
        "url": "http://arxiv.org/pdf/1803.03148v3.pdf"
    },
    {
        "title": "Residual Unfairness in Fair Machine Learning from Prejudiced Data",
        "abstract": "Recent work in fairness in machine learning has proposed adjusting for\nfairness by equalizing accuracy metrics across groups and has also studied how\ndatasets affected by historical prejudices may lead to unfair decision\npolicies. We connect these lines of work and study the residual unfairness that\narises when a fairness-adjusted predictor is not actually fair on the target\npopulation due to systematic censoring of training data by existing biased\npolicies. This scenario is particularly common in the same applications where\nfairness is a concern. We characterize theoretically the impact of such\ncensoring on standard fairness metrics for binary classifiers and provide\ncriteria for when residual unfairness may or may not appear. We prove that,\nunder certain conditions, fairness-adjusted classifiers will in fact induce\nresidual unfairness that perpetuates the same injustices, against the same\ngroups, that biased the data to begin with, thus showing that even\nstate-of-the-art fair machine learning can have a \"bias in, bias out\" property.\nWhen certain benchmark data is available, we show how sample reweighting can\nestimate and adjust fairness metrics while accounting for censoring. We use\nthis to study the case of Stop, Question, and Frisk (SQF) and demonstrate that\nattempting to adjust for fairness perpetuates the same injustices that the\npolicy is infamous for.",
        "url": "http://arxiv.org/pdf/1806.02887v1.pdf"
    },
    {
        "title": "Pros and Cons of GAN Evaluation Measures",
        "abstract": "Generative models, in particular generative adversarial networks (GANs), have\nreceived significant attention recently. A number of GAN variants have been\nproposed and have been utilized in many applications. Despite large strides in\nterms of theoretical progress, evaluating and comparing GANs remains a daunting\ntask. While several measures have been introduced, as of yet, there is no\nconsensus as to which measure best captures strengths and limitations of models\nand should be used for fair model comparison. As in other areas of computer\nvision and machine learning, it is critical to settle on one or few good\nmeasures to steer the progress in this field. In this paper, I review and\ncritically discuss more than 24 quantitative and 5 qualitative measures for\nevaluating generative models with a particular emphasis on GAN-derived models.\nI also provide a set of 7 desiderata followed by an evaluation of whether a\ngiven measure or a family of measures is compatible with them.",
        "url": "http://arxiv.org/pdf/1802.03446v5.pdf"
    },
    {
        "title": "Learning Tasks for Multitask Learning: Heterogenous Patient Populations in the ICU",
        "abstract": "Machine learning approaches have been effective in predicting adverse\noutcomes in different clinical settings. These models are often developed and\nevaluated on datasets with heterogeneous patient populations. However, good\npredictive performance on the aggregate population does not imply good\nperformance for specific groups.\n  In this work, we present a two-step framework to 1) learn relevant patient\nsubgroups, and 2) predict an outcome for separate patient populations in a\nmulti-task framework, where each population is a separate task. We demonstrate\nhow to discover relevant groups in an unsupervised way with a\nsequence-to-sequence autoencoder. We show that using these groups in a\nmulti-task framework leads to better predictive performance of in-hospital\nmortality both across groups and overall. We also highlight the need for more\ngranular evaluation of performance when dealing with heterogeneous populations.",
        "url": "http://arxiv.org/pdf/1806.02878v1.pdf"
    },
    {
        "title": "An Exploration of Unreliable News Classification in Brazil and The U.S",
        "abstract": "The propagation of unreliable information is on the rise in many places\naround the world. This expansion is facilitated by the rapid spread of\ninformation and anonymity granted by the Internet. The spread of unreliable\ninformation is a wellstudied issue and it is associated with negative social\nimpacts. In a previous work, we have identified significant differences in the\nstructure of news articles from reliable and unreliable sources in the US\nmedia. Our goal in this work was to explore such differences in the Brazilian\nmedia. We found significant features in two data sets: one with Brazilian news\nin Portuguese and another one with US news in English. Our results show that\nfeatures related to the writing style were prominent in both data sets and,\ndespite the language difference, some features have a universal behavior, being\nsignificant to both US and Brazilian news articles. Finally, we combined both\ndata sets and used the universal features to build a machine learning\nclassifier to predict the source type of a news article as reliable or\nunreliable.",
        "url": "http://arxiv.org/pdf/1806.02875v1.pdf"
    },
    {
        "title": "Concentration of tempered posteriors and of their variational approximations",
        "abstract": "While Bayesian methods are extremely popular in statistics and machine\nlearning, their application to massive datasets is often challenging, when\npossible at all. Indeed, the classical MCMC algorithms are prohibitively slow\nwhen both the model dimension and the sample size are large. Variational\nBayesian methods aim at approximating the posterior by a distribution in a\ntractable family. Thus, MCMC are replaced by an optimization algorithm which is\norders of magnitude faster. VB methods have been applied in such\ncomputationally demanding applications as including collaborative filtering,\nimage and video processing, NLP and text processing... However, despite very\nnice results in practice, the theoretical properties of these approximations\nare usually not known. In this paper, we propose a general approach to prove\nthe concentration of variational approximations of fractional posteriors. We\napply our theory to two examples: matrix completion, and Gaussian VB.",
        "url": "http://arxiv.org/pdf/1706.09293v3.pdf"
    },
    {
        "title": "Where are we now? A large benchmark study of recent symbolic regression methods",
        "abstract": "In this paper we provide a broad benchmarking of recent genetic programming\napproaches to symbolic regression in the context of state of the art machine\nlearning approaches. We use a set of nearly 100 regression benchmark problems\nculled from open source repositories across the web. We conduct a rigorous\nbenchmarking of four recent symbolic regression approaches as well as nine\nmachine learning approaches from scikit-learn. The results suggest that\nsymbolic regression performs strongly compared to state-of-the-art gradient\nboosting algorithms, although in terms of running times is among the slowest of\nthe available methodologies. We discuss the results in detail and point to\nfuture research directions that may allow symbolic regression to gain wider\nadoption in the machine learning community.",
        "url": "http://arxiv.org/pdf/1804.09331v2.pdf"
    },
    {
        "title": "Online Adaptive Machine Learning Based Algorithm for Implied Volatility Surface Modeling",
        "abstract": "In this work, we design a machine learning based method, online adaptive\nprimal support vector regression (SVR), to model the implied volatility surface\n(IVS). The algorithm proposed is the first derivation and implementation of an\nonline primal kernel SVR. It features enhancements that allow efficient online\nadaptive learning by embedding the idea of local fitness and budget maintenance\nto dynamically update support vectors upon pattern drifts. For algorithm\nacceleration, we implement its most computationally intensive parts in a Field\nProgrammable Gate Arrays hardware, where a 132x speedup over CPU is achieved\nduring online prediction. Using intraday tick data from the E-mini S&P 500\noptions market, we show that the Gaussian kernel outperforms the linear kernel\nin regulating the size of support vectors, and that our empirical IVS algorithm\nbeats two competing online methods with regards to model complexity and\nregression errors (the mean absolute percentage error of our algorithm is up to\n13%). Best results are obtained at the center of the IVS grid due to its larger\nnumber of adjacent support vectors than the edges of the grid. Sensitivity\nanalysis is also presented to demonstrate how hyper parameters affect the error\nrates and model complexity.",
        "url": "http://arxiv.org/pdf/1706.01833v2.pdf"
    },
    {
        "title": "Speaker-Follower Models for Vision-and-Language Navigation",
        "abstract": "Navigation guided by natural language instructions presents a challenging\nreasoning problem for instruction followers. Natural language instructions\ntypically identify only a few high-level decisions and landmarks rather than\ncomplete low-level motor behaviors; much of the missing information must be\ninferred based on perceptual context. In machine learning settings, this is\ndoubly challenging: it is difficult to collect enough annotated data to enable\nlearning of this reasoning process from scratch, and also difficult to\nimplement the reasoning process using generic sequence models. Here we describe\nan approach to vision-and-language navigation that addresses both these issues\nwith an embedded speaker model. We use this speaker model to (1) synthesize new\ninstructions for data augmentation and to (2) implement pragmatic reasoning,\nwhich evaluates how well candidate action sequences explain an instruction.\nBoth steps are supported by a panoramic action space that reflects the\ngranularity of human-generated instructions. Experiments show that all three\ncomponents of this approach---speaker-driven data augmentation, pragmatic\nreasoning and panoramic action space---dramatically improve the performance of\na baseline instruction follower, more than doubling the success rate over the\nbest existing approach on a standard benchmark.",
        "url": "http://arxiv.org/pdf/1806.02724v2.pdf"
    },
    {
        "title": "A representer theorem for deep kernel learning",
        "abstract": "In this paper we provide a finite-sample and an infinite-sample representer\ntheorem for the concatenation of (linear combinations of) kernel functions of\nreproducing kernel Hilbert spaces. These results serve as mathematical\nfoundation for the analysis of machine learning algorithms based on\ncompositions of functions. As a direct consequence in the finite-sample case,\nthe corresponding infinite-dimensional minimization problems can be recast into\n(nonlinear) finite-dimensional minimization problems, which can be tackled with\nnonlinear optimization algorithms. Moreover, we show how concatenated machine\nlearning problems can be reformulated as neural networks and how our\nrepresenter theorem applies to a broad class of state-of-the-art deep learning\nmethods.",
        "url": "http://arxiv.org/pdf/1709.10441v3.pdf"
    },
    {
        "title": "Does Distributionally Robust Supervised Learning Give Robust Classifiers?",
        "abstract": "Distributionally Robust Supervised Learning (DRSL) is necessary for building\nreliable machine learning systems. When machine learning is deployed in the\nreal world, its performance can be significantly degraded because test data may\nfollow a different distribution from training data. DRSL with f-divergences\nexplicitly considers the worst-case distribution shift by minimizing the\nadversarially reweighted training loss. In this paper, we analyze this DRSL,\nfocusing on the classification scenario. Since the DRSL is explicitly\nformulated for a distribution shift scenario, we naturally expect it to give a\nrobust classifier that can aggressively handle shifted distributions. However,\nsurprisingly, we prove that the DRSL just ends up giving a classifier that\nexactly fits the given training distribution, which is too pessimistic. This\npessimism comes from two sources: the particular losses used in classification\nand the fact that the variety of distributions to which the DRSL tries to be\nrobust is too wide. Motivated by our analysis, we propose simple DRSL that\novercomes this pessimism and empirically demonstrate its effectiveness.",
        "url": "http://arxiv.org/pdf/1611.02041v6.pdf"
    },
    {
        "title": "Augment and Reduce: Stochastic Inference for Large Categorical Distributions",
        "abstract": "Categorical distributions are ubiquitous in machine learning, e.g., in\nclassification, language models, and recommendation systems. However, when the\nnumber of possible outcomes is very large, using categorical distributions\nbecomes computationally expensive, as the complexity scales linearly with the\nnumber of outcomes. To address this problem, we propose augment and reduce\n(A&R), a method to alleviate the computational complexity. A&R uses two ideas:\nlatent variable augmentation and stochastic variational inference. It maximizes\na lower bound on the marginal likelihood of the data. Unlike existing methods\nwhich are specific to softmax, A&R is more general and is amenable to other\ncategorical models, such as multinomial probit. On several large-scale\nclassification problems, we show that A&R provides a tighter bound on the\nmarginal likelihood and has better predictive performance than existing\napproaches.",
        "url": "http://arxiv.org/pdf/1802.04220v3.pdf"
    },
    {
        "title": "Re-evaluating Evaluation",
        "abstract": "Progress in machine learning is measured by careful evaluation on problems of\noutstanding common interest. However, the proliferation of benchmark suites and\nenvironments, adversarial attacks, and other complications has diluted the\nbasic evaluation model by overwhelming researchers with choices. Deliberate or\naccidental cherry picking is increasingly likely, and designing well-balanced\nevaluation suites requires increasing effort. In this paper we take a step back\nand propose Nash averaging. The approach builds on a detailed analysis of the\nalgebraic structure of evaluation in two basic scenarios: agent-vs-agent and\nagent-vs-task. The key strength of Nash averaging is that it automatically\nadapts to redundancies in evaluation data, so that results are not biased by\nthe incorporation of easy tasks or weak agents. Nash averaging thus encourages\nmaximally inclusive evaluation -- since there is no harm (computational cost\naside) from including all available tasks and agents.",
        "url": "http://arxiv.org/pdf/1806.02643v2.pdf"
    },
    {
        "title": "Nonparametric Density Flows for MRI Intensity Normalisation",
        "abstract": "With the adoption of powerful machine learning methods in medical image\nanalysis, it is becoming increasingly desirable to aggregate data that is\nacquired across multiple sites. However, the underlying assumption of many\nanalysis techniques that corresponding tissues have consistent intensities in\nall images is often violated in multi-centre databases. We introduce a novel\nintensity normalisation scheme based on density matching, wherein the\nhistograms are modelled as Dirichlet process Gaussian mixtures. The source\nmixture model is transformed to minimise its $L^2$ divergence towards a target\nmodel, then the voxel intensities are transported through a mass-conserving\nflow to maintain agreement with the moving density. In a multi-centre study\nwith brain MRI data, we show that the proposed technique produces excellent\ncorrespondence between the matched densities and histograms. We further\ndemonstrate that our method makes tissue intensity statistics substantially\nmore compatible between images than a baseline affine transformation and is\ncomparable to state-of-the-art while providing considerably smoother\ntransformations. Finally, we validate that nonlinear intensity normalisation is\na step toward effective imaging data harmonisation.",
        "url": "http://arxiv.org/pdf/1806.02613v1.pdf"
    },
    {
        "title": "Learning Multi-Modal Self-Awareness Models for Autonomous Vehicles from Human Driving",
        "abstract": "This paper presents a novel approach for learning self-awareness models for\nautonomous vehicles. The proposed technique is based on the availability of\nsynchronized multi-sensor dynamic data related to different maneuvering tasks\nperformed by a human operator. It is shown that different machine learning\napproaches can be used to first learn single modality models using coupled\nDynamic Bayesian Networks; such models are then correlated at event level to\ndiscover contextual multi-modal concepts. In the presented case, visual\nperception and localization are used as modalities. Cross-correlations among\nmodalities in time is discovered from data and are described as probabilistic\nlinks connecting shared and private multi-modal DBNs at the event (discrete)\nlevel. Results are presented on experiments performed on an autonomous vehicle,\nhighlighting potentiality of the proposed approach to allow anomaly detection\nand autonomous decision making based on learned self-awareness models.",
        "url": "http://arxiv.org/pdf/1806.02609v1.pdf"
    },
    {
        "title": "On the Effect of Inter-observer Variability for a Reliable Estimation of Uncertainty of Medical Image Segmentation",
        "abstract": "Uncertainty estimation methods are expected to improve the understanding and\nquality of computer-assisted methods used in medical applications (e.g.,\nneurosurgical interventions, radiotherapy planning), where automated medical\nimage segmentation is crucial. In supervised machine learning, a common\npractice to generate ground truth label data is to merge observer annotations.\nHowever, as many medical image tasks show a high inter-observer variability\nresulting from factors such as image quality, different levels of user\nexpertise and domain knowledge, little is known as to how inter-observer\nvariability and commonly used fusion methods affect the estimation of\nuncertainty of automated image segmentation. In this paper we analyze the\neffect of common image label fusion techniques on uncertainty estimation, and\npropose to learn the uncertainty among observers. The results highlight the\nnegative effect of fusion methods applied in deep learning, to obtain reliable\nestimates of segmentation uncertainty. Additionally, we show that the learned\nobservers' uncertainty can be combined with current standard Monte Carlo\ndropout Bayesian neural networks to characterize uncertainty of model's\nparameters.",
        "url": "http://arxiv.org/pdf/1806.02562v1.pdf"
    },
    {
        "title": "Semi-Dynamic Load Balancing: Efficient Distributed Learning in Non-Dedicated Environments",
        "abstract": "Machine learning (ML) models are increasingly trained in clusters with non-dedicated workers possessing heterogeneous resources. In such scenarios, model training efficiency can be negatively affected by stragglers -- workers that run much slower than others. Efficient model training requires eliminating such stragglers, yet for modern ML workloads, existing load balancing strategies are inefficient and even infeasible. In this paper, we propose a novel strategy called semi-dynamic load balancing to eliminate stragglers of distributed ML workloads. The key insight is that ML workers shall be load-balanced at iteration boundaries, being non-intrusive to intra-iteration execution. We develop LB-BSP based on such an insight, which is an integrated worker coordination mechanism that adapts workers' load to their instantaneous processing capabilities by right-sizing the sample batches at the synchronization barriers. We have custom-designed the batch sizing algorithm respectively for CPU and GPU clusters based on their own characteristics. LB-BSP has been implemented as a Python module for ML frameworks like TensorFlow and PyTorch. Our EC2 deployment confirms that LB-BSP is practical, effective and light-weight, and is able to accelerating distributed training by up to $54\\%$.",
        "url": "https://arxiv.org/pdf/1806.02508v2.pdf"
    },
    {
        "title": "Large scale classification in deep neural network with Label Mapping",
        "abstract": "In recent years, deep neural network is widely used in machine learning. The\nmulti-class classification problem is a class of important problem in machine\nlearning. However, in order to solve those types of multi-class classification\nproblems effectively, the required network size should have hyper-linear growth\nwith respect to the number of classes. Therefore, it is infeasible to solve the\nmulti-class classification problem using deep neural network when the number of\nclasses are huge. This paper presents a method, so called Label Mapping (LM),\nto solve this problem by decomposing the original classification problem to\nseveral smaller sub-problems which are solvable theoretically. Our method is an\nensemble method like error-correcting output codes (ECOC), but it allows base\nlearners to be multi-class classifiers with different number of class labels.\nWe propose two design principles for LM, one is to maximize the number of base\nclassifier which can separate two different classes, and the other is to keep\nall base learners to be independent as possible in order to reduce the\nredundant information. Based on these principles, two different LM algorithms\nare derived using number theory and information theory. Since each base learner\ncan be trained independently, it is easy to scale our method into a large scale\ntraining system. Experiments show that our proposed method outperforms the\nstandard one-hot encoding and ECOC significantly in terms of accuracy and model\ncomplexity.",
        "url": "http://arxiv.org/pdf/1806.02507v1.pdf"
    },
    {
        "title": "Safe Element Screening for Submodular Function Minimization",
        "abstract": "Submodular functions are discrete analogs of convex functions, which have\napplications in various fields, including machine learning and computer vision.\nHowever, in large-scale applications, solving Submodular Function Minimization\n(SFM) problems remains challenging. In this paper, we make the first attempt to\nextend the emerging technique named screening in large-scale sparse learning to\nSFM for accelerating its optimization process. We first conduct a careful\nstudying of the relationships between SFM and the corresponding convex proximal\nproblems, as well as the accurate primal optimum estimation of the proximal\nproblems. Relying on this study, we subsequently propose a novel safe screening\nmethod to quickly identify the elements guaranteed to be included (we refer to\nthem as active) or excluded (inactive) in the final optimal solution of SFM\nduring the optimization process. By removing the inactive elements and fixing\nthe active ones, the problem size can be dramatically reduced, leading to great\nsavings in the computational cost without sacrificing any accuracy. To the best\nof our knowledge, the proposed method is the first screening method in the\nfields of SFM and even combinatorial optimization, thus pointing out a new\ndirection for accelerating SFM algorithms. Experiment results on both synthetic\nand real datasets demonstrate the significant speedups gained by our approach.",
        "url": "http://arxiv.org/pdf/1805.08527v4.pdf"
    },
    {
        "title": "K-Beam Minimax: Efficient Optimization for Deep Adversarial Learning",
        "abstract": "Minimax optimization plays a key role in adversarial training of machine\nlearning algorithms, such as learning generative models, domain adaptation,\nprivacy preservation, and robust learning. In this paper, we demonstrate the\nfailure of alternating gradient descent in minimax optimization problems due to\nthe discontinuity of solutions of the inner maximization. To address this, we\npropose a new epsilon-subgradient descent algorithm that addresses this problem\nby simultaneously tracking K candidate solutions. Practically, the algorithm\ncan find solutions that previous saddle-point algorithms cannot find, with only\na sublinear increase of complexity in K. We analyze the conditions under which\nthe algorithm converges to the true solution in detail. A significant\nimprovement in stability and convergence speed of the algorithm is observed in\nsimple representative problems, GAN training, and domain-adaptation problems.",
        "url": "http://arxiv.org/pdf/1805.11640v2.pdf"
    },
    {
        "title": "Deep learning based inverse method for layout design",
        "abstract": "Layout design with complex constraints is a challenging problem to solve due\nto the non-uniqueness of the solution and the difficulties in incorporating the\nconstraints into the conventional optimization-based methods. In this paper, we\npropose a design method based on the recently developed machine learning\ntechnique, Variational Autoencoder (VAE). We utilize the learning capability of\nthe VAE to learn the constraints and the generative capability of the VAE to\ngenerate design candidates that automatically satisfy all the constraints. As\nsuch, no constraints need to be imposed during the design stage. In addition,\nwe show that the VAE network is also capable of learning the underlying physics\nof the design problem, leading to an efficient design tool that does not need\nany physical simulation once the network is constructed. We demonstrated the\nperformance of the method on two cases: inverse design of surface diffusion\ninduced morphology change and mask design for optical microlithography.",
        "url": "http://arxiv.org/pdf/1806.03182v1.pdf"
    },
    {
        "title": "Stochastic Zeroth-order Optimization via Variance Reduction method",
        "abstract": "Derivative-free optimization has become an important technique used in\nmachine learning for optimizing black-box models. To conduct updates without\nexplicitly computing gradient, most current approaches iteratively sample a\nrandom search direction from Gaussian distribution and compute the estimated\ngradient along that direction. However, due to the variance in the search\ndirection, the convergence rates and query complexities of existing methods\nsuffer from a factor of $d$, where $d$ is the problem dimension. In this paper,\nwe introduce a novel Stochastic Zeroth-order method with Variance Reduction\nunder Gaussian smoothing (SZVR-G) and establish the complexity for optimizing\nnon-convex problems. With variance reduction on both sample space and search\nspace, the complexity of our algorithm is sublinear to $d$ and is strictly\nbetter than current approaches, in both smooth and non-smooth cases. Moreover,\nwe extend the proposed method to the mini-batch version. Our experimental\nresults demonstrate the superior performance of the proposed method over\nexisting derivative-free optimization techniques. Furthermore, we successfully\napply our method to conduct a universal black-box attack to deep neural\nnetworks and present some interesting results.",
        "url": "http://arxiv.org/pdf/1805.11811v3.pdf"
    },
    {
        "title": "Quantitative Phase Imaging and Artificial Intelligence: A Review",
        "abstract": "Recent advances in quantitative phase imaging (QPI) and artificial\nintelligence (AI) have opened up the possibility of an exciting frontier. The\nfast and label-free nature of QPI enables the rapid generation of large-scale\nand uniform-quality imaging data in two, three, and four dimensions.\nSubsequently, the AI-assisted interrogation of QPI data using data-driven\nmachine learning techniques results in a variety of biomedical applications.\nAlso, machine learning enhances QPI itself. Herein, we review the synergy\nbetween QPI and machine learning with a particular focus on deep learning.\nFurther, we provide practical guidelines and perspectives for further\ndevelopment.",
        "url": "http://arxiv.org/pdf/1806.03982v2.pdf"
    },
    {
        "title": "Hyperbolic Entailment Cones for Learning Hierarchical Embeddings",
        "abstract": "Learning graph representations via low-dimensional embeddings that preserve\nrelevant network properties is an important class of problems in machine\nlearning. We here present a novel method to embed directed acyclic graphs.\nFollowing prior work, we first advocate for using hyperbolic spaces which\nprovably model tree-like structures better than Euclidean geometry. Second, we\nview hierarchical relations as partial orders defined using a family of nested\ngeodesically convex cones. We prove that these entailment cones admit an\noptimal shape with a closed form expression both in the Euclidean and\nhyperbolic spaces, and they canonically define the embedding learning process.\nExperiments show significant improvements of our method over strong recent\nbaselines both in terms of representational capacity and generalization.",
        "url": "http://arxiv.org/pdf/1804.01882v3.pdf"
    },
    {
        "title": "Human-aided Multi-Entity Bayesian Networks Learning from Relational Data",
        "abstract": "An Artificial Intelligence (AI) system is an autonomous system which emulates\nhuman mental and physical activities such as Observe, Orient, Decide, and Act,\ncalled the OODA process. An AI system performing the OODA process requires a\nsemantically rich representation to handle a complex real world situation and\nability to reason under uncertainty about the situation. Multi-Entity Bayesian\nNetworks (MEBNs) combines First-Order Logic with Bayesian Networks for\nrepresenting and reasoning about uncertainty in complex, knowledge-rich\ndomains. MEBN goes beyond standard Bayesian networks to enable reasoning about\nan unknown number of entities interacting with each other in various types of\nrelationships, a key requirement for the OODA process of an AI system. MEBN\nmodels have heretofore been constructed manually by a domain expert. However,\nmanual MEBN modeling is labor-intensive and insufficiently agile. To address\nthese problems, an efficient method is needed for MEBN modeling. One of the\nmethods is to use machine learning to learn a MEBN model in whole or in part\nfrom data. In the era of Big Data, data-rich environments, characterized by\nuncertainty and complexity, have become ubiquitous. The larger the data sample\nis, the more accurate the results of the machine learning approach can be.\nTherefore, machine learning has potential to improve the quality of MEBN models\nas well as the effectiveness for MEBN modeling. In this research, we study a\nMEBN learning framework to develop a MEBN model from a combination of domain\nexpert's knowledge and data. To evaluate the MEBN learning framework, we\nconduct an experiment to compare the MEBN learning framework and the existing\nmanual MEBN modeling in terms of development efficiency.",
        "url": "http://arxiv.org/pdf/1806.02421v1.pdf"
    },
    {
        "title": "Human-like generalization in a machine through predicate learning",
        "abstract": "Humans readily generalize, applying prior knowledge to novel situations and\nstimuli. Advances in machine learning and artificial intelligence have begun to\napproximate and even surpass human performance, but machine systems reliably\nstruggle to generalize information to untrained situations. We describe a\nneural network model that is trained to play one video game (Breakout) and\ndemonstrates one-shot generalization to a new game (Pong). The model\ngeneralizes by learning representations that are functionally and formally\nsymbolic from training data, without feedback, and without requiring that\nstructured representations be specified a priori. The model uses unsupervised\ncomparison to discover which characteristics of the input are invariant, and to\nlearn relational predicates; it then applies these predicates to arguments in a\nsymbolic fashion, using oscillatory regularities in network firing to\ndynamically bind predicates to arguments. We argue that models of human\ncognition must account for far-reaching and flexible generalization, and that\nin order to do so, models must be able to discover symbolic representations\nfrom unstructured data, a process we call predicate learning. Only then can\nmodels begin to adequately explain where human-like representations come from,\nwhy human cognition is the way it is, and why it continues to differ from\nmachine intelligence in crucial ways.",
        "url": "http://arxiv.org/pdf/1806.01709v3.pdf"
    },
    {
        "title": "Evidential Deep Learning to Quantify Classification Uncertainty",
        "abstract": "Deterministic neural nets have been shown to learn effective predictors on a\nwide range of machine learning problems. However, as the standard approach is\nto train the network to minimize a prediction loss, the resultant model remains\nignorant to its prediction confidence. Orthogonally to Bayesian neural nets\nthat indirectly infer prediction uncertainty through weight uncertainties, we\npropose explicit modeling of the same using the theory of subjective logic. By\nplacing a Dirichlet distribution on the class probabilities, we treat\npredictions of a neural net as subjective opinions and learn the function that\ncollects the evidence leading to these opinions by a deterministic neural net\nfrom data. The resultant predictor for a multi-class classification problem is\nanother Dirichlet distribution whose parameters are set by the continuous\noutput of a neural net. We provide a preliminary analysis on how the\npeculiarities of our new loss function drive improved uncertainty estimation.\nWe observe that our method achieves unprecedented success on detection of\nout-of-distribution queries and endurance against adversarial perturbations.",
        "url": "http://arxiv.org/pdf/1806.01768v3.pdf"
    },
    {
        "title": "Not All Attributes are Created Equal: $d_{\\mathcal{X}}$-Private Mechanisms for Linear Queries",
        "abstract": "Differential privacy provides strong privacy guarantees simultaneously enabling useful insights from sensitive datasets. However, it provides the same level of protection for all elements (individuals and attributes) in the data. There are practical scenarios where some data attributes need more/less protection than others. In this paper, we consider $d_{\\mathcal{X}}$-privacy, an instantiation of the privacy notion introduced in \\cite{chatzikokolakis2013broadening}, which allows this flexibility by specifying a separate privacy budget for each pair of elements in the data domain. We describe a systematic procedure to tailor any existing differentially private mechanism that assumes a query set and a sensitivity vector as input into its $d_{\\mathcal{X}}$-private variant, specifically focusing on linear queries. Our proposed meta procedure has broad applications as linear queries form the basis of a range of data analysis and machine learning algorithms, and the ability to define a more flexible privacy budget across the data domain results in improved privacy/utility tradeoff in these applications. We propose several $d_{\\mathcal{X}}$-private mechanisms, and provide theoretical guarantees on the trade-off between utility and privacy. We also experimentally demonstrate the effectiveness of our procedure, by evaluating our proposed $d_{\\mathcal{X}}$-private Laplace mechanism on both synthetic and real datasets using a set of randomly generated linear queries.",
        "url": "https://arxiv.org/pdf/1806.02389v2.pdf"
    },
    {
        "title": "Causal Interventions for Fairness",
        "abstract": "Most approaches in algorithmic fairness constrain machine learning methods so\nthe resulting predictions satisfy one of several intuitive notions of fairness.\nWhile this may help private companies comply with non-discrimination laws or\navoid negative publicity, we believe it is often too little, too late. By the\ntime the training data is collected, individuals in disadvantaged groups have\nalready suffered from discrimination and lost opportunities due to factors out\nof their control. In the present work we focus instead on interventions such as\na new public policy, and in particular, how to maximize their positive effects\nwhile improving the fairness of the overall system. We use causal methods to\nmodel the effects of interventions, allowing for potential interference--each\nindividual's outcome may depend on who else receives the intervention. We\ndemonstrate this with an example of allocating a budget of teaching resources\nusing a dataset of schools in New York City.",
        "url": "http://arxiv.org/pdf/1806.02380v1.pdf"
    },
    {
        "title": "Conditional Linear Regression",
        "abstract": "Work in machine learning and statistics commonly focuses on building models that capture the vast majority of data, possibly ignoring a segment of the population as outliers. However, there does not often exist a good model on the whole dataset, so we seek to find a small subset where there exists a useful model. We are interested in finding a linear rule capable of achieving more accurate predictions for just a segment of the population. We give an efficient algorithm with theoretical analysis for the conditional linear regression task, which is the joint task of identifying a significant segment of the population, described by a k-DNF, along with its linear regression fit.",
        "url": "https://arxiv.org/pdf/1806.02326v2.pdf"
    },
    {
        "title": "Learning Kolmogorov Models for Binary Random Variables",
        "abstract": "We summarize our recent findings, where we proposed a framework for learning\na Kolmogorov model, for a collection of binary random variables. More\nspecifically, we derive conditions that link outcomes of specific random\nvariables, and extract valuable relations from the data. We also propose an\nalgorithm for computing the model and show its first-order optimality, despite\nthe combinatorial nature of the learning problem. We apply the proposed\nalgorithm to recommendation systems, although it is applicable to other\nscenarios. We believe that the work is a significant step toward interpretable\nmachine learning.",
        "url": "http://arxiv.org/pdf/1806.02322v1.pdf"
    },
    {
        "title": "Deploying Deep Ranking Models for Search Verticals",
        "abstract": "In this paper, we present an architecture executing a complex machine\nlearning model such as a neural network capturing semantic similarity between a\nquery and a document; and deploy to a real-world production system serving\n500M+users. We present the challenges that arise in a real-world system and how\nwe solve them. We demonstrate that our architecture provides competitive\nmodeling capability without any significant performance impact to the system in\nterms of latency. Our modular solution and insights can be used by other\nreal-world search systems to realize and productionize recent gains in neural\nnetworks.",
        "url": "http://arxiv.org/pdf/1806.02281v1.pdf"
    },
    {
        "title": "Celer: a Fast Solver for the Lasso with Dual Extrapolation",
        "abstract": "Convex sparsity-inducing regularizations are ubiquitous in high-dimensional\nmachine learning, but solving the resulting optimization problems can be slow.\nTo accelerate solvers, state-of-the-art approaches consist in reducing the size\nof the optimization problem at hand. In the context of regression, this can be\nachieved either by discarding irrelevant features (screening techniques) or by\nprioritizing features likely to be included in the support of the solution\n(working set techniques). Duality comes into play at several steps in these\ntechniques. Here, we propose an extrapolation technique starting from a\nsequence of iterates in the dual that leads to the construction of improved\ndual points. This enables a tighter control of optimality as used in stopping\ncriterion, as well as better screening performance of Gap Safe rules. Finally,\nwe propose a working set strategy based on an aggressive use of Gap Safe\nscreening rules. Thanks to our new dual point construction, we show significant\ncomputational speedups on multiple real-world problems.",
        "url": "http://arxiv.org/pdf/1802.07481v3.pdf"
    },
    {
        "title": "Adversarial Regression with Multiple Learners",
        "abstract": "Despite the considerable success enjoyed by machine learning techniques in\npractice, numerous studies demonstrated that many approaches are vulnerable to\nattacks. An important class of such attacks involves adversaries changing\nfeatures at test time to cause incorrect predictions. Previous investigations\nof this problem pit a single learner against an adversary. However, in many\nsituations an adversary's decision is aimed at a collection of learners, rather\nthan specifically targeted at each independently. We study the problem of\nadversarial linear regression with multiple learners. We approximate the\nresulting game by exhibiting an upper bound on learner loss functions, and show\nthat the resulting game has a unique symmetric equilibrium. We present an\nalgorithm for computing this equilibrium, and show through extensive\nexperiments that equilibrium models are significantly more robust than\nconventional regularized linear regression.",
        "url": "http://arxiv.org/pdf/1806.02256v1.pdf"
    },
    {
        "title": "Improving the Privacy and Accuracy of ADMM-Based Distributed Algorithms",
        "abstract": "Alternating direction method of multiplier (ADMM) is a popular method used to\ndesign distributed versions of a machine learning algorithm, whereby local\ncomputations are performed on local data with the output exchanged among\nneighbors in an iterative fashion. During this iterative process the leakage of\ndata privacy arises. A differentially private ADMM was proposed in prior work\n(Zhang & Zhu, 2017) where only the privacy loss of a single node during one\niteration was bounded, a method that makes it difficult to balance the tradeoff\nbetween the utility attained through distributed computation and privacy\nguarantees when considering the total privacy loss of all nodes over the entire\niterative process. We propose a perturbation method for ADMM where the\nperturbed term is correlated with the penalty parameters; this is shown to\nimprove the utility and privacy simultaneously. The method is based on a\nmodified ADMM where each node independently determines its own penalty\nparameter in every iteration and decouples it from the dual updating step size.\nThe condition for convergence of the modified ADMM and the lower bound on the\nconvergence rate are also derived.",
        "url": "http://arxiv.org/pdf/1806.02246v1.pdf"
    },
    {
        "title": "PAC-learning in the presence of evasion adversaries",
        "abstract": "The existence of evasion attacks during the test phase of machine learning\nalgorithms represents a significant challenge to both their deployment and\nunderstanding. These attacks can be carried out by adding imperceptible\nperturbations to inputs to generate adversarial examples and finding effective\ndefenses and detectors has proven to be difficult. In this paper, we step away\nfrom the attack-defense arms race and seek to understand the limits of what can\nbe learned in the presence of an evasion adversary. In particular, we extend\nthe Probably Approximately Correct (PAC)-learning framework to account for the\npresence of an adversary. We first define corrupted hypothesis classes which\narise from standard binary hypothesis classes in the presence of an evasion\nadversary and derive the Vapnik-Chervonenkis (VC)-dimension for these, denoted\nas the adversarial VC-dimension. We then show that sample complexity upper\nbounds from the Fundamental Theorem of Statistical learning can be extended to\nthe case of evasion adversaries, where the sample complexity is controlled by\nthe adversarial VC-dimension. We then explicitly derive the adversarial\nVC-dimension for halfspace classifiers in the presence of a sample-wise\nnorm-constrained adversary of the type commonly studied for evasion attacks and\nshow that it is the same as the standard VC-dimension, closing an open\nquestion. Finally, we prove that the adversarial VC-dimension can be either\nlarger or smaller than the standard VC-dimension depending on the hypothesis\nclass and adversary, making it an interesting object of study in its own right.",
        "url": "http://arxiv.org/pdf/1806.01471v2.pdf"
    },
    {
        "title": "GraKeL: A Graph Kernel Library in Python",
        "abstract": "The problem of accurately measuring the similarity between graphs is at the core of many applications in a variety of disciplines. Graph kernels have recently emerged as a promising approach to this problem. There are now many kernels, each focusing on different structural aspects of graphs. Here, we present GraKeL, a library that unifies several graph kernels into a common framework. The library is written in Python and adheres to the scikit-learn interface. It is simple to use and can be naturally combined with scikit-learn's modules to build a complete machine learning pipeline for tasks such as graph classification and clustering. The code is BSD licensed and is available at: https://github.com/ysig/GraKeL .",
        "url": "https://arxiv.org/pdf/1806.02193v2.pdf"
    },
    {
        "title": "Incorporating Features Learned by an Enhanced Deep Knowledge Tracing Model for STEM/Non-STEM Job Prediction",
        "abstract": "The 2017 ASSISTments Data Mining competition aims to use data from a\nlongitudinal study for predicting a brand-new outcome of students which had\nnever been studied before by the educational data mining research community.\nSpecifically, it facilitates research in developing predictive models that\npredict whether the first job of a student out of college belongs to a STEM\n(the acronym for science, technology, engineering, and mathematics) field. This\nis based on the student's learning history on the ASSISTments blended learning\nplatform in the form of extensive clickstream data gathered during the middle\nschool years. To tackle this challenge, we first estimate the expected\nknowledge state of students with respect to different mathematical skills using\na deep knowledge tracing (DKT) model and an enhanced DKT (DKT+) model. We then\ncombine the features corresponding to the DKT/DKT+ expected knowledge state\nwith other features extracted directly from the student profile in the dataset\nto train several machine learning models for the STEM/non-STEM job prediction.\nOur experiments show that models trained with the combined features generally\nperform better than the models trained with the student profile alone. Detailed\nanalysis of the student's knowledge state reveals that, when compared with\nnon-STEM students, STEM students generally show a higher mastery level and a\nhigher learning gain in mathematics.",
        "url": "http://arxiv.org/pdf/1806.03256v1.pdf"
    },
    {
        "title": "SBAF: A New Activation Function for Artificial Neural Net based Habitability Classification",
        "abstract": "We explore the efficacy of using a novel activation function in Artificial\nNeural Networks (ANN) in characterizing exoplanets into different classes. We\ncall this Saha-Bora Activation Function (SBAF) as the motivation is derived\nfrom long standing understanding of using advanced calculus in modeling\nhabitability score of Exoplanets. The function is demonstrated to possess nice\nanalytical properties and doesn't seem to suffer from local oscillation\nproblems. The manuscript presents the analytical properties of the activation\nfunction and the architecture implemented on the function. Keywords:\nAstroinformatics, Machine Learning, Exoplanets, ANN, Activation Function.",
        "url": "http://arxiv.org/pdf/1806.01844v1.pdf"
    },
    {
        "title": "Adversarial Auto-encoders for Speech Based Emotion Recognition",
        "abstract": "Recently, generative adversarial networks and adversarial autoencoders have\ngained a lot of attention in machine learning community due to their\nexceptional performance in tasks such as digit classification and face\nrecognition. They map the autoencoder's bottleneck layer output (termed as code\nvectors) to different noise Probability Distribution Functions (PDFs), that can\nbe further regularized to cluster based on class information. In addition, they\nalso allow a generation of synthetic samples by sampling the code vectors from\nthe mapped PDFs. Inspired by these properties, we investigate the application\nof adversarial autoencoders to the domain of emotion recognition. Specifically,\nwe conduct experiments on the following two aspects: (i) their ability to\nencode high dimensional feature vector representations for emotional utterances\ninto a compressed space (with a minimal loss of emotion class discriminability\nin the compressed space), and (ii) their ability to regenerate synthetic\nsamples in the original feature space, to be later used for purposes such as\ntraining emotion recognition classifiers. We demonstrate the promise of\nadversarial autoencoders with regards to these aspects on the Interactive\nEmotional Dyadic Motion Capture (IEMOCAP) corpus and present our analysis.",
        "url": "http://arxiv.org/pdf/1806.02146v1.pdf"
    },
    {
        "title": "A New Framework for Machine Intelligence: Concepts and Prototype",
        "abstract": "Machine learning (ML) and artificial intelligence (AI) have become hot topics\nin many information processing areas, from chatbots to scientific data\nanalysis. At the same time, there is uncertainty about the possibility of\nextending predominant ML technologies to become general solutions with\ncontinuous learning capabilities. Here, a simple, yet comprehensive,\ntheoretical framework for intelligent systems is presented. A combination of\nMirror Compositional Representations (MCR) and a Solution-Critic Loop (SCL) is\nproposed as a generic approach for different types of problems. A prototype\nimplementation is presented for document comparison using English Wikipedia\ncorpus.",
        "url": "http://arxiv.org/pdf/1806.02137v1.pdf"
    },
    {
        "title": "Efficient Differentiable Programming in a Functional Array-Processing Language",
        "abstract": "We present a system for the automatic differentiation of a higher-order\nfunctional array-processing language. The core functional language underlying\nthis system simultaneously supports both source-to-source automatic\ndifferentiation and global optimizations such as loop transformations. Thanks\nto this feature, we demonstrate how for some real-world machine learning and\ncomputer vision benchmarks, the system outperforms the state-of-the-art\nautomatic differentiation tools.",
        "url": "http://arxiv.org/pdf/1806.02136v1.pdf"
    },
    {
        "title": "PieAPP: Perceptual Image-Error Assessment through Pairwise Preference",
        "abstract": "The ability to estimate the perceptual error between images is an important\nproblem in computer vision with many applications. Although it has been studied\nextensively, however, no method currently exists that can robustly predict\nvisual differences like humans. Some previous approaches used hand-coded\nmodels, but they fail to model the complexity of the human visual system.\nOthers used machine learning to train models on human-labeled datasets, but\ncreating large, high-quality datasets is difficult because people are unable to\nassign consistent error labels to distorted images. In this paper, we present a\nnew learning-based method that is the first to predict perceptual image error\nlike human observers. Since it is much easier for people to compare two given\nimages and identify the one more similar to a reference than to assign quality\nscores to each, we propose a new, large-scale dataset labeled with the\nprobability that humans will prefer one image over another. We then train a\ndeep-learning model using a novel, pairwise-learning framework to predict the\npreference of one distorted image over the other. Our key observation is that\nour trained network can then be used separately with only one distorted image\nand a reference to predict its perceptual error, without ever being trained on\nexplicit human perceptual-error labels. The perceptual error estimated by our\nnew metric, PieAPP, is well-correlated with human opinion. Furthermore, it\nsignificantly outperforms existing algorithms, beating the state-of-the-art by\nalmost 3x on our test set in terms of binary error rate, while also\ngeneralizing to new kinds of distortions, unlike previous learning-based\nmethods.",
        "url": "http://arxiv.org/pdf/1806.02067v1.pdf"
    },
    {
        "title": "Killing four birds with one Gaussian process: the relation between different test-time attacks",
        "abstract": "In machine learning (ML) security, attacks like evasion, model stealing or membership inference are generally studied in individually. Previous work has also shown a relationship between some attacks and decision function curvature of the targeted model. Consequently, we study an ML model allowing direct control over the decision surface curvature: Gaussian Process classifiers (GPCs). For evasion, we find that changing GPC's curvature to be robust against one attack algorithm boils down to enabling a different norm or attack algorithm to succeed. This is backed up by our formal analysis showing that static security guarantees are opposed to learning. Concerning intellectual property, we show formally that lazy learning does not necessarily leak all information when applied. In practice, often a seemingly secure curvature can be found. For example, we are able to secure GPC against empirical membership inference by proper configuration. In this configuration, however, the GPC's hyper-parameters are leaked, e.g. model reverse engineering succeeds. We conclude that attacks on classification should not be studied in isolation, but in relation to each other.",
        "url": "https://arxiv.org/pdf/1806.02032v3.pdf"
    },
    {
        "title": "Distributionally Robust Submodular Maximization",
        "abstract": "Submodular functions have applications throughout machine learning, but in\nmany settings, we do not have direct access to the underlying function $f$. We\nfocus on stochastic functions that are given as an expectation of functions\nover a distribution $P$. In practice, we often have only a limited set of\nsamples $f_i$ from $P$. The standard approach indirectly optimizes $f$ by\nmaximizing the sum of $f_i$. However, this ignores generalization to the true\n(unknown) distribution. In this paper, we achieve better performance on the\nactual underlying function $f$ by directly optimizing a combination of bias and\nvariance. Algorithmically, we accomplish this by showing how to carry out\ndistributionally robust optimization (DRO) for submodular functions, providing\nefficient algorithms backed by theoretical guarantees which leverage several\nnovel contributions to the general theory of DRO. We also show compelling\nempirical evidence that DRO improves generalization to the unknown stochastic\nsubmodular function.",
        "url": "http://arxiv.org/pdf/1802.05249v2.pdf"
    },
    {
        "title": "Detecting Comma-shaped Clouds for Severe Weather Forecasting using Shape and Motion",
        "abstract": "Meteorologists use shapes and movements of clouds in satellite images as\nindicators of several major types of severe storms. Satellite imaginary data\nare in increasingly higher resolution, both spatially and temporally, making it\nimpossible for humans to fully leverage the data in their forecast. Automatic\nsatellite imagery analysis methods that can find storm-related cloud patterns\nas soon as they are detectable are in demand. We propose a machine learning and\npattern recognition based approach to detect \"comma-shaped\" clouds in satellite\nimages, which are specific cloud distribution patterns strongly associated with\nthe cyclone formulation. In order to detect regions with the targeted movement\npatterns, our method is trained on manually annotated cloud examples\nrepresented by both shape and motion-sensitive features. Sliding windows in\ndifferent scales are used to ensure that dense clouds will be captured, and we\nimplement effective selection rules to shrink the region of interest among\nthese sliding windows. Finally, we evaluate the method on a hold-out annotated\ncomma-shaped cloud dataset and cross-match the results with recorded storm\nevents in the severe weather database. The validated utility and accuracy of\nour method suggest a high potential for assisting meteorologists in weather\nforecasting.",
        "url": "http://arxiv.org/pdf/1802.08937v3.pdf"
    },
    {
        "title": "Reverse iterative volume sampling for linear regression",
        "abstract": "We study the following basic machine learning task: Given a fixed set of\n$d$-dimensional input points for a linear regression problem, we wish to\npredict a hidden response value for each of the points. We can only afford to\nattain the responses for a small subset of the points that are then used to\nconstruct linear predictions for all points in the dataset. The performance of\nthe predictions is evaluated by the total square loss on all responses (the\nattained as well as the hidden ones). We show that a good approximate solution\nto this least squares problem can be obtained from just dimension $d$ many\nresponses by using a joint sampling technique called volume sampling. Moreover,\nthe least squares solution obtained for the volume sampled subproblem is an\nunbiased estimator of optimal solution based on all n responses. This\nunbiasedness is a desirable property that is not shared by other common subset\nselection techniques.\n  Motivated by these basic properties, we develop a theoretical framework for\nstudying volume sampling, resulting in a number of new matrix expectation\nequalities and statistical guarantees which are of importance not only to least\nsquares regression but also to numerical linear algebra in general. Our methods\nalso lead to a regularized variant of volume sampling, and we propose the first\nefficient algorithms for volume sampling which make this technique a practical\ntool in the machine learning toolbox. Finally, we provide experimental evidence\nwhich confirms our theoretical findings.",
        "url": "http://arxiv.org/pdf/1806.01969v1.pdf"
    },
    {
        "title": "Mining for meaning: from vision to language through multiple networks consensus",
        "abstract": "Describing visual data into natural language is a very challenging task, at\nthe intersection of computer vision, natural language processing and machine\nlearning. Language goes well beyond the description of physical objects and\ntheir interactions and can convey the same abstract idea in many ways. It is\nboth about content at the highest semantic level as well as about fluent form.\nHere we propose an approach to describe videos in natural language by reaching\na consensus among multiple encoder-decoder networks. Finding such a consensual\nlinguistic description, which shares common properties with a larger group, has\na better chance to convey the correct meaning. We propose and train several\nnetwork architectures and use different types of image, audio and video\nfeatures. Each model produces its own description of the input video and the\nbest one is chosen through an efficient, two-phase consensus process. We\ndemonstrate the strength of our approach by obtaining state of the art results\non the challenging MSR-VTT dataset.",
        "url": "http://arxiv.org/pdf/1806.01954v2.pdf"
    },
    {
        "title": "Reduced-Order Modeling through Machine Learning Approaches for Brittle Fracture Applications",
        "abstract": "In this paper, five different approaches for reduced-order modeling of\nbrittle fracture in geomaterials, specifically concrete, are presented and\ncompared. Four of the five methods rely on machine learning (ML) algorithms to\napproximate important aspects of the brittle fracture problem. In addition to\nthe ML algorithms, each method incorporates different physics-based assumptions\nin order to reduce the computational complexity while maintaining the physics\nas much as possible. This work specifically focuses on using the ML approaches\nto model a 2D concrete sample under low strain rate pure tensile loading\nconditions with 20 preexisting cracks present. A high-fidelity finite\nelement-discrete element model is used to both produce a training dataset of\n150 simulations and an additional 35 simulations for validation. Results from\nthe ML approaches are directly compared against the results from the\nhigh-fidelity model. Strengths and weaknesses of each approach are discussed\nand the most important conclusion is that a combination of physics-informed and\ndata-driven features are necessary for emulating the physics of crack\npropagation, interaction and coalescence. All of the models presented here have\nruntimes that are orders of magnitude faster than the original high-fidelity\nmodel and pave the path for developing accurate reduced order models that could\nbe used to inform larger length-scale models with important sub-scale physics\nthat often cannot be accounted for due to computational cost.",
        "url": "http://arxiv.org/pdf/1806.01949v1.pdf"
    },
    {
        "title": "Supervised learning with quantum enhanced feature spaces",
        "abstract": "Machine learning and quantum computing are two technologies each with the\npotential for altering how computation is performed to address previously\nuntenable problems. Kernel methods for machine learning are ubiquitous for\npattern recognition, with support vector machines (SVMs) being the most\nwell-known method for classification problems. However, there are limitations\nto the successful solution to such problems when the feature space becomes\nlarge, and the kernel functions become computationally expensive to estimate. A\ncore element to computational speed-ups afforded by quantum algorithms is the\nexploitation of an exponentially large quantum state space through controllable\nentanglement and interference. Here, we propose and experimentally implement\ntwo novel methods on a superconducting processor. Both methods represent the\nfeature space of a classification problem by a quantum state, taking advantage\nof the large dimensionality of quantum Hilbert space to obtain an enhanced\nsolution. One method, the quantum variational classifier builds on [1,2] and\noperates through using a variational quantum circuit to classify a training set\nin direct analogy to conventional SVMs. In the second, a quantum kernel\nestimator, we estimate the kernel function and optimize the classifier\ndirectly. The two methods present a new class of tools for exploring the\napplications of noisy intermediate scale quantum computers [3] to machine\nlearning.",
        "url": "http://arxiv.org/pdf/1804.11326v2.pdf"
    },
    {
        "title": "Explainable Neural Networks based on Additive Index Models",
        "abstract": "Machine Learning algorithms are increasingly being used in recent years due\nto their flexibility in model fitting and increased predictive performance.\nHowever, the complexity of the models makes them hard for the data analyst to\ninterpret the results and explain them without additional tools. This has led\nto much research in developing various approaches to understand the model\nbehavior. In this paper, we present the Explainable Neural Network (xNN), a\nstructured neural network designed especially to learn interpretable features.\nUnlike fully connected neural networks, the features engineered by the xNN can\nbe extracted from the network in a relatively straightforward manner and the\nresults displayed. With appropriate regularization, the xNN provides a\nparsimonious explanation of the relationship between the features and the\noutput. We illustrate this interpretable feature--engineering property on\nsimulated examples.",
        "url": "http://arxiv.org/pdf/1806.01933v1.pdf"
    },
    {
        "title": "An explicit analysis of the entropic penalty in linear programming",
        "abstract": "Solving linear programs by using entropic penalization has recently attracted\nnew interest in the optimization community, since this strategy forms the basis\nfor the fastest-known algorithms for the optimal transport problem, with many\napplications in modern large-scale machine learning. Crucial to these\napplications has been an analysis of how quickly solutions to the penalized\nprogram approach true optima to the original linear program. More than 20 years\nago, Cominetti and San Mart\\'in showed that this convergence is exponentially\nfast; however, their proof is asymptotic and does not give any indication of\nhow accurately the entropic program approximates the original program for any\nparticular choice of the penalization parameter. We close this long-standing\ngap in the literature regarding entropic penalization by giving a new proof of\nthe exponential convergence, valid for any linear program. Our proof is\nnon-asymptotic, yields explicit constants, and has the virtue of being\nextremely simple. We provide matching lower bounds and show that the entropic\napproach does not lead to a near-linear time approximation scheme for the\nlinear assignment problem.",
        "url": "http://arxiv.org/pdf/1806.01879v1.pdf"
    },
    {
        "title": "A Review of Challenges and Opportunities in Machine Learning for Health",
        "abstract": "Modern electronic health records (EHRs) provide data to answer clinically meaningful questions. The growing data in EHRs makes healthcare ripe for the use of machine learning. However, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. For example, diseases in EHRs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. This article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.",
        "url": "https://arxiv.org/pdf/1806.00388v4.pdf"
    },
    {
        "title": "Stochastic Gradient Descent on Separable Data: Exact Convergence with a Fixed Learning Rate",
        "abstract": "Stochastic Gradient Descent (SGD) is a central tool in machine learning. We prove that SGD converges to zero loss, even with a fixed (non-vanishing) learning rate - in the special case of homogeneous linear classifiers with smooth monotone loss functions, optimized on linearly separable data. Previous works assumed either a vanishing learning rate, iterate averaging, or loss assumptions that do not hold for monotone loss functions used for classification, such as the logistic loss. We prove our result on a fixed dataset, both for sampling with or without replacement. Furthermore, for logistic loss (and similar exponentially-tailed losses), we prove that with SGD the weight vector converges in direction to the $L_2$ max margin vector as $O(1/\\log(t))$ for almost all separable datasets, and the loss converges as $O(1/t)$ - similarly to gradient descent. Lastly, we examine the case of a fixed learning rate proportional to the minibatch size. We prove that in this case, the asymptotic convergence rate of SGD (with replacement) does not depend on the minibatch size in terms of epochs, if the support vectors span the data. These results may suggest an explanation to similar behaviors observed in deep networks, when trained with SGD.",
        "url": "https://arxiv.org/pdf/1806.01796v3.pdf"
    },
    {
        "title": "New Hybrid Neuro-Evolutionary Algorithms for Renewable Energy and Facilities Management Problems",
        "abstract": "This Ph.D. thesis deals with the optimization of several renewable energy\nresources development as well as the improvement of facilities management in\noceanic engineering and airports, using computational hybrid methods belonging\nto AI to this end. Energy is essential to our society in order to ensure a good\nquality of life. This means that predictions over the characteristics on which\nrenewable energies depend are necessary, in order to know the amount of energy\nthat will be obtained at any time. The second topic tackled in this thesis is\nrelated to the basic parameters that influence in different marine activities\nand airports, whose knowledge is necessary to develop a proper facilities\nmanagement in these environments. Within this work, a study of the\nstate-of-the-art Machine Learning have been performed to solve the problems\nassociated with the topics above-mentioned, and several contributions have been\nproposed: One of the pillars of this work is focused on the estimation of the\nmost important parameters in the exploitation of renewable resources. The\nsecond contribution of this thesis is related to feature selection problems.\nThe proposed methodologies are applied to multiple problems: the prediction of\n$H_s$, relevant for marine energy applications and marine activities, the\nestimation of WPREs, undesirable variations in the electric power produced by a\nwind farm, the prediction of global solar radiation in areas from Spain and\nAustralia, really important in terms of solar energy, and the prediction of\nlow-visibility events at airports. All of these practical issues are developed\nwith the consequent previous data analysis, normally, in terms of\nmeteorological variables.",
        "url": "http://arxiv.org/pdf/1806.02654v1.pdf"
    },
    {
        "title": "Neural-Kernelized Conditional Density Estimation",
        "abstract": "Conditional density estimation is a general framework for solving various\nproblems in machine learning. Among existing methods, non-parametric and/or\nkernel-based methods are often difficult to use on large datasets, while\nmethods based on neural networks usually make restrictive parametric\nassumptions on the probability densities. Here, we propose a novel method for\nestimating the conditional density based on score matching. In contrast to\nexisting methods, we employ scalable neural networks, but do not make explicit\nparametric assumptions on densities. The key challenge in applying score\nmatching to neural networks is computation of the first- and second-order\nderivatives of a model for the log-density. We tackle this challenge by\ndeveloping a new neural-kernelized approach, which can be applied on large\ndatasets with stochastic gradient descent, while the reproducing kernels allow\nfor easy computation of the derivatives needed in score matching. We show that\nthe neural-kernelized function approximator has universal approximation\ncapability and that our method is consistent in conditional density estimation.\nWe numerically demonstrate that our method is useful in high-dimensional\nconditional density estimation, and compares favourably with existing methods.\nFinally, we prove that the proposed method has interesting connections to two\nprobabilistically principled frameworks of representation learning: Nonlinear\nsufficient dimension reduction and nonlinear independent component analysis.",
        "url": "http://arxiv.org/pdf/1806.01754v1.pdf"
    },
    {
        "title": "A Machine Learning Framework for Stock Selection",
        "abstract": "This paper demonstrates how to apply machine learning algorithms to\ndistinguish good stocks from the bad stocks. To this end, we construct 244\ntechnical and fundamental features to characterize each stock, and label stocks\naccording to their ranking with respect to the return-to-volatility ratio.\nAlgorithms ranging from traditional statistical learning methods to recently\npopular deep learning method, e.g. Logistic Regression (LR), Random Forest\n(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the\nclassification task. Genetic Algorithm (GA) is also used to implement feature\nselection. The effectiveness of the stock selection strategy is validated in\nChinese stock market in both statistical and practical aspects, showing that:\n1) Stacking outperforms other models reaching an AUC score of 0.972; 2) Genetic\nAlgorithm picks a subset of 114 features and the prediction performances of all\nmodels remain almost unchanged after the selection procedure, which suggests\nsome features are indeed redundant; 3) LR and DNN are radical models; RF is\nrisk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfolios\nconstructed by our models outperform market average in back tests.",
        "url": "http://arxiv.org/pdf/1806.01743v2.pdf"
    },
    {
        "title": "Machine Learning for Yield Curve Feature Extraction: Application to Illiquid Corporate Bonds (Preliminary Draft)",
        "abstract": "This paper studies the application of machine learning in extracting the\nmarket implied features from historical risk neutral corporate bond yields. We\nconsider the example of a hypothetical illiquid fixed income market. After\nchoosing a surrogate liquid market, we apply the Denoising Autoencoder\nalgorithm from the field of computer vision and pattern recognition to learn\nthe features of the missing yield parameters from the historically implied data\nof the instruments traded in the chosen liquid market. The results of the\ntrained machine learning algorithm are compared with the outputs of a point in-\ntime 2 dimensional interpolation algorithm known as the Thin Plate Spline.\nFinally, the performances of the two algorithms are compared.",
        "url": "http://arxiv.org/pdf/1806.01731v1.pdf"
    },
    {
        "title": "Scikit-learn: Machine Learning in Python",
        "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art\nmachine learning algorithms for medium-scale supervised and unsupervised\nproblems. This package focuses on bringing machine learning to non-specialists\nusing a general-purpose high-level language. Emphasis is put on ease of use,\nperformance, documentation, and API consistency. It has minimal dependencies\nand is distributed under the simplified BSD license, encouraging its use in\nboth academic and commercial settings. Source code, binaries, and documentation\ncan be downloaded from http://scikit-learn.org.",
        "url": "http://arxiv.org/pdf/1201.0490v4.pdf"
    },
    {
        "title": "A Projection Method for Metric-Constrained Optimization",
        "abstract": "We outline a new approach for solving optimization problems which enforce\ntriangle inequalities on output variables. We refer to this as\nmetric-constrained optimization, and give several examples where problems of\nthis form arise in machine learning applications and theoretical approximation\nalgorithms for graph clustering. Although these problem are interesting from a\ntheoretical perspective, they are challenging to solve in practice due to the\nhigh memory requirement of black-box solvers. In order to address this\nchallenge we first prove that the metric-constrained linear program relaxation\nof correlation clustering is equivalent to a special case of the metric\nnearness problem. We then developed a general solver for metric-constrained\nlinear and quadratic programs by generalizing and improving a simple projection\nalgorithm originally developed for metric nearness. We give several novel\napproximation guarantees for using our framework to find lower bounds for\noptimal solutions to several challenging graph clustering problems. We also\ndemonstrate the power of our framework by solving optimizing problems involving\nup to 10^{8} variables and 10^{11} constraints.",
        "url": "http://arxiv.org/pdf/1806.01678v1.pdf"
    },
    {
        "title": "Stochastic Optimization from Distributed, Streaming Data in Rate-limited Networks",
        "abstract": "Motivated by machine learning applications in networks of sensors,\ninternet-of-things (IoT) devices, and autonomous agents, we propose techniques\nfor distributed stochastic convex learning from high-rate data streams. The\nsetup involves a network of nodes---each one of which has a stream of data\narriving at a constant rate---that solve a stochastic convex optimization\nproblem by collaborating with each other over rate-limited communication links.\nTo this end, we present and analyze two algorithms---termed distributed\nstochastic approximation mirror descent (D-SAMD) and accelerated distributed\nstochastic approximation mirror descent (AD-SAMD)---that are based on two\nstochastic variants of mirror descent and in which nodes collaborate via\napproximate averaging of the local, noisy subgradients using distributed\nconsensus. Our main contributions are (i) bounds on the convergence rates of\nD-SAMD and AD-SAMD in terms of the number of nodes, network topology, and ratio\nof the data streaming and communication rates, and (ii) sufficient conditions\nfor order-optimum convergence of these algorithms. In particular, we show that\nfor sufficiently well-connected networks, distributed learning schemes can\nobtain order-optimum convergence even if the communications rate is small.\nFurther we find that the use of accelerated methods significantly enlarges the\nregime in which order-optimum convergence is achieved; this is in contrast to\nthe centralized setting, where accelerated methods usually offer only a modest\nimprovement. Finally, we demonstrate the effectiveness of the proposed\nalgorithms using numerical experiments.",
        "url": "http://arxiv.org/pdf/1704.07888v4.pdf"
    },
    {
        "title": "A Visual Quality Index for Fuzzy C-Means",
        "abstract": "Cluster analysis is widely used in the areas of machine learning and data\nmining. Fuzzy clustering is a particular method that considers that a data\npoint can belong to more than one cluster. Fuzzy clustering helps obtain\nflexible clusters, as needed in such applications as text categorization. The\nperformance of a clustering algorithm critically depends on the number of\nclusters, and estimating the optimal number of clusters is a challenging task.\nQuality indices help estimate the optimal number of clusters. However, there is\nno quality index that can obtain an accurate number of clusters for different\ndatasets. Thence, in this paper, we propose a new cluster quality index\nassociated with a visual, graph-based solution that helps choose the optimal\nnumber of clusters in fuzzy partitions. Moreover, we validate our theoretical\nresults through extensive comparison experiments against state-of-the-art\nquality indices on a variety of numerical real-world and artificial datasets.",
        "url": "http://arxiv.org/pdf/1806.01552v1.pdf"
    },
    {
        "title": "Sensitivity Analysis for Mirror-Stratifiable Convex Functions",
        "abstract": "This paper provides a set of sensitivity analysis and activity identification\nresults for a class of convex functions with a strong geometric structure, that\nwe coined \"mirror-stratifiable\". These functions are such that there is a\nbijection between a primal and a dual stratification of the space into\npartitioning sets, called strata. This pairing is crucial to track the strata\nthat are identifiable by solutions of parametrized optimization problems or by\niterates of optimization algorithms. This class of functions encompasses all\nregularizers routinely used in signal and image processing, machine learning,\nand statistics. We show that this \"mirror-stratifiable\" structure enjoys a nice\nsensitivity theory, allowing us to study stability of solutions of optimization\nproblems to small perturbations, as well as activity identification of\nfirst-order proximal splitting-type algorithms. Existing results in the\nliterature typically assume that, under a non-degeneracy condition, the active\nset associated to a minimizer is stable to small perturbations and is\nidentified in finite time by optimization schemes. In contrast, our results do\nnot require any non-degeneracy assumption: in consequence, the optimal active\nset is not necessarily stable anymore, but we are able to track precisely the\nset of identifiable strata.We show that these results have crucial implications\nwhen solving challenging ill-posed inverse problems via regularization, a\ntypical scenario where the non-degeneracy condition is not fulfilled. Our\ntheoretical results, illustrated by numerical simulations, allow to\ncharacterize the instability behaviour of the regularized solutions, by\nlocating the set of all low-dimensional strata that can be potentially\nidentified by these solutions.",
        "url": "http://arxiv.org/pdf/1707.03194v3.pdf"
    },
    {
        "title": "Semi-Supervised Clustering with Neural Networks",
        "abstract": "Clustering using neural networks has recently demonstrated promising\nperformance in machine learning and computer vision applications. However, the\nperformance of current approaches is limited either by unsupervised learning or\ntheir dependence on large set of labeled data samples. In this paper, we\npropose ClusterNet that uses pairwise semantic constraints from very few\nlabeled data samples (<5% of total data) and exploits the abundant unlabeled\ndata to drive the clustering approach. We define a new loss function that uses\npairwise semantic similarity between objects combined with constrained k-means\nclustering to efficiently utilize both labeled and unlabeled data in the same\nframework. The proposed network uses convolution autoencoder to learn a latent\nrepresentation that groups data into k specified clusters, while also learning\nthe cluster centers simultaneously. We evaluate and compare the performance of\nClusterNet on several datasets and state of the art deep clustering approaches.",
        "url": "http://arxiv.org/pdf/1806.01547v2.pdf"
    },
    {
        "title": "SEGEN: Sample-Ensemble Genetic Evolutional Network Model",
        "abstract": "Deep learning, a rebranding of deep neural network research works, has\nachieved a remarkable success in recent years. With multiple hidden layers,\ndeep learning models aim at computing the hierarchical feature representations\nof the observational data. Meanwhile, due to its severe disadvantages in data\nconsumption, computational resources, parameter tuning costs and the lack of\nresult explainability, deep learning has also suffered from lots of criticism.\nIn this paper, we will introduce a new representation learning model, namely\n\"Sample-Ensemble Genetic Evolutionary Network\" (SEGEN), which can serve as an\nalternative approach to deep learning models. Instead of building one single\ndeep model, based on a set of sampled sub-instances, SEGEN adopts a\ngenetic-evolutionary learning strategy to build a group of unit models\ngenerations by generations. The unit models incorporated in SEGEN can be either\ntraditional machine learning models or the recent deep learning models with a\nmuch \"narrower\" and \"shallower\" architecture. The learning results of each\ninstance at the final generation will be effectively combined from each unit\nmodel via diffusive propagation and ensemble learning strategies. From the\ncomputational perspective, SEGEN requires far less data, fewer computational\nresources and parameter tuning efforts, but has sound theoretic\ninterpretability of the learning process and results. Extensive experiments\nhave been done on several different real-world benchmark datasets, and the\nexperimental results obtained by SEGEN have demonstrated its advantages over\nthe state-of-the-art representation learning models.",
        "url": "http://arxiv.org/pdf/1803.08631v2.pdf"
    },
    {
        "title": "Deep Learning for IoT Big Data and Streaming Analytics: A Survey",
        "abstract": "In the era of the Internet of Things (IoT), an enormous amount of sensing\ndevices collect and/or generate various sensory data over time for a wide range\nof fields and applications. Based on the nature of the application, these\ndevices will result in big or fast/real-time data streams. Applying analytics\nover such data streams to discover new information, predict future insights,\nand make control decisions is a crucial process that makes IoT a worthy\nparadigm for businesses and a quality-of-life improving technology. In this\npaper, we provide a thorough overview on using a class of advanced machine\nlearning techniques, namely Deep Learning (DL), to facilitate the analytics and\nlearning in the IoT domain. We start by articulating IoT data characteristics\nand identifying two major treatments for IoT data from a machine learning\nperspective, namely IoT big data analytics and IoT streaming data analytics. We\nalso discuss why DL is a promising approach to achieve the desired analytics in\nthese types of data and applications. The potential of using emerging DL\ntechniques for IoT data analytics are then discussed, and its promises and\nchallenges are introduced. We present a comprehensive background on different\nDL architectures and algorithms. We also analyze and summarize major reported\nresearch attempts that leveraged DL in the IoT domain. The smart IoT devices\nthat have incorporated DL in their intelligence background are also discussed.\nDL implementation approaches on the fog and cloud centers in support of IoT\napplications are also surveyed. Finally, we shed light on some challenges and\npotential directions for future research. At the end of each section, we\nhighlight the lessons learned based on our experiments and review of the recent\nliterature.",
        "url": "http://arxiv.org/pdf/1712.04301v2.pdf"
    },
    {
        "title": "Graph-based regularization for regression problems with alignment and highly-correlated designs",
        "abstract": "Sparse models for high-dimensional linear regression and machine learning have received substantial attention over the past two decades. Model selection, or determining which features or covariates are the best explanatory variables, is critical to the interpretability of a learned model. Much of the current literature assumes that covariates are only mildly correlated. However, in many modern applications covariates are highly correlated and do not exhibit key properties (such as the restricted eigenvalue condition, restricted isometry property, or other related assumptions). This work considers a high-dimensional regression setting in which a graph governs both correlations among the covariates and the similarity among regression coefficients -- meaning there is \\emph{alignment} between the covariates and regression coefficients. Using side information about the strength of correlations among features, we form a graph with edge weights corresponding to pairwise covariances. This graph is used to define a graph total variation regularizer that promotes similar weights for correlated features. This work shows how the proposed graph-based regularization yields mean-squared error guarantees for a broad range of covariance graph structures. These guarantees are optimal for many specific covariance graphs, including block and lattice graphs. Our proposed approach outperforms other methods for highly-correlated design in a variety of experiments on synthetic data and real biochemistry data.",
        "url": "https://arxiv.org/pdf/1803.07658v3.pdf"
    },
    {
        "title": "Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark",
        "abstract": "Researchers have proposed hardware, software, and algorithmic optimizations to improve the computational performance of deep learning. While some of these optimizations perform the same operations faster (e.g., increasing GPU clock speed), many others modify the semantics of the training procedure (e.g., reduced precision), and can impact the final model's accuracy on unseen data. Due to a lack of standard evaluation criteria that considers these trade-offs, it is difficult to directly compare these optimizations. To address this problem, we recently introduced DAWNBench, a benchmark competition focused on end-to-end training time to achieve near-state-of-the-art accuracy on an unseen dataset---a combined metric called time-to-accuracy (TTA). In this work, we analyze the entries from DAWNBench, which received optimized submissions from multiple industrial groups, to investigate the behavior of TTA as a metric as well as trends in the best-performing entries. We show that TTA has a low coefficient of variation and that models optimized for TTA generalize nearly as well as those trained using standard methods. Additionally, even though DAWNBench entries were able to train ImageNet models in under 3 minutes, we find they still underutilize hardware capabilities such as Tensor Cores. Furthermore, we find that distributed entries can spend more than half of their time on communication. We show similar findings with entries to the MLPERF v0.5 benchmark.",
        "url": "https://arxiv.org/pdf/1806.01427v2.pdf"
    },
    {
        "title": "BindsNET: A machine learning-oriented spiking neural networks library in Python",
        "abstract": "The development of spiking neural network simulation software is a critical\ncomponent enabling the modeling of neural systems and the development of\nbiologically inspired algorithms. Existing software frameworks support a wide\nrange of neural functionality, software abstraction levels, and hardware\ndevices, yet are typically not suitable for rapid prototyping or application to\nproblems in the domain of machine learning. In this paper, we describe a new\nPython package for the simulation of spiking neural networks, specifically\ngeared towards machine learning and reinforcement learning. Our software,\ncalled BindsNET, enables rapid building and simulation of spiking networks and\nfeatures user-friendly, concise syntax. BindsNET is built on top of the PyTorch\ndeep neural networks library, enabling fast CPU and GPU computation for large\nspiking networks. The BindsNET framework can be adjusted to meet the needs of\nother existing computing and hardware environments, e.g., TensorFlow. We also\nprovide an interface into the OpenAI gym library, allowing for training and\nevaluation of spiking networks on reinforcement learning problems. We argue\nthat this package facilitates the use of spiking networks for large-scale\nmachine learning experimentation, and show some simple examples of how we\nenvision BindsNET can be used in practice. BindsNET code is available at\nhttps://github.com/Hananel-Hazan/bindsnet",
        "url": "http://arxiv.org/pdf/1806.01423v2.pdf"
    },
    {
        "title": "Less is More: Simultaneous View Classification and Landmark Detection for Abdominal Ultrasound Images",
        "abstract": "An abdominal ultrasound examination, which is the most common ultrasound\nexamination, requires substantial manual efforts to acquire standard abdominal\norgan views, annotate the views in texts, and record clinically relevant organ\nmeasurements. Hence, automatic view classification and landmark detection of\nthe organs can be instrumental to streamline the examination workflow. However,\nthis is a challenging problem given not only the inherent difficulties from the\nultrasound modality, e.g., low contrast and large variations, but also the\nheterogeneity across tasks, i.e., one classification task for all views, and\nthen one landmark detection task for each relevant view. While convolutional\nneural networks (CNN) have demonstrated more promising outcomes on ultrasound\nimage analytics than traditional machine learning approaches, it becomes\nimpractical to deploy multiple networks (one for each task) due to the limited\ncomputational and memory resources on most existing ultrasound scanners. To\novercome such limits, we propose a multi-task learning framework to handle all\nthe tasks by a single network. This network is integrated to perform view\nclassification and landmark detection simultaneously; it is also equipped with\nglobal convolutional kernels, coordinate constraints, and a conditional\nadversarial module to leverage the performances. In an experimental study based\non 187,219 ultrasound images, with the proposed simplified approach we achieve\n(1) view classification accuracy better than the agreement between two clinical\nexperts and (2) landmark-based measurement errors on par with inter-user\nvariability. The multi-task approach also benefits from sharing the feature\nextraction during the training process across all tasks and, as a result,\noutperforms the approaches that address each task individually.",
        "url": "http://arxiv.org/pdf/1805.10376v2.pdf"
    },
    {
        "title": "Learning a Code: Machine Learning for Approximate Non-Linear Coded Computation",
        "abstract": "Machine learning algorithms are typically run on large scale, distributed\ncompute infrastructure that routinely face a number of unavailabilities such as\nfailures and temporary slowdowns. Adding redundant computations using\ncoding-theoretic tools called \"codes\" is an emerging technique to alleviate the\nadverse effects of such unavailabilities. A code consists of an encoding\nfunction that proactively introduces redundant computation and a decoding\nfunction that reconstructs unavailable outputs using the available ones. Past\nwork focuses on using codes to provide resilience for linear computations and\nspecific iterative optimization algorithms. However, computations performed for\na variety of applications including inference on state-of-the-art machine\nlearning algorithms, such as neural networks, typically fall outside this\nrealm. In this paper, we propose taking a learning-based approach to designing\ncodes that can handle non-linear computations. We present carefully designed\nneural network architectures and a training methodology for learning encoding\nand decoding functions that produce approximate reconstructions of unavailable\ncomputation results. We present extensive experimental results demonstrating\nthe effectiveness of the proposed approach: we show that the our learned codes\ncan accurately reconstruct $64 - 98\\%$ of the unavailable predictions from\nneural-network based image classifiers on the MNIST, Fashion-MNIST, and\nCIFAR-10 datasets. To the best of our knowledge, this work proposes the first\nlearning-based approach for designing codes, and also presents the first\ncoding-theoretic solution that can provide resilience for any non-linear\n(differentiable) computation. Our results show that learning can be an\neffective technique for designing codes, and that learned codes are a highly\npromising approach for bringing the benefits of coding to non-linear\ncomputations.",
        "url": "http://arxiv.org/pdf/1806.01259v1.pdf"
    },
    {
        "title": "Agreement-based Learning",
        "abstract": "Model selection is a problem that has occupied machine learning researchers\nfor a long time. Recently, its importance has become evident through\napplications in deep learning. We propose an agreement-based learning framework\nthat prevents many of the pitfalls associated with model selection. It relies\non coupling the training of multiple models by encouraging them to agree on\ntheir predictions while training. In contrast with other model selection and\ncombination approaches used in machine learning, the proposed framework is\ninspired by human learning. We also propose a learning algorithm defined within\nthis framework which manages to significantly outperform alternatives in\npractice, and whose performance improves further with the availability of\nunlabeled data. Finally, we describe a number of potential directions for\ndeveloping more flexible agreement-based learning algorithms.",
        "url": "http://arxiv.org/pdf/1806.01258v1.pdf"
    },
    {
        "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
        "abstract": "Machine learning (ML) has become a core component of many real-world\napplications and training data is a key factor that drives current progress.\nThis huge success has led Internet companies to deploy machine learning as a\nservice (MLaaS). Recently, the first membership inference attack has shown that\nextraction of information on the training set is possible in such MLaaS\nsettings, which has severe security and privacy implications.\n  However, the early demonstrations of the feasibility of such attacks have\nmany assumptions on the adversary, such as using multiple so-called shadow\nmodels, knowledge of the target model structure, and having a dataset from the\nsame distribution as the target model's training data. We relax all these key\nassumptions, thereby showing that such attacks are very broadly applicable at\nlow cost and thereby pose a more severe risk than previously thought. We\npresent the most comprehensive study so far on this emerging and developing\nthreat using eight diverse datasets which show the viability of the proposed\nattacks across domains.\n  In addition, we propose the first effective defense mechanisms against such\nbroader class of membership inference attacks that maintain a high level of\nutility of the ML model.",
        "url": "http://arxiv.org/pdf/1806.01246v2.pdf"
    },
    {
        "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning",
        "abstract": "There has recently been a surge of work in explanatory artificial\nintelligence (XAI). This research area tackles the important problem that\ncomplex machines and algorithms often cannot provide insights into their\nbehavior and thought processes. XAI allows users and parts of the internal\nsystem to be more transparent, providing explanations of their decisions in\nsome level of detail. These explanations are important to ensure algorithmic\nfairness, identify potential bias/problems in the training data, and to ensure\nthat the algorithms perform as expected. However, explanations produced by\nthese systems is neither standardized nor systematically assessed. In an effort\nto create best practices and identify open challenges, we provide our\ndefinition of explainability and show how it can be used to classify existing\nliterature. We discuss why current approaches to explanatory methods especially\nfor deep neural networks are insufficient. Finally, based on our survey, we\nconclude with suggested future research directions for explanatory artificial\nintelligence.",
        "url": "http://arxiv.org/pdf/1806.00069v3.pdf"
    },
    {
        "title": "Learning from Exemplars and Prototypes in Machine Learning and Psychology",
        "abstract": "This paper draws a parallel between similarity-based categorisation models\ndeveloped in cognitive psychology and the nearest neighbour classifier (1-NN)\nin machine learning. Conceived as a result of the historical rivalry between\nprototype theories (abstraction) and exemplar theories (memorisation), recent\nmodels of human categorisation seek a compromise in-between. Regarding the\nstimuli (entities to be categorised) as points in a metric space, machine\nlearning offers a large collection of methods to select a small, representative\nand discriminative point set. These methods are known under various names:\ninstance selection, data editing, prototype selection, prototype generation or\nprototype replacement. The nearest neighbour classifier is used with the\nselected reference set. Such a set can be interpreted as a data-driven\ncategorisation model. We juxtapose the models from the two fields to enable\ncross-referencing. We believe that both machine learning and cognitive\npsychology can draw inspiration from the comparison and enrich their repertoire\nof similarity-based models.",
        "url": "http://arxiv.org/pdf/1806.01130v1.pdf"
    },
    {
        "title": "A Comparison of Machine Learning Algorithms for the Surveillance of Autism Spectrum Disorder",
        "abstract": "The Centers for Disease Control and Prevention (CDC) coordinates a\nlabor-intensive process to measure the prevalence of autism spectrum disorder\n(ASD) among children in the United States. Random forests methods have shown\npromise in speeding up this process, but they lag behind human classification\naccuracy by about 5%. We explore whether more recently available document\nclassification algorithms can close this gap. We applied 8 supervised learning\nalgorithms to predict whether children meet the case definition for ASD based\nsolely on the words in their evaluations. We compared the algorithms'\nperformance across 10 random train-test splits of the data, using\nclassification accuracy, F1 score, and number of positive calls to evaluate\ntheir potential use for surveillance. Across the 10 train-test cycles, the\nrandom forest and support vector machine with Naive Bayes features (NB-SVM)\neach achieved slightly more than 87% mean accuracy. The NB-SVM produced\nsignificantly more false negatives than false positives (P = 0.027), but the\nrandom forest did not, making its prevalence estimates very close to the true\nprevalence in the data. The best-performing neural network performed similarly\nto the random forest on both measures. The random forest performed as well as\nmore recently available models like the NB-SVM and the neural network, and it\nalso produced good prevalence estimates. NB-SVM may not be a good candidate for\nuse in a fully-automated surveillance workflow due to increased false\nnegatives. More sophisticated algorithms, like hierarchical convolutional\nneural networks, may not be feasible to train due to characteristics of the\ndata. Current algorithms might perform better if the data are abstracted and\nprocessed differently and if they take into account information about the\nchildren in addition to their evaluations.",
        "url": "http://arxiv.org/pdf/1804.06223v3.pdf"
    },
    {
        "title": "iFair: Learning Individually Fair Data Representations for Algorithmic Decision Making",
        "abstract": "People are rated and ranked, towards algorithmic decision making in an\nincreasing number of applications, typically based on machine learning.\nResearch on how to incorporate fairness into such tasks has prevalently pursued\nthe paradigm of group fairness: giving adequate success rates to specifically\nprotected groups. In contrast, the alternative paradigm of individual fairness\nhas received relatively little attention, and this paper advances this less\nexplored direction. The paper introduces a method for probabilistically mapping\nuser records into a low-rank representation that reconciles individual fairness\nand the utility of classifiers and rankings in downstream applications. Our\nnotion of individual fairness requires that users who are similar in all\ntask-relevant attributes such as job qualification, and disregarding all\npotentially discriminating attributes such as gender, should have similar\noutcomes. We demonstrate the versatility of our method by applying it to\nclassification and learning-to-rank tasks on a variety of real-world datasets.\nOur experiments show substantial improvements over the best prior work for this\nsetting.",
        "url": "http://arxiv.org/pdf/1806.01059v2.pdf"
    },
    {
        "title": "Similarity encoding for learning with dirty categorical variables",
        "abstract": "For statistical learning, categorical variables in a table are usually\nconsidered as discrete entities and encoded separately to feature vectors,\ne.g., with one-hot encoding. \"Dirty\" non-curated data gives rise to categorical\nvariables with a very high cardinality but redundancy: several categories\nreflect the same entity. In databases, this issue is typically solved with a\ndeduplication step. We show that a simple approach that exposes the redundancy\nto the learning algorithm brings significant gains. We study a generalization\nof one-hot encoding, similarity encoding, that builds feature vectors from\nsimilarities across categories. We perform a thorough empirical validation on\nnon-curated tables, a problem seldom studied in machine learning. Results on\nseven real-world datasets show that similarity encoding brings significant\ngains in prediction in comparison with known encoding methods for categories or\nstrings, notably one-hot encoding and bag of character n-grams. We draw\npractical recommendations for encoding dirty categories: 3-gram similarity\nappears to be a good choice to capture morphological resemblance. For very\nhigh-cardinality, dimensionality reduction significantly reduces the\ncomputational cost with little loss in performance: random projections or\nchoosing a subset of prototype categories still outperforms classic encoding\napproaches.",
        "url": "http://arxiv.org/pdf/1806.00979v1.pdf"
    },
    {
        "title": "Techniques for proving Asynchronous Convergence results for Markov Chain Monte Carlo methods",
        "abstract": "Markov Chain Monte Carlo (MCMC) methods such as Gibbs sampling are finding\nwidespread use in applied statistics and machine learning. These often lead to\ndifficult computational problems, which are increasingly being solved on\nparallel and distributed systems such as compute clusters. Recent work has\nproposed running iterative algorithms such as gradient descent and MCMC in\nparallel asynchronously for increased performance, with good empirical results\nin certain problems. Unfortunately, for MCMC this parallelization technique\nrequires new convergence theory, as it has been explicitly demonstrated to lead\nto divergence on some examples. Recent theory on Asynchronous Gibbs sampling\ndescribes why these algorithms can fail, and provides a way to alter them to\nmake them converge. In this article, we describe how to apply this theory in a\ngeneric setting, to understand the asynchronous behavior of any MCMC algorithm,\nincluding those implemented using parameter servers, and those not based on\nGibbs sampling.",
        "url": "http://arxiv.org/pdf/1711.06719v5.pdf"
    },
    {
        "title": "Deploying Customized Data Representation and Approximate Computing in Machine Learning Applications",
        "abstract": "Major advancements in building general-purpose and customized hardware have\nbeen one of the key enablers of versatility and pervasiveness of machine\nlearning models such as deep neural networks. To sustain this ubiquitous\ndeployment of machine learning models and cope with their computational and\nstorage complexity, several solutions such as low-precision representation of\nmodel parameters using fixed-point representation and deploying approximate\narithmetic operations have been employed. Studying the potency of such\nsolutions in different applications requires integrating them into existing\nmachine learning frameworks for high-level simulations as well as implementing\nthem in hardware to analyze their effects on power/energy dissipation,\nthroughput, and chip area. Lop is a library for design space exploration that\nbridges the gap between machine learning and efficient hardware realization. It\ncomprises a Python module, which can be integrated with some of the existing\nmachine learning frameworks and implements various customizable data\nrepresentations including fixed-point and floating-point as well as approximate\narithmetic operations.Furthermore, it includes a highly-parameterized Scala\nmodule, which allows synthesizing hardware based on the said data\nrepresentations and arithmetic operations. Lop allows researchers and designers\nto quickly compare quality of their models using various data representations\nand arithmetic operations in Python and contrast the hardware cost of viable\nrepresentations by synthesizing them on their target platforms (e.g., FPGA or\nASIC). To the best of our knowledge, Lop is the first library that allows both\nsoftware simulation and hardware realization using customized data\nrepresentations and approximate computing techniques.",
        "url": "http://arxiv.org/pdf/1806.00875v1.pdf"
    },
    {
        "title": "Learning graphs from data: A signal representation perspective",
        "abstract": "The construction of a meaningful graph topology plays a crucial role in the effective representation, processing, analysis and visualization of structured data. When a natural choice of the graph is not readily available from the data sets, it is thus desirable to infer or learn a graph topology from the data. In this tutorial overview, we survey solutions to the problem of graph learning, including classical viewpoints from statistics and physics, and more recent approaches that adopt a graph signal processing (GSP) perspective. We further emphasize the conceptual similarities and differences between classical and GSP-based graph inference methods, and highlight the potential advantage of the latter in a number of theoretical and practical scenarios. We conclude with several open issues and challenges that are keys to the design of future signal processing and machine learning algorithms for learning graphs from data.",
        "url": "https://arxiv.org/pdf/1806.00848v3.pdf"
    },
    {
        "title": "Locally Interpretable Models and Effects based on Supervised Partitioning (LIME-SUP)",
        "abstract": "Supervised Machine Learning (SML) algorithms such as Gradient Boosting,\nRandom Forest, and Neural Networks have become popular in recent years due to\ntheir increased predictive performance over traditional statistical methods.\nThis is especially true with large data sets (millions or more observations and\nhundreds to thousands of predictors). However, the complexity of the SML models\nmakes them opaque and hard to interpret without additional tools. There has\nbeen a lot of interest recently in developing global and local diagnostics for\ninterpreting and explaining SML models. In this paper, we propose locally\ninterpretable models and effects based on supervised partitioning (trees)\nreferred to as LIME-SUP. This is in contrast with the KLIME approach that is\nbased on clustering the predictor space. We describe LIME-SUP based on fitting\ntrees to the fitted response (LIM-SUP-R) as well as the derivatives of the\nfitted response (LIME-SUP-D). We compare the results with KLIME and describe\nits advantages using simulation and real data.",
        "url": "http://arxiv.org/pdf/1806.00663v1.pdf"
    },
    {
        "title": "GamePad: A Learning Environment for Theorem Proving",
        "abstract": "In this paper, we introduce a system called GamePad that can be used to\nexplore the application of machine learning methods to theorem proving in the\nCoq proof assistant. Interactive theorem provers such as Coq enable users to\nconstruct machine-checkable proofs in a step-by-step manner. Hence, they\nprovide an opportunity to explore theorem proving with human supervision. We\nuse GamePad to synthesize proofs for a simple algebraic rewrite problem and\ntrain baseline models for a formalization of the Feit-Thompson theorem. We\naddress position evaluation (i.e., predict the number of proof steps left) and\ntactic prediction (i.e., predict the next proof step) tasks, which arise\nnaturally in tactic-based theorem proving.",
        "url": "http://arxiv.org/pdf/1806.00608v2.pdf"
    },
    {
        "title": "Autoencoders Learn Generative Linear Models",
        "abstract": "We provide a series of results for unsupervised learning with autoencoders.\nSpecifically, we study shallow two-layer autoencoder architectures with shared\nweights. We focus on three generative models for data that are common in\nstatistical machine learning: (i) the mixture-of-gaussians model, (ii) the\nsparse coding model, and (iii) the sparsity model with non-negative\ncoefficients. For each of these models, we prove that under suitable choices of\nhyperparameters, architectures, and initialization, autoencoders learned by\ngradient descent can successfully recover the parameters of the corresponding\nmodel. To our knowledge, this is the first result that rigorously studies the\ndynamics of gradient descent for weight-sharing autoencoders. Our analysis can\nbe viewed as theoretical evidence that shallow autoencoder modules indeed can\nbe used as feature learning mechanisms for a variety of data models, and may\nshed insight on how to train larger stacked architectures with autoencoders as\nbasic building blocks.",
        "url": "http://arxiv.org/pdf/1806.00572v3.pdf"
    },
    {
        "title": "Bayesian approach to model-based extrapolation of nuclear observables",
        "abstract": "The mass, or binding energy, is the basis property of the atomic nucleus. It\ndetermines its stability, and reaction and decay rates. Quantifying the nuclear\nbinding is important for understanding the origin of elements in the universe.\nThe astrophysical processes responsible for the nucleosynthesis in stars often\ntake place far from the valley of stability, where experimental masses are not\nknown. In such cases, missing nuclear information must be provided by\ntheoretical predictions using extreme extrapolations. Bayesian machine learning\ntechniques can be applied to improve predictions by taking full advantage of\nthe information contained in the deviations between experimental and calculated\nmasses. We consider 10 global models based on nuclear Density Functional Theory\nas well as two more phenomenological mass models. The emulators of S2n\nresiduals and credibility intervals defining theoretical error bars are\nconstructed using Bayesian Gaussian processes and Bayesian neural networks. We\nconsider a large training dataset pertaining to nuclei whose masses were\nmeasured before 2003. For the testing datasets, we considered those exotic\nnuclei whose masses have been determined after 2003. We then carried out\nextrapolations towards the 2n dripline. While both Gaussian processes and\nBayesian neural networks reduce the rms deviation from experiment\nsignificantly, GP offers a better and much more stable performance. The\nincrease in the predictive power is quite astonishing: the resulting rms\ndeviations from experiment on the testing dataset are similar to those of more\nphenomenological models. The empirical coverage probability curves we obtain\nmatch very well the reference values which is highly desirable to ensure\nhonesty of uncertainty quantification, and the estimated credibility intervals\non predictions make it possible to evaluate predictive power of individual\nmodels.",
        "url": "http://arxiv.org/pdf/1806.00552v3.pdf"
    },
    {
        "title": "Do CIFAR-10 Classifiers Generalize to CIFAR-10?",
        "abstract": "Machine learning is currently dominated by largely experimental work focused\non improvements in a few key tasks. However, the impressive accuracy numbers of\nthe best performing models are questionable because the same test sets have\nbeen used to select these models for multiple years now. To understand the\ndanger of overfitting, we measure the accuracy of CIFAR-10 classifiers by\ncreating a new test set of truly unseen images. Although we ensure that the new\ntest set is as close to the original data distribution as possible, we find a\nlarge drop in accuracy (4% to 10%) for a broad range of deep learning models.\nYet more recent models with higher original accuracy show a smaller drop and\nbetter overall performance, indicating that this drop is likely not due to\noverfitting based on adaptivity. Instead, we view our results as evidence that\ncurrent accuracy numbers are brittle and susceptible to even minute natural\nvariations in the data distribution.",
        "url": "http://arxiv.org/pdf/1806.00451v1.pdf"
    },
    {
        "title": "A Guide to Constraining Effective Field Theories with Machine Learning",
        "abstract": "We develop, discuss, and compare several inference techniques to constrain\ntheory parameters in collider experiments. By harnessing the latent-space\nstructure of particle physics processes, we extract extra information from the\nsimulator. This augmented data can be used to train neural networks that\nprecisely estimate the likelihood ratio. The new methods scale well to many\nobservables and high-dimensional parameter spaces, do not require any\napproximations of the parton shower and detector response, and can be evaluated\nin microseconds. Using weak-boson-fusion Higgs production as an example\nprocess, we compare the performance of several techniques. The best results are\nfound for likelihood ratio estimators trained with extra information about the\nscore, the gradient of the log likelihood function with respect to the theory\nparameters. The score also provides sufficient statistics that contain all the\ninformation needed for inference in the neighborhood of the Standard Model.\nThese methods enable us to put significantly stronger bounds on effective\ndimension-six operators than the traditional approach based on histograms. They\nalso outperform generic machine learning methods that do not make use of the\nparticle physics structure, demonstrating their potential to substantially\nimprove the new physics reach of the LHC legacy results.",
        "url": "http://arxiv.org/pdf/1805.00020v4.pdf"
    },
    {
        "title": "Constraining Effective Field Theories with Machine Learning",
        "abstract": "We present powerful new analysis techniques to constrain effective field\ntheories at the LHC. By leveraging the structure of particle physics processes,\nwe extract extra information from Monte-Carlo simulations, which can be used to\ntrain neural network models that estimate the likelihood ratio. These methods\nscale well to processes with many observables and theory parameters, do not\nrequire any approximations of the parton shower or detector response, and can\nbe evaluated in microseconds. We show that they allow us to put significantly\nstronger bounds on dimension-six operators than existing methods, demonstrating\ntheir potential to improve the precision of the LHC legacy constraints.",
        "url": "http://arxiv.org/pdf/1805.00013v4.pdf"
    },
    {
        "title": "Domain Adaptation for MRI Organ Segmentation using Reverse Classification Accuracy",
        "abstract": "The variations in multi-center data in medical imaging studies have brought\nthe necessity of domain adaptation. Despite the advancement of machine learning\nin automatic segmentation, performance often degrades when algorithms are\napplied on new data acquired from different scanners or sequences than the\ntraining data. Manual annotation is costly and time consuming if it has to be\ncarried out for every new target domain. In this work, we investigate automatic\nselection of suitable subjects to be annotated for supervised domain adaptation\nusing the concept of reverse classification accuracy (RCA). RCA predicts the\nperformance of a trained model on data from the new domain and different\nstrategies of selecting subjects to be included in the adaptation via transfer\nlearning are evaluated. We perform experiments on a two-center MR database for\nthe task of organ segmentation. We show that subject selection via RCA can\nreduce the burden of annotation of new data for the target domain.",
        "url": "http://arxiv.org/pdf/1806.00363v1.pdf"
    },
    {
        "title": "Natural Language Generation for Electronic Health Records",
        "abstract": "A variety of methods existing for generating synthetic electronic health\nrecords (EHRs), but they are not capable of generating unstructured text, like\nemergency department (ED) chief complaints, history of present illness or\nprogress notes. Here, we use the encoder-decoder model, a deep learning\nalgorithm that features in many contemporary machine translation systems, to\ngenerate synthetic chief complaints from discrete variables in EHRs, like age\ngroup, gender, and discharge diagnosis. After being trained end-to-end on\nauthentic records, the model can generate realistic chief complaint text that\npreserves much of the epidemiological information in the original data. As a\nside effect of the model's optimization goal, these synthetic chief complaints\nare also free of relatively uncommon abbreviation and misspellings, and they\ninclude none of the personally-identifiable information (PII) that was in the\ntraining data, suggesting it may be used to support the de-identification of\ntext in EHRs. When combined with algorithms like generative adversarial\nnetworks (GANs), our model could be used to generate fully-synthetic EHRs,\nfacilitating data sharing between healthcare providers and researchers and\nimproving our ability to develop machine learning methods tailored to the\ninformation in healthcare data.",
        "url": "http://arxiv.org/pdf/1806.01353v1.pdf"
    },
    {
        "title": "Deep Predictive Models in Interactive Music",
        "abstract": "Musical performance requires prediction to operate instruments, to perform in\ngroups and to improvise. In this paper, we investigate how a number of digital\nmusical instruments (DMIs), including two of our own, have applied predictive\nmachine learning models that assist users by predicting unknown states of\nmusical processes. We characterise these predictions as focussed within a\nmusical instrument, at the level of individual performers, and between members\nof an ensemble. These models can connect to existing frameworks for DMI design\nand have parallels in the cognitive predictions of human musicians.\n  We discuss how recent advances in deep learning highlight the role of\nprediction in DMIs, by allowing data-driven predictive models with a long\nmemory of past states. The systems we review are used to motivate musical\nuse-cases where prediction is a necessary component, and to highlight a number\nof challenges for DMI designers seeking to apply deep predictive models in\ninteractive music systems of the future.",
        "url": "http://arxiv.org/pdf/1801.10492v3.pdf"
    },
    {
        "title": "NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm",
        "abstract": "Deep neural networks (DNNs) have begun to have a pervasive impact on various\napplications of machine learning. However, the problem of finding an optimal\nDNN architecture for large applications is challenging. Common approaches go\nfor deeper and larger DNN architectures but may incur substantial redundancy.\nTo address these problems, we introduce a network growth algorithm that\ncomplements network pruning to learn both weights and compact DNN architectures\nduring training. We propose a DNN synthesis tool (NeST) that combines both\nmethods to automate the generation of compact and accurate DNNs. NeST starts\nwith a randomly initialized sparse network called the seed architecture. It\niteratively tunes the architecture with gradient-based growth and\nmagnitude-based pruning of neurons and connections. Our experimental results\nshow that NeST yields accurate, yet very compact DNNs, with a wide range of\nseed architecture selection. For the LeNet-300-100 (LeNet-5) architecture, we\nreduce network parameters by 70.2x (74.3x) and floating-point operations\n(FLOPs) by 79.4x (43.7x). For the AlexNet and VGG-16 architectures, we reduce\nnetwork parameters (FLOPs) by 15.7x (4.6x) and 30.2x (8.6x), respectively.\nNeST's grow-and-prune paradigm delivers significant additional parameter and\nFLOPs reduction relative to pruning-only methods.",
        "url": "http://arxiv.org/pdf/1711.02017v3.pdf"
    },
    {
        "title": "Neural Control Variates for Variance Reduction",
        "abstract": "In statistics and machine learning, approximation of an intractable integration is often achieved by using the unbiased Monte Carlo estimator, but the variances of the estimation are generally high in many applications. Control variates approaches are well-known to reduce the variance of the estimation. These control variates are typically constructed by employing predefined parametric functions or polynomials, determined by using those samples drawn from the relevant distributions. Instead, we propose to construct those control variates by learning neural networks to handle the cases when test functions are complex. In many applications, obtaining a large number of samples for Monte Carlo estimation is expensive, which may result in overfitting when training a neural network. We thus further propose to employ auxiliary random variables induced by the original ones to extend data samples for training the neural networks. We apply the proposed control variates with augmented variables to thermodynamic integration and reinforcement learning. Experimental results demonstrate that our method can achieve significant variance reduction compared with other alternatives.",
        "url": "https://arxiv.org/pdf/1806.00159v2.pdf"
    },
    {
        "title": "Interpreting Deep Learning: The Machine Learning Rorschach Test?",
        "abstract": "Theoretical understanding of deep learning is one of the most important tasks\nfacing the statistics and machine learning communities. While deep neural\nnetworks (DNNs) originated as engineering methods and models of biological\nnetworks in neuroscience and psychology, they have quickly become a centerpiece\nof the machine learning toolbox. Unfortunately, DNN adoption powered by recent\nsuccesses combined with the open-source nature of the machine learning\ncommunity, has outpaced our theoretical understanding. We cannot reliably\nidentify when and why DNNs will make mistakes. In some applications like text\ntranslation these mistakes may be comical and provide for fun fodder in\nresearch talks, a single error can be very costly in tasks like medical\nimaging. As we utilize DNNs in increasingly sensitive applications, a better\nunderstanding of their properties is thus imperative. Recent advances in DNN\ntheory are numerous and include many different sources of intuition, such as\nlearning theory, sparse signal analysis, physics, chemistry, and psychology. An\ninteresting pattern begins to emerge in the breadth of possible\ninterpretations. The seemingly limitless approaches are mostly constrained by\nthe lens with which the mathematical operations are viewed. Ultimately, the\ninterpretation of DNNs appears to mimic a type of Rorschach test --- a\npsychological test wherein subjects interpret a series of seemingly ambiguous\nink-blots. Validation for DNN theory requires a convergence of the literature.\nWe must distinguish between universal results that are invariant to the\nanalysis perspective and those that are specific to a particular network\nconfiguration. Simultaneously we must deal with the fact that many standard\nstatistical tools for quantifying generalization or empirically assessing\nimportant network features are difficult to apply to DNNs.",
        "url": "http://arxiv.org/pdf/1806.00148v1.pdf"
    },
    {
        "title": "On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes",
        "abstract": "Stochastic gradient descent is the method of choice for large scale\noptimization of machine learning objective functions. Yet, its performance is\ngreatly variable and heavily depends on the choice of the stepsizes. This has\nmotivated a large body of research on adaptive stepsizes. However, there is\ncurrently a gap in our theoretical understanding of these methods, especially\nin the non-convex setting. In this paper, we start closing this gap: we\ntheoretically analyze in the convex and non-convex settings a generalized\nversion of the AdaGrad stepsizes. We show sufficient conditions for these\nstepsizes to achieve almost sure asymptotic convergence of the gradients to\nzero, proving the first guarantee for generalized AdaGrad stepsizes in the\nnon-convex setting. Moreover, we show that these stepsizes allow to\nautomatically adapt to the level of noise of the stochastic gradients in both\nthe convex and non-convex settings, interpolating between $O(1/T)$ and\n$O(1/\\sqrt{T})$, up to logarithmic terms.",
        "url": "http://arxiv.org/pdf/1805.08114v3.pdf"
    },
    {
        "title": "Defending Against Machine Learning Model Stealing Attacks Using Deceptive Perturbations",
        "abstract": "Machine learning models are vulnerable to simple model stealing attacks if\nthe adversary can obtain output labels for chosen inputs. To protect against\nthese attacks, it has been proposed to limit the information provided to the\nadversary by omitting probability scores, significantly impacting the utility\nof the provided service. In this work, we illustrate how a service provider can\nstill provide useful, albeit misleading, class probability information, while\nsignificantly limiting the success of the attack. Our defense forces the\nadversary to discard the class probabilities, requiring significantly more\nqueries before they can train a model with comparable performance. We evaluate\nseveral attack strategies, model architectures, and hyperparameters under\nvarying adversarial models, and evaluate the efficacy of our defense against\nthe strongest adversary. Finally, we quantify the amount of noise injected into\nthe class probabilities to mesure the loss in utility, e.g., adding 1.26 nats\nper query on CIFAR-10 and 3.27 on MNIST. Our evaluation shows our defense can\ndegrade the accuracy of the stolen model at least 20%, or require up to 64\ntimes more queries while keeping the accuracy of the protected model almost\nintact.",
        "url": "http://arxiv.org/pdf/1806.00054v4.pdf"
    },
    {
        "title": "Dropping Convexity for More Efficient and Scalable Online Multiview Learning",
        "abstract": "Multiview representation learning is very popular for latent factor analysis.\nIt naturally arises in many data analysis, machine learning, and information\nretrieval applications to model dependent structures among multiple data\nsources. For computational convenience, existing approaches usually formulate\nthe multiview representation learning as convex optimization problems, where\nglobal optima can be obtained by certain algorithms in polynomial time.\nHowever, many pieces of evidence have corroborated that heuristic nonconvex\napproaches also have good empirical computational performance and convergence\nto the global optima, although there is a lack of theoretical justification.\nSuch a gap between theory and practice motivates us to study a nonconvex\nformulation for multiview representation learning, which can be efficiently\nsolved by a simple stochastic gradient descent (SGD) algorithm. We first\nillustrate the geometry of the nonconvex formulation; Then, we establish\nasymptotic global rates of convergence to the global optima by diffusion\napproximations. Numerical experiments are provided to support our theory.",
        "url": "http://arxiv.org/pdf/1702.08134v9.pdf"
    },
    {
        "title": "Dimensionality Reduction for Stationary Time Series via Stochastic Nonconvex Optimization",
        "abstract": "Stochastic optimization naturally arises in machine learning. Efficient\nalgorithms with provable guarantees, however, are still largely missing, when\nthe objective function is nonconvex and the data points are dependent. This\npaper studies this fundamental challenge through a streaming PCA problem for\nstationary time series data. Specifically, our goal is to estimate the\nprinciple component of time series data with respect to the covariance matrix\nof the stationary distribution. Computationally, we propose a variant of Oja's\nalgorithm combined with downsampling to control the bias of the stochastic\ngradient caused by the data dependency. Theoretically, we quantify the\nuncertainty of our proposed stochastic algorithm based on diffusion\napproximations. This allows us to prove the asymptotic rate of convergence and\nfurther implies near optimal asymptotic sample complexity. Numerical\nexperiments are provided to support our analysis.",
        "url": "http://arxiv.org/pdf/1803.02312v4.pdf"
    },
    {
        "title": "RMDL: Random Multimodel Deep Learning for Classification",
        "abstract": "The continually increasing number of complex datasets each year necessitates\never improving machine learning methods for robust and accurate categorization\nof these data. This paper introduces Random Multimodel Deep Learning (RMDL): a\nnew ensemble, deep learning approach for classification. Deep learning models\nhave achieved state-of-the-art results across many domains. RMDL solves the\nproblem of finding the best deep learning structure and architecture while\nsimultaneously improving robustness and accuracy through ensembles of deep\nlearning architectures. RDML can accept as input a variety data to include\ntext, video, images, and symbolic. This paper describes RMDL and shows test\nresults for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB,\nand 20newsgroup. These test results show that RDML produces consistently better\nperformance than standard methods over a broad range of data types and\nclassification problems.",
        "url": "http://arxiv.org/pdf/1805.01890v2.pdf"
    },
    {
        "title": "Accurate pedestrian localization in overhead depth images via Height-Augmented HOG",
        "abstract": "We tackle the challenge of reliably and automatically localizing pedestrians\nin real-life conditions through overhead depth imaging at unprecedented\nhigh-density conditions. Leveraging upon a combination of Histogram of Oriented\nGradients-like feature descriptors, neural networks, data augmentation and\ncustom data annotation strategies, this work contributes a robust and scalable\nmachine learning-based localization algorithm, which delivers near-human\nlocalization performance in real-time, even with local pedestrian density of\nabout 3 ped/m2, a case in which most state-of-the art algorithms degrade\nsignificantly in performance.",
        "url": "http://arxiv.org/pdf/1805.12510v1.pdf"
    },
    {
        "title": "On GANs and GMMs",
        "abstract": "A longstanding problem in machine learning is to find unsupervised methods\nthat can learn the statistical structure of high dimensional signals. In recent\nyears, GANs have gained much attention as a possible solution to the problem,\nand in particular have shown the ability to generate remarkably realistic high\nresolution sampled images. At the same time, many authors have pointed out that\nGANs may fail to model the full distribution (\"mode collapse\") and that using\nthe learned models for anything other than generating samples may be very\ndifficult. In this paper, we examine the utility of GANs in learning\nstatistical models of images by comparing them to perhaps the simplest\nstatistical model, the Gaussian Mixture Model. First, we present a simple\nmethod to evaluate generative models based on relative proportions of samples\nthat fall into predetermined bins. Unlike previous automatic methods for\nevaluating models, our method does not rely on an additional neural network nor\ndoes it require approximating intractable computations. Second, we compare the\nperformance of GANs to GMMs trained on the same datasets. While GMMs have\npreviously been shown to be successful in modeling small patches of images, we\nshow how to train them on full sized images despite the high dimensionality.\nOur results show that GMMs can generate realistic samples (although less sharp\nthan those of GANs) but also capture the full distribution, which GANs fail to\ndo. Furthermore, GMMs allow efficient inference and explicit representation of\nthe underlying statistical structure. Finally, we discuss how GMMs can be used\nto generate sharp images.",
        "url": "http://arxiv.org/pdf/1805.12462v2.pdf"
    },
    {
        "title": "Metric on Nonlinear Dynamical Systems with Perron-Frobenius Operators",
        "abstract": "The development of a metric for structural data is a long-term problem in\npattern recognition and machine learning. In this paper, we develop a general\nmetric for comparing nonlinear dynamical systems that is defined with\nPerron-Frobenius operators in reproducing kernel Hilbert spaces. Our metric\nincludes the existing fundamental metrics for dynamical systems, which are\nbasically defined with principal angles between some appropriately-chosen\nsubspaces, as its special cases. We also describe the estimation of our metric\nfrom finite data. We empirically illustrate our metric with an example of\nrotation dynamics in a unit disk in a complex plane, and evaluate the\nperformance with real-world time-series data.",
        "url": "http://arxiv.org/pdf/1805.12324v2.pdf"
    },
    {
        "title": "MBA: Mini-Batch AUC Optimization",
        "abstract": "Area under the receiver operating characteristics curve (AUC) is an important\nmetric for a wide range of signal processing and machine learning problems, and\nscalable methods for optimizing AUC have recently been proposed. However,\nhandling very large datasets remains an open challenge for this problem. This\npaper proposes a novel approach to AUC maximization, based on sampling\nmini-batches of positive/negative instance pairs and computing U-statistics to\napproximate a global risk minimization problem. The resulting algorithm is\nsimple, fast, and learning-rate free. We show that the number of samples\nrequired for good performance is independent of the number of pairs available,\nwhich is a quadratic function of the positive and negative instances. Extensive\nexperiments show the practical utility of the proposed method.",
        "url": "http://arxiv.org/pdf/1805.11221v2.pdf"
    },
    {
        "title": "Multiaccuracy: Black-Box Post-Processing for Fairness in Classification",
        "abstract": "Prediction systems are successfully deployed in applications ranging from\ndisease diagnosis, to predicting credit worthiness, to image recognition. Even\nwhen the overall accuracy is high, these systems may exhibit systematic biases\nthat harm specific subpopulations; such biases may arise inadvertently due to\nunderrepresentation in the data used to train a machine-learning model, or as\nthe result of intentional malicious discrimination. We develop a rigorous\nframework of *multiaccuracy* auditing and post-processing to ensure accurate\npredictions across *identifiable subgroups*.\n  Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have\nblack-box access to a predictor and a relatively small set of labeled data for\nauditing; importantly, this black-box framework allows for improved fairness\nand accountability of predictions, even when the predictor is minimally\ntransparent. We prove that MULTIACCURACY-BOOST converges efficiently and show\nthat if the initial model is accurate on an identifiable subgroup, then the\npost-processed model will be also. We experimentally demonstrate the\neffectiveness of the approach to improve the accuracy among minority subgroups\nin diverse applications (image classification, finance, population health).\nInterestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for\n\"black women\") even when the sensitive features (e.g. \"race\", \"gender\") are not\ngiven to the algorithm explicitly.",
        "url": "http://arxiv.org/pdf/1805.12317v2.pdf"
    },
    {
        "title": "Fast Context-Annotated Classification of Different Types of Web Service Descriptions",
        "abstract": "In the recent rapid growth of web services, IoT, and cloud computing, many\nweb services and APIs appeared on the web. With the failure of global UDDI\nregistries, different service repositories started to appear, trying to list\nand categorize various types of web services for client applications' discover\nand use. In order to increase the effectiveness and speed up the task of\nfinding compatible Web Services in the brokerage when performing service\ncomposition or suggesting Web Services to the requests, high-level\nfunctionality of the service needs to be determined. Due to the lack of\nstructured support for specifying such functionality, classification of\nservices into a set of abstract categories is necessary. We employ a wide range\nof Machine Learning and Signal Processing algorithms and techniques in order to\nfind the highest precision achievable in the scope of this article for the fast\nclassification of three type of service descriptions: WSDL, REST, and WADL. In\naddition, we complement our approach by showing the importance and effect of\ncontextual information on the classification of the service descriptions and\nshow that it improves the accuracy in 5 different categories of services.",
        "url": "http://arxiv.org/pdf/1806.02374v1.pdf"
    },
    {
        "title": "Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization",
        "abstract": "Adversarial attacks involve adding, small, often imperceptible, perturbations\nto inputs with the goal of getting a machine learning model to misclassifying\nthem. While many different adversarial attack strategies have been proposed on\nimage classification models, object detection pipelines have been much harder\nto break. In this paper, we propose a novel strategy to craft adversarial\nexamples by solving a constrained optimization problem using an adversarial\ngenerator network. Our approach is fast and scalable, requiring only a forward\npass through our trained generator network to craft an adversarial sample.\nUnlike in many attack strategies, we show that the same trained generator is\ncapable of attacking new images without explicitly optimizing on them. We\nevaluate our attack on a trained Faster R-CNN face detector on the cropped\n300-W face dataset where we manage to reduce the number of detected faces to\n$0.5\\%$ of all originally detected faces. In a different experiment, also on\n300-W, we demonstrate the robustness of our attack to a JPEG compression based\ndefense typical JPEG compression level of $75\\%$ reduces the effectiveness of\nour attack from only $0.5\\%$ of detected faces to a modest $5.0\\%$.",
        "url": "http://arxiv.org/pdf/1805.12302v1.pdf"
    },
    {
        "title": "Algebraic Expression of Subjective Spatial and Temporal Patterns",
        "abstract": "Universal learning machine is a theory trying to study machine learning from\nmathematical point of view. The outside world is reflected inside an universal\nlearning machine according to pattern of incoming data. This is subjective\npattern of learning machine. In [2,4], we discussed subjective spatial pattern,\nand established a powerful tool -- X-form, which is an algebraic expression for\nsubjective spatial pattern. However, as the initial stage of study, there we\nonly discussed spatial pattern. Here, we will discuss spatial and temporal\npatterns, and algebraic expression for them.",
        "url": "http://arxiv.org/pdf/1805.11959v2.pdf"
    },
    {
        "title": "Evaluating Reinforcement Learning Algorithms in Observational Health Settings",
        "abstract": "Much attention has been devoted recently to the development of machine\nlearning algorithms with the goal of improving treatment policies in\nhealthcare. Reinforcement learning (RL) is a sub-field within machine learning\nthat is concerned with learning how to make sequences of decisions so as to\noptimize long-term effects. Already, RL algorithms have been proposed to\nidentify decision-making strategies for mechanical ventilation, sepsis\nmanagement and treatment of schizophrenia. However, before implementing\ntreatment policies learned by black-box algorithms in high-stakes clinical\ndecision problems, special care must be taken in the evaluation of these\npolicies.\n  In this document, our goal is to expose some of the subtleties associated\nwith evaluating RL algorithms in healthcare. We aim to provide a conceptual\nstarting point for clinical and computational researchers to ask the right\nquestions when designing and evaluating algorithms for new ways of treating\npatients. In the following, we describe how choices about how to summarize a\nhistory, variance of statistical estimators, and confounders in more ad-hoc\nmeasures can result in unreliable, even misleading estimates of the quality of\na treatment policy. We also provide suggestions for mitigating these\neffects---for while there is much promise for mining observational health data\nto uncover better treatment policies, evaluation must be performed\nthoughtfully.",
        "url": "http://arxiv.org/pdf/1805.12298v1.pdf"
    },
    {
        "title": "Dynamic Advisor-Based Ensemble (dynABE): Case study in stock trend prediction of critical metal companies",
        "abstract": "Stock trend prediction is a challenging task due to the market's noise, and\nmachine learning techniques have recently been successful in coping with this\nchallenge. In this research, we create a novel framework for stock prediction,\nDynamic Advisor-Based Ensemble (dynABE). dynABE explores domain-specific areas\nbased on the companies of interest, diversifies the feature set by creating\ndifferent \"advisors\" that each handles a different area, follows an effective\nmodel ensemble procedure for each advisor, and combines the advisors together\nin a second-level ensemble through an online update strategy we developed.\ndynABE is able to adapt to price pattern changes of the market during the\nactive trading period robustly, without needing to retrain the entire model. We\ntest dynABE on three cobalt-related companies, and it achieves the best-case\nmisclassification error of 31.12% and an annualized absolute return of 359.55%\nwith zero maximum drawdown. dynABE also consistently outperforms the baseline\nmodels of support vector machine, neural network, and random forest in all case\nstudies.",
        "url": "http://arxiv.org/pdf/1805.12111v4.pdf"
    },
    {
        "title": "Geometric Understanding of Deep Learning",
        "abstract": "Deep learning is the mainstream technique for many machine learning tasks,\nincluding image recognition, machine translation, speech recognition, and so\non. It has outperformed conventional methods in various fields and achieved\ngreat successes. Unfortunately, the understanding on how it works remains\nunclear. It has the central importance to lay down the theoretic foundation for\ndeep learning.\n  In this work, we give a geometric view to understand deep learning: we show\nthat the fundamental principle attributing to the success is the manifold\nstructure in data, namely natural high dimensional data concentrates close to a\nlow-dimensional manifold, deep learning learns the manifold and the probability\ndistribution on it.\n  We further introduce the concepts of rectified linear complexity for deep\nneural network measuring its learning capability, rectified linear complexity\nof an embedding manifold describing the difficulty to be learned. Then we show\nfor any deep neural network with fixed architecture, there exists a manifold\nthat cannot be learned by the network. Finally, we propose to apply optimal\nmass transportation theory to control the probability distribution in the\nlatent space.",
        "url": "http://arxiv.org/pdf/1805.10451v2.pdf"
    },
    {
        "title": "Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy",
        "abstract": "Diabetic retinopathy (DR) and diabetic macular edema are common complications\nof diabetes which can lead to vision loss. The grading of DR is a fairly\ncomplex process that requires the detection of fine features such as\nmicroaneurysms, intraretinal hemorrhages, and intraretinal microvascular\nabnormalities. Because of this, there can be a fair amount of grader\nvariability. There are different methods of obtaining the reference standard\nand resolving disagreements between graders, and while it is usually accepted\nthat adjudication until full consensus will yield the best reference standard,\nthe difference between various methods of resolving disagreements has not been\nexamined extensively. In this study, we examine the variability in different\nmethods of grading, definitions of reference standards, and their effects on\nbuilding deep learning models for the detection of diabetic eye disease. We\nfind that a small set of adjudicated DR grades allows substantial improvements\nin algorithm performance. The resulting algorithm's performance was on par with\nthat of individual U.S. board-certified ophthalmologists and retinal\nspecialists.",
        "url": "http://arxiv.org/pdf/1710.01711v3.pdf"
    },
    {
        "title": "Convolutional Embedded Networks for Population Scale Clustering and Bio-ancestry Inferencing",
        "abstract": "The study of genetic variants can help find correlating population groups to identify cohorts that are predisposed to common diseases and explain differences in disease susceptibility and how patients react to drugs. Machine learning algorithms are increasingly being applied to identify interacting GVs to understand their complex phenotypic traits. Since the performance of a learning algorithm not only depends on the size and nature of the data but also on the quality of underlying representation, deep neural networks can learn non-linear mappings that allow transforming GVs data into more clustering and classification friendly representations than manual feature selection. In this paper, we proposed convolutional embedded networks in which we combine two DNN architectures called convolutional embedded clustering and convolutional autoencoder classifier for clustering individuals and predicting geographic ethnicity based on GVs, respectively. We employed CAE-based representation learning on 95 million GVs from the 1000 genomes and Simons genome diversity projects. Quantitative and qualitative analyses with a focus on accuracy and scalability show that our approach outperforms state-of-the-art approaches such as VariantSpark and ADMIXTURE. In particular, CEC can cluster targeted population groups in 22 hours with an adjusted rand index of 0.915, the normalized mutual information of 0.92, and the clustering accuracy of 89%. Contrarily, the CAE classifier can predict the geographic ethnicity of unknown samples with an F1 and Mathews correlation coefficient(MCC) score of 0.9004 and 0.8245, respectively. To provide interpretations of the predictions, we identify significant biomarkers using gradient boosted trees(GBT) and SHAP. Overall, our approach is transparent and faster than the baseline methods, and scalable for 5% to 100% of the full human genome.",
        "url": "https://arxiv.org/pdf/1805.12218v2.pdf"
    },
    {
        "title": "On Consensus-Optimality Trade-offs in Collaborative Deep Learning",
        "abstract": "In distributed machine learning, where agents collaboratively learn from\ndiverse private data sets, there is a fundamental tension between consensus and\noptimality. In this paper, we build on recent algorithmic progresses in\ndistributed deep learning to explore various consensus-optimality trade-offs\nover a fixed communication topology. First, we propose the incremental\nconsensus-based distributed SGD (i-CDSGD) algorithm, which involves multiple\nconsensus steps (where each agent communicates information with its neighbors)\nwithin each SGD iteration. Second, we propose the generalized consensus-based\ndistributed SGD (g-CDSGD) algorithm that enables us to navigate the full\nspectrum from complete consensus (all agents agree) to complete disagreement\n(each agent converges to individual model parameters). We analytically\nestablish convergence of the proposed algorithms for strongly convex and\nnonconvex objective functions; we also analyze the momentum variants of the\nalgorithms for the strongly convex case. We support our algorithms via\nnumerical experiments, and demonstrate significant improvements over existing\nmethods for collaborative deep learning.",
        "url": "http://arxiv.org/pdf/1805.12120v1.pdf"
    },
    {
        "title": "Efficient Distributed Semi-Supervised Learning using Stochastic Regularization over Affinity Graphs",
        "abstract": "We describe a computationally efficient, stochastic graph-regularization\ntechnique that can be utilized for the semi-supervised training of deep neural\nnetworks in a parallel or distributed setting. We utilize a technique, first\ndescribed in [13] for the construction of mini-batches for stochastic gradient\ndescent (SGD) based on synthesized partitions of an affinity graph that are\nconsistent with the graph structure, but also preserve enough stochasticity for\nconvergence of SGD to good local minima. We show how our technique allows a\ngraph-based semi-supervised loss function to be decomposed into a sum over\nobjectives, facilitating data parallelism for scalable training of machine\nlearning models. Empirical results indicate that our method significantly\nimproves classification accuracy compared to the fully-supervised case when the\nfraction of labeled data is low, and in the parallel case, achieves significant\nspeed-up in terms of wall-clock time to convergence. We show the results for\nboth sequential and distributed-memory semi-supervised DNN training on a speech\ncorpus.",
        "url": "http://arxiv.org/pdf/1612.04898v2.pdf"
    },
    {
        "title": "MPDCompress - Matrix Permutation Decomposition Algorithm for Deep Neural Network Compression",
        "abstract": "Deep neural networks (DNNs) have become the state-of-the-art technique for\nmachine learning tasks in various applications. However, due to their size and\nthe computational complexity, large DNNs are not readily deployable on edge\ndevices in real-time. To manage complexity and accelerate computation, network\ncompression techniques based on pruning and quantization have been proposed and\nshown to be effective in reducing network size. However, such network\ncompression can result in irregular matrix structures that are mismatched with\nmodern hardware-accelerated platforms, such as graphics processing units (GPUs)\ndesigned to perform the DNN matrix multiplications in a structured\n(block-based) way. We propose MPDCompress, a DNN compression algorithm based on\nmatrix permutation decomposition via random mask generation. In-training\napplication of the masks molds the synaptic weight connection matrix to a\nsub-graph separation format. Aided by the random permutations, a\nhardware-desirable block matrix is generated, allowing for a more efficient\nimplementation and compression of the network. To show versatility, we\nempirically verify MPDCompress on several network models, compression rates,\nand image datasets. On the LeNet 300-100 model (MNIST dataset), Deep MNIST, and\nCIFAR10, we achieve 10 X network compression with less than 1% accuracy loss\ncompared to non-compressed accuracy performance. On AlexNet for the full\nImageNet ILSVRC-2012 dataset, we achieve 8 X network compression with less than\n1% accuracy loss, with top-5 and top-1 accuracies of 79.6% and 56.4%,\nrespectively. Finally, we observe that the algorithm can offer inference\nspeedups across various hardware platforms, with 4 X faster operation achieved\non several mobile GPUs.",
        "url": "http://arxiv.org/pdf/1805.12085v1.pdf"
    },
    {
        "title": "A Contextual Bandit Bake-off",
        "abstract": "Contextual bandit algorithms are essential for solving many real-world interactive machine learning problems. Despite multiple recent successes on statistically and computationally efficient methods, the practical behavior of these algorithms is still poorly understood. We leverage the availability of large numbers of supervised learning datasets to empirically evaluate contextual bandit algorithms, focusing on practical methods that learn by relying on optimization oracles from supervised learning. We find that a recent method (Foster et al., 2018) using optimism under uncertainty works the best overall. A surprisingly close second is a simple greedy baseline that only explores implicitly through the diversity of contexts, followed by a variant of Online Cover (Agarwal et al., 2014) which tends to be more conservative but robust to problem specification by design. Along the way, we also evaluate various components of contextual bandit algorithm design such as loss estimators. Overall, this is a thorough study and review of contextual bandit methodology.",
        "url": "https://arxiv.org/pdf/1802.04064v4.pdf"
    },
    {
        "title": "Towards Adversarial Configurations for Software Product Lines",
        "abstract": "Ensuring that all supposedly valid configurations of a software product line\n(SPL) lead to well-formed and acceptable products is challenging since it is\nmost of the time impractical to enumerate and test all individual products of\nan SPL. Machine learning classifiers have been recently used to predict the\nacceptability of products associated with unseen configurations. For some\nconfigurations, a tiny change in their feature values can make them pass from\nacceptable to non-acceptable regarding users' requirements and vice-versa. In\nthis paper, we introduce the idea of leveraging these specific configurations\nand their positions in the feature space to improve the classifier and\ntherefore the engineering of an SPL. Starting from a variability model, we\npropose to use Adversarial Machine Learning techniques to create new,\nadversarial configurations out of already known configurations by modifying\ntheir feature values. Using an industrial video generator we show how\nadversarial configurations can improve not only the classifier, but also the\nvariability model, the variability implementation, and the testing oracle.",
        "url": "http://arxiv.org/pdf/1805.12021v1.pdf"
    },
    {
        "title": "Neural Discrete Representation Learning",
        "abstract": "Learning useful representations without supervision remains a key challenge\nin machine learning. In this paper, we propose a simple yet powerful generative\nmodel that learns such discrete representations. Our model, the Vector\nQuantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways:\nthe encoder network outputs discrete, rather than continuous, codes; and the\nprior is learnt rather than static. In order to learn a discrete latent\nrepresentation, we incorporate ideas from vector quantisation (VQ). Using the\nVQ method allows the model to circumvent issues of \"posterior collapse\" --\nwhere the latents are ignored when they are paired with a powerful\nautoregressive decoder -- typically observed in the VAE framework. Pairing\nthese representations with an autoregressive prior, the model can generate high\nquality images, videos, and speech as well as doing high quality speaker\nconversion and unsupervised learning of phonemes, providing further evidence of\nthe utility of the learnt representations.",
        "url": "http://arxiv.org/pdf/1711.00937v2.pdf"
    },
    {
        "title": "On the Spectrum of Random Features Maps of High Dimensional Data",
        "abstract": "Random feature maps are ubiquitous in modern statistical machine learning,\nwhere they generalize random projections by means of powerful, yet often\ndifficult to analyze nonlinear operators. In this paper, we leverage the\n\"concentration\" phenomenon induced by random matrix theory to perform a\nspectral analysis on the Gram matrix of these random feature maps, here for\nGaussian mixture models of simultaneously large dimension and size. Our results\nare instrumental to a deeper understanding on the interplay of the nonlinearity\nand the statistics of the data, thereby allowing for a better tuning of random\nfeature-based techniques.",
        "url": "http://arxiv.org/pdf/1805.11916v2.pdf"
    },
    {
        "title": "Predictive Performance Modeling for Distributed Computing using Black-Box Monitoring and Machine Learning",
        "abstract": "In many domains, the previous decade was characterized by increasing data\nvolumes and growing complexity of computational workloads, creating new demands\nfor highly data-parallel computing in distributed systems. Effective operation\nof these systems is challenging when facing uncertainties about the performance\nof jobs and tasks under varying resource configurations, e.g., for scheduling\nand resource allocation. We survey predictive performance modeling (PPM)\napproaches to estimate performance metrics such as execution duration, required\nmemory or wait times of future jobs and tasks based on past performance\nobservations. We focus on non-intrusive methods, i.e., methods that can be\napplied to any workload without modification, since the workload is usually a\nblack-box from the perspective of the systems managing the computational\ninfrastructure. We classify and compare sources of performance variation,\npredicted performance metrics, required training data, use cases, and the\nunderlying prediction techniques. We conclude by identifying several open\nproblems and pressing research needs in the field.",
        "url": "http://arxiv.org/pdf/1805.11877v1.pdf"
    },
    {
        "title": "An English-Hindi Code-Mixed Corpus: Stance Annotation and Baseline System",
        "abstract": "Social media has become one of the main channels for peo- ple to communicate\nand share their views with the society. We can often detect from these views\nwhether the person is in favor, against or neu- tral towards a given topic.\nThese opinions from social media are very useful for various companies. We\npresent a new dataset that consists of 3545 English-Hindi code-mixed tweets\nwith opinion towards Demoneti- sation that was implemented in India in 2016\nwhich was followed by a large countrywide debate. We present a baseline\nsupervised classification system for stance detection developed using the same\ndataset that uses various machine learning techniques to achieve an accuracy of\n58.7% on 10-fold cross validation.",
        "url": "http://arxiv.org/pdf/1805.11868v1.pdf"
    },
    {
        "title": "ADAGIO: Interactive Experimentation with Adversarial Attack and Defense for Audio",
        "abstract": "Adversarial machine learning research has recently demonstrated the\nfeasibility to confuse automatic speech recognition (ASR) models by introducing\nacoustically imperceptible perturbations to audio samples. To help researchers\nand practitioners gain better understanding of the impact of such attacks, and\nto provide them with tools to help them more easily evaluate and craft strong\ndefenses for their models, we present ADAGIO, the first tool designed to allow\ninteractive experimentation with adversarial attacks and defenses on an ASR\nmodel in real time, both visually and aurally. ADAGIO incorporates AMR and MP3\naudio compression techniques as defenses, which users can interactively apply\nto attacked audio samples. We show that these techniques, which are based on\npsychoacoustic principles, effectively eliminate targeted attacks, reducing the\nattack success rate from 92.5% to 0%. We will demonstrate ADAGIO and invite the\naudience to try it on the Mozilla Common Voice dataset.",
        "url": "http://arxiv.org/pdf/1805.11852v1.pdf"
    },
    {
        "title": "LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning",
        "abstract": "This paper presents a new class of gradient methods for distributed machine\nlearning that adaptively skip the gradient calculations to learn with reduced\ncommunication and computation. Simple rules are designed to detect\nslowly-varying gradients and, therefore, trigger the reuse of outdated\ngradients. The resultant gradient-based algorithms are termed Lazily Aggregated\nGradient --- justifying our acronym LAG used henceforth. Theoretically, the\nmerits of this contribution are: i) the convergence rate is the same as batch\ngradient descent in strongly-convex, convex, and nonconvex smooth cases; and,\nii) if the distributed datasets are heterogeneous (quantified by certain\nmeasurable constants), the communication rounds needed to achieve a targeted\naccuracy are reduced thanks to the adaptive reuse of lagged gradients.\nNumerical experiments on both synthetic and real data corroborate a significant\ncommunication reduction compared to alternatives.",
        "url": "http://arxiv.org/pdf/1805.09965v2.pdf"
    },
    {
        "title": "A Progressive Batching L-BFGS Method for Machine Learning",
        "abstract": "The standard L-BFGS method relies on gradient approximations that are not\ndominated by noise, so that search directions are descent directions, the line\nsearch is reliable, and quasi-Newton updating yields useful quadratic models of\nthe objective function. All of this appears to call for a full batch approach,\nbut since small batch sizes give rise to faster algorithms with better\ngeneralization properties, L-BFGS is currently not considered an algorithm of\nchoice for large-scale machine learning applications. One need not, however,\nchoose between the two extremes represented by the full batch or highly\nstochastic regimes, and may instead follow a progressive batching approach in\nwhich the sample size increases during the course of the optimization. In this\npaper, we present a new version of the L-BFGS algorithm that combines three\nbasic components - progressive batching, a stochastic line search, and stable\nquasi-Newton updating - and that performs well on training logistic regression\nand deep neural networks. We provide supporting convergence theory for the\nmethod.",
        "url": "http://arxiv.org/pdf/1802.05374v2.pdf"
    },
    {
        "title": "Automated proof synthesis for propositional logic with deep neural networks",
        "abstract": "This work explores the application of deep learning, a machine learning\ntechnique that uses deep neural networks (DNN) in its core, to an automated\ntheorem proving (ATP) problem. To this end, we construct a statistical model\nwhich quantifies the likelihood that a proof is indeed a correct one of a given\nproposition. Based on this model, we give a proof-synthesis procedure that\nsearches for a proof in the order of the likelihood. This procedure uses an\nestimator of the likelihood of an inference rule being applied at each step of\na proof. As an implementation of the estimator, we propose a\nproposition-to-proof architecture, which is a DNN tailored to the automated\nproof synthesis problem. To empirically demonstrate its usefulness, we apply\nour model to synthesize proofs of propositional logic. We train the\nproposition-to-proof model using a training dataset of proposition-proof pairs.\nThe evaluation against a benchmark set shows the very high accuracy and an\nimprovement to the recent work of neural proof synthesis.",
        "url": "http://arxiv.org/pdf/1805.11799v1.pdf"
    },
    {
        "title": "To Trust Or Not To Trust A Classifier",
        "abstract": "Knowing when a classifier's prediction can be trusted is useful in many\napplications and critical for safely using AI. While the bulk of the effort in\nmachine learning research has been towards improving classifier performance,\nunderstanding when a classifier's predictions should and should not be trusted\nhas received far less attention. The standard approach is to use the\nclassifier's discriminant or confidence score; however, we show there exists an\nalternative that is more effective in many situations. We propose a new score,\ncalled the trust score, which measures the agreement between the classifier and\na modified nearest-neighbor classifier on the testing example. We show\nempirically that high (low) trust scores produce surprisingly high precision at\nidentifying correctly (incorrectly) classified examples, consistently\noutperforming the classifier's confidence score as well as many other\nbaselines. Further, under some mild distributional assumptions, we show that if\nthe trust score for an example is high (low), the classifier will likely agree\n(disagree) with the Bayes-optimal classifier. Our guarantees consist of\nnon-asymptotic rates of statistical consistency under various nonparametric\nsettings and build on recent developments in topological data analysis.",
        "url": "http://arxiv.org/pdf/1805.11783v2.pdf"
    },
    {
        "title": "AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks",
        "abstract": "Recent studies have shown that adversarial examples in state-of-the-art image classifiers trained by deep neural networks (DNN) can be easily generated when the target model is transparent to an attacker, known as the white-box setting. However, when attacking a deployed machine learning service, one can only acquire the input-output correspondences of the target model; this is the so-called black-box attack setting. The major drawback of existing black-box attacks is the need for excessive model queries, which may give a false sense of model robustness due to inefficient query designs. To bridge this gap, we propose a generic framework for query-efficient black-box attacks. Our framework, AutoZOOM, which is short for Autoencoder-based Zeroth Order Optimization Method, has two novel building blocks towards efficient black-box attacks: (i) an adaptive random gradient estimation strategy to balance query counts and distortion, and (ii) an autoencoder that is either trained offline with unlabeled data or a bilinear resizing operation for attack acceleration. Experimental results suggest that, by applying AutoZOOM to a state-of-the-art black-box attack (ZOO), a significant reduction in model queries can be achieved without sacrificing the attack success rate and the visual quality of the resulting adversarial examples. In particular, when compared to the standard ZOO method, AutoZOOM can consistently reduce the mean query counts in finding successful adversarial examples (or reaching the same distortion level) by at least 93% on MNIST, CIFAR-10 and ImageNet datasets, leading to novel insights on adversarial robustness.",
        "url": "https://arxiv.org/pdf/1805.11770v5.pdf"
    },
    {
        "title": "Invariant Representation of Mathematical Expressions",
        "abstract": "While there exist many methods in machine learning for comparison of letter string data, most are better equipped to handle strings that represent natural language, and their performance will not hold up when presented with strings that correspond to mathematical expressions. Based on the graphical representation of the expression tree, here we propose a simple method for encoding such expressions that is only sensitive to their structural properties, and invariant to the specifics which can vary between two seemingly different, but semantically similar mathematical expressions.",
        "url": "https://arxiv.org/pdf/1805.12495v2.pdf"
    },
    {
        "title": "Active and Adaptive Sequential learning",
        "abstract": "A framework is introduced for actively and adaptively solving a sequence of\nmachine learning problems, which are changing in bounded manner from one time\nstep to the next. An algorithm is developed that actively queries the labels of\nthe most informative samples from an unlabeled data pool, and that adapts to\nthe change by utilizing the information acquired in the previous steps. Our\nanalysis shows that the proposed active learning algorithm based on stochastic\ngradient descent achieves a near-optimal excess risk performance for maximum\nlikelihood estimation. Furthermore, an estimator of the change in the learning\nproblems using the active learning samples is constructed, which provides an\nadaptive sample size selection rule that guarantees the excess risk is bounded\nfor sufficiently large number of time steps. Experiments with synthetic and\nreal data are presented to validate our algorithm and theoretical results.",
        "url": "http://arxiv.org/pdf/1805.11710v1.pdf"
    },
    {
        "title": "Teaching Meaningful Explanations",
        "abstract": "The adoption of machine learning in high-stakes applications such as\nhealthcare and law has lagged in part because predictions are not accompanied\nby explanations comprehensible to the domain user, who often holds the ultimate\nresponsibility for decisions and outcomes. In this paper, we propose an\napproach to generate such explanations in which training data is augmented to\ninclude, in addition to features and labels, explanations elicited from domain\nusers. A joint model is then learned to produce both labels and explanations\nfrom the input features. This simple idea ensures that explanations are\ntailored to the complexity expectations and domain knowledge of the consumer.\nEvaluation spans multiple modeling techniques on a game dataset, a (visual)\naesthetics dataset, a chemical odor dataset and a Melanoma dataset showing that\nour approach is generalizable across domains and algorithms. Results\ndemonstrate that meaningful explanations can be reliably taught to machine\nlearning algorithms, and in some cases, also improve modeling accuracy.",
        "url": "http://arxiv.org/pdf/1805.11648v2.pdf"
    },
    {
        "title": "AdapterNet - learning input transformation for domain adaptation",
        "abstract": "Deep neural networks have demonstrated impressive performance in various\nmachine learning tasks. However, they are notoriously sensitive to changes in\ndata distribution. Often, even a slight change in the distribution can lead to\ndrastic performance reduction. Artificially augmenting the data may help to\nsome extent, but in most cases, fails to achieve model invariance to the data\ndistribution. Some examples where this sub-class of domain adaptation can be\nvaluable are various imaging modalities such as thermal imaging, X-ray,\nultrasound, and MRI, where changes in acquisition parameters or acquisition\ndevice manufacturer will result in a different representation of the same\ninput. Our work shows that standard fine-tuning fails to adapt the model in\ncertain important cases. We propose a novel method of adapting to a new data\nsource, and demonstrate near perfect adaptation on a customized ImageNet\nbenchmark. Moreover, our method does not require any samples from the original\ndata set, it is completely explainable and can be tailored to the task.",
        "url": "http://arxiv.org/pdf/1805.11601v2.pdf"
    },
    {
        "title": "CoupleNet: Paying Attention to Couples with Coupled Attention for Relationship Recommendation",
        "abstract": "Dating and romantic relationships not only play a huge role in our personal\nlives but also collectively influence and shape society. Today, many romantic\npartnerships originate from the Internet, signifying the importance of\ntechnology and the web in modern dating. In this paper, we present a text-based\ncomputational approach for estimating the relationship compatibility of two\nusers on social media. Unlike many previous works that propose reciprocal\nrecommender systems for online dating websites, we devise a distant supervision\nheuristic to obtain real world couples from social platforms such as Twitter.\nOur approach, the CoupleNet is an end-to-end deep learning based estimator that\nanalyzes the social profiles of two users and subsequently performs a\nsimilarity match between the users. Intuitively, our approach performs both\nuser profiling and match-making within a unified end-to-end framework.\nCoupleNet utilizes hierarchical recurrent neural models for learning\nrepresentations of user profiles and subsequently coupled attention mechanisms\nto fuse information aggregated from two users. To the best of our knowledge,\nour approach is the first data-driven deep learning approach for our novel\nrelationship recommendation problem. We benchmark our CoupleNet against several\nmachine learning and deep learning baselines. Experimental results show that\nour approach outperforms all approaches significantly in terms of precision.\nQualitative analysis shows that our model is capable of also producing\nexplainable results to users.",
        "url": "http://arxiv.org/pdf/1805.11535v1.pdf"
    },
    {
        "title": "Hyperparameter Importance Across Datasets",
        "abstract": "With the advent of automated machine learning, automated hyperparameter\noptimization methods are by now routinely used in data mining. However, this\nprogress is not yet matched by equal progress on automatic analyses that yield\ninformation beyond performance-optimizing hyperparameter settings. In this\nwork, we aim to answer the following two questions: Given an algorithm, what\nare generally its most important hyperparameters, and what are typically good\nvalues for these? We present methodology and a framework to answer these\nquestions based on meta-learning across many datasets. We apply this\nmethodology using the experimental meta-data available on OpenML to determine\nthe most important hyperparameters of support vector machines, random forests\nand Adaboost, and to infer priors for all their hyperparameters. The results,\nobtained fully automatically, provide a quantitative basis to focus efforts in\nboth manual algorithm design and in automated hyperparameter optimization. The\nconducted experiments confirm that the hyperparameters selected by the proposed\nmethod are indeed the most important ones and that the obtained priors also\nlead to statistically significant improvements in hyperparameter optimization.",
        "url": "http://arxiv.org/pdf/1710.04725v2.pdf"
    },
    {
        "title": "On consistent vertex nomination schemes",
        "abstract": "Given a vertex of interest in a network $G_1$, the vertex nomination problem\nseeks to find the corresponding vertex of interest (if it exists) in a second\nnetwork $G_2$. A vertex nomination scheme produces a list of the vertices in\n$G_2$, ranked according to how likely they are judged to be the corresponding\nvertex of interest in $G_2$. The vertex nomination problem and related\ninformation retrieval tasks have attracted much attention in the machine\nlearning literature, with numerous applications to social and biological\nnetworks. However, the current framework has often been confined to a\ncomparatively small class of network models, and the concept of statistically\nconsistent vertex nomination schemes has been only shallowly explored. In this\npaper, we extend the vertex nomination problem to a very general statistical\nmodel of graphs. Further, drawing inspiration from the long-established\nclassification framework in the pattern recognition literature, we provide\ndefinitions for the key notions of Bayes optimality and consistency in our\nextended vertex nomination framework, including a derivation of the Bayes\noptimal vertex nomination scheme. In addition, we prove that no universally\nconsistent vertex nomination schemes exist. Illustrative examples are provided\nthroughout.",
        "url": "http://arxiv.org/pdf/1711.05610v4.pdf"
    },
    {
        "title": "Multivariate Time Series Classification with WEASEL+MUSE",
        "abstract": "Multivariate time series (MTS) arise when multiple interconnected sensors\nrecord data over time. Dealing with this high-dimensional data is challenging\nfor every classifier for at least two aspects: First, an MTS is not only\ncharacterized by individual feature values, but also by the interplay of\nfeatures in different dimensions. Second, this typically adds large amounts of\nirrelevant data and noise. We present our novel MTS classifier WEASEL+MUSE\nwhich addresses both challenges. WEASEL+MUSE builds a multivariate feature\nvector, first using a sliding-window approach applied to each dimension of the\nMTS, then extracts discrete features per window and dimension. The feature\nvector is subsequently fed through feature selection, removing\nnon-discriminative features, and analysed by a machine learning classifier. The\nnovelty of WEASEL+MUSE lies in its specific way of extracting and filtering\nmultivariate features from MTS by encoding context information into each\nfeature. Still the resulting feature set is small, yet very discriminative and\nuseful for MTS classification. Based on a popular benchmark of 20 MTS datasets,\nwe found that WEASEL+MUSE is among the most accurate classifiers, when compared\nto the state of the art. The outstanding robustness of WEASEL+MUSE is further\nconfirmed based on motion gesture recognition data, where it out-of-the-box\nachieved similar accuracies as domain-specific methods.",
        "url": "http://arxiv.org/pdf/1711.11343v4.pdf"
    },
    {
        "title": "Automating Personnel Rostering by Learning Constraints Using Tensors",
        "abstract": "Many problems in operations research require that constraints be specified in\nthe model. Determining the right constraints is a hard and laborsome task. We\npropose an approach to automate this process using artificial intelligence and\nmachine learning principles. So far there has been only little work on learning\nconstraints within the operations research community. We focus on personnel\nrostering and scheduling problems in which there are often past schedules\navailable and show that it is possible to automatically learn constraints from\nsuch examples. To realize this, we adapted some techniques from the constraint\nprogramming community and we have extended them in order to cope with\nmultidimensional examples. The method uses a tensor representation of the\nexample, which helps in capturing the dimensionality as well as the structure\nof the example, and applies tensor operations to find the constraints that are\nsatisfied by the example. To evaluate the proposed algorithm, we used\nconstraints from the Nurse Rostering Competition and generated solutions that\nsatisfy these constraints; these solutions were then used as examples to learn\nconstraints. Experiments demonstrate that the proposed algorithm is capable of\nproducing human readable constraints that capture the underlying\ncharacteristics of the examples.",
        "url": "http://arxiv.org/pdf/1805.11375v1.pdf"
    },
    {
        "title": "Regularized Optimal Transport and the Rot Mover's Distance",
        "abstract": "This paper presents a unified framework for smooth convex regularization of\ndiscrete optimal transport problems. In this context, the regularized optimal\ntransport turns out to be equivalent to a matrix nearness problem with respect\nto Bregman divergences. Our framework thus naturally generalizes a previously\nproposed regularization based on the Boltzmann-Shannon entropy related to the\nKullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We\ncall the regularized optimal transport distance the rot mover's distance in\nreference to the classical earth mover's distance. We develop two generic\nschemes that we respectively call the alternate scaling algorithm and the\nnon-negative alternate scaling algorithm, to compute efficiently the\nregularized optimal plans depending on whether the domain of the regularizer\nlies within the non-negative orthant or not. These schemes are based on\nDykstra's algorithm with alternate Bregman projections, and further exploit the\nNewton-Raphson method when applied to separable divergences. We enhance the\nseparable case with a sparse extension to deal with high data dimensions. We\nalso instantiate our proposed framework and discuss the inherent specificities\nfor well-known regularizers and statistical divergences in the machine learning\nand information geometry communities. Finally, we demonstrate the merits of our\nmethods with experiments using synthetic data to illustrate the effect of\ndifferent regularizers and penalties on the solutions, as well as real-world\ndata for a pattern recognition application to audio scene classification.",
        "url": "http://arxiv.org/pdf/1610.06447v4.pdf"
    },
    {
        "title": "Learning Data Augmentation for Brain Tumor Segmentation with Coarse-to-Fine Generative Adversarial Networks",
        "abstract": "There is a common belief that the successful training of deep neural networks\nrequires many annotated training samples, which are often expensive and\ndifficult to obtain especially in the biomedical imaging field. While it is\noften easy for researchers to use data augmentation to expand the size of\ntraining sets, constructing and generating generic augmented data that is able\nto teach the network the desired invariance and robustness properties using\ntraditional data augmentation techniques is challenging in practice. In this\npaper, we propose a novel automatic data augmentation method that uses\ngenerative adversarial networks to learn augmentations that enable machine\nlearning based method to learn the available annotated samples more\nefficiently. The architecture consists of a coarse-to-fine generator to capture\nthe manifold of the training sets and generate generic augmented data. In our\nexperiments, we show the efficacy of our approach on a Magnetic Resonance\nImaging (MRI) image, achieving improvements of 3.5% Dice coefficient on the\nBRATS15 Challenge dataset as compared to traditional augmentation approaches.\nAlso, our proposed method successfully boosts a common segmentation network to\nreach the state-of-the-art performance on the BRATS15 Challenge.",
        "url": "http://arxiv.org/pdf/1805.11291v2.pdf"
    },
    {
        "title": "Fast Convergence for Stochastic and Distributed Gradient Descent in the Interpolation Limit",
        "abstract": "Modern supervised learning techniques, particularly those using deep nets,\ninvolve fitting high dimensional labelled data sets with functions containing\nvery large numbers of parameters. Much of this work is empirical. Interesting\nphenomena have been observed that require theoretical explanations; however the\nnon-convexity of the loss functions complicates the analysis. Recently it has\nbeen proposed that the success of these techniques rests partly in the\neffectiveness of the simple stochastic gradient descent algorithm in the so\ncalled interpolation limit in which all labels are fit perfectly. This analysis\nis made possible since the SGD algorithm reduces to a stochastic linear system\nnear the interpolating minimum of the loss function. Here we exploit this\ninsight by presenting and analyzing a new distributed algorithm for gradient\ndescent, also in the interpolating limit. The distributed SGD algorithm\npresented in the paper corresponds to gradient descent applied to a simple\npenalized distributed loss function, $L({\\bf w}_1,...,{\\bf w}_n) = \\Sigma_i\nl_i({\\bf w}_i) + \\mu \\sum_{<i,j>}|{\\bf w}_i-{\\bf w}_j|^2$. Here each node holds\nonly one sample, and its own parameter vector. The notation $<i,j>$ denotes\nedges of a connected graph defining the links between nodes. It is shown that\nthis distributed algorithm converges linearly (ie the error reduces\nexponentially with iteration number), with a rate\n$1-\\frac{\\eta}{n}\\lambda_{min}(H)<R<1$ where $\\lambda_{min}(H)$ is the smallest\nnonzero eigenvalue of the sample covariance or the Hessian H. In contrast with\nprevious usage of similar penalty functions to enforce consensus between nodes,\nin the interpolating limit it is not required to take the penalty parameter to\ninfinity for consensus to occur. The analysis further reinforces the utility of\nthe interpolation limit in the theoretical treatment of modern machine learning\nalgorithms.",
        "url": "http://arxiv.org/pdf/1803.02922v3.pdf"
    },
    {
        "title": "Currency exchange prediction using machine learning, genetic algorithms and technical analysis",
        "abstract": "Technical analysis is used to discover investment opportunities. To test this\nhypothesis we propose an hybrid system using machine learning techniques\ntogether with genetic algorithms. Using technical analysis there are more ways\nto represent a currency exchange time series than the ones it is possible to\ntest computationally, i.e., it is unfeasible to search the whole input feature\nspace thus a genetic algorithm is an alternative. In this work, an architecture\nfor automatic feature selection is proposed to optimize the cross validated\nperformance estimation of a Naive Bayes model using a genetic algorithm. The\nproposed architecture improves the return on investment of the unoptimized\nsystem from 0,43% to 10,29% in the validation set. The features selected and\nthe model decision boundary are visualized using the algorithm t-Distributed\nStochastic Neighbor embedding.",
        "url": "http://arxiv.org/pdf/1805.11232v1.pdf"
    },
    {
        "title": "Putting a bug in ML: The moth olfactory network learns to read MNIST",
        "abstract": "We seek to (i) characterize the learning architectures exploited in\nbiological neural networks for training on very few samples, and (ii) port\nthese algorithmic structures to a machine learning context. The Moth Olfactory\nNetwork is among the simplest biological neural systems that can learn, and its\narchitecture includes key structural elements and mechanisms widespread in\nbiological neural nets, such as cascaded networks, competitive inhibition, high\nintrinsic noise, sparsity, reward mechanisms, and Hebbian plasticity. These\nstructural biological elements, in combination, enable rapid learning.\n  MothNet is a computational model of the Moth Olfactory Network, closely\naligned with the moth's known biophysics and with in vivo electrode data\ncollected from moths learning new odors. We assign this model the task of\nlearning to read the MNIST digits. We show that MothNet successfully learns to\nread given very few training samples (1 to 10 samples per class). In this\nfew-samples regime, it outperforms standard machine learning methods such as\nnearest-neighbors, support-vector machines, and neural networks (NNs), and\nmatches specialized one-shot transfer-learning methods but without the need for\npre-training. The MothNet architecture illustrates how algorithmic structures\nderived from biological brains can be used to build alternative NNs that may\navoid some of the learning rate limitations of current engineered NNs.",
        "url": "http://arxiv.org/pdf/1802.05405v3.pdf"
    },
    {
        "title": "A Statistical Recurrent Model on the Manifold of Symmetric Positive Definite Matrices",
        "abstract": "In a number of disciplines, the data (e.g., graphs, manifolds) to be analyzed\nare non-Euclidean in nature. Geometric deep learning corresponds to techniques\nthat generalize deep neural network models to such non-Euclidean spaces.\nSeveral recent papers have shown how convolutional neural networks (CNNs) can\nbe extended to learn with graph-based data. In this work, we study the setting\nwhere the data (or measurements) are ordered, longitudinal or temporal in\nnature and live on a Riemannian manifold -- this setting is common in a variety\nof problems in statistical machine learning, vision and medical imaging. We\nshow how recurrent statistical recurrent network models can be defined in such\nspaces. We give an efficient algorithm and conduct a rigorous analysis of its\nstatistical properties. We perform extensive numerical experiments\ndemonstrating competitive performance with state of the art methods but with\nsignificantly less number of parameters. We also show applications to a\nstatistical analysis task in brain imaging, a regime where deep neural network\nmodels have only been utilized in limited ways.",
        "url": "http://arxiv.org/pdf/1805.11204v2.pdf"
    },
    {
        "title": "Learning From Less Data: Diversified Subset Selection and Active Learning in Image Classification Tasks",
        "abstract": "Supervised machine learning based state-of-the-art computer vision techniques\nare in general data hungry and pose the challenges of not having adequate\ncomputing resources and of high costs involved in human labeling efforts.\nTraining data subset selection and active learning techniques have been\nproposed as possible solutions to these challenges respectively. A special\nclass of subset selection functions naturally model notions of diversity,\ncoverage and representation and they can be used to eliminate redundancy and\nthus lend themselves well for training data subset selection. They can also\nhelp improve the efficiency of active learning in further reducing human\nlabeling efforts by selecting a subset of the examples obtained using the\nconventional uncertainty sampling based techniques. In this work we empirically\ndemonstrate the effectiveness of two diversity models, namely the\nFacility-Location and Disparity-Min models for training-data subset selection\nand reducing labeling effort. We do this for a variety of computer vision tasks\nincluding Gender Recognition, Scene Recognition and Object Recognition. Our\nresults show that subset selection done in the right way can add 2-3% in\naccuracy on existing baselines, particularly in the case of less training data.\nThis allows the training of complex machine learning models (like Convolutional\nNeural Networks) with much less training data while incurring minimal\nperformance loss.",
        "url": "http://arxiv.org/pdf/1805.11191v1.pdf"
    },
    {
        "title": "Towards computational fluorescence microscopy: Machine learning-based integrated prediction of morphological and molecular tumor profiles",
        "abstract": "Recent advances in cancer research largely rely on new developments in\nmicroscopic or molecular profiling techniques offering high level of detail\nwith respect to either spatial or molecular features, but usually not both.\nHere, we present a novel machine learning-based computational approach that\nallows for the identification of morphological tissue features and the\nprediction of molecular properties from breast cancer imaging data. This\nintegration of microanatomic information of tumors with complex molecular\nprofiling data, including protein or gene expression, copy number variation,\ngene methylation and somatic mutations, provides a novel means to\ncomputationally score molecular markers with respect to their relevance to\ncancer and their spatial associations within the tumor microenvironment.",
        "url": "http://arxiv.org/pdf/1805.11178v1.pdf"
    },
    {
        "title": "A CNN for homogneous Riemannian manifolds with applications to Neuroimaging",
        "abstract": "Convolutional neural networks are ubiquitous in Machine Learning applications\nfor solving a variety of problems. They however can not be used in their native\nform when the domain of the data is commonly encountered manifolds such as the\nsphere, the special orthogonal group, the Grassmanian, the manifold of\nsymmetric positive definite matrices and others. Most recently, generalization\nof CNNs to data domains such as the 2-sphere has been reported by some research\ngroups, which is referred to as the spherical CNNs (SCNNs). The key property of\nSCNNs distinct from CNNs is that they exhibit the rotational equivariance\nproperty that allows for sharing learned weights within a layer. In this paper,\nwe theoretically generalize the CNNs to Riemannian homogeneous manifolds, that\ninclude but are not limited to the aforementioned example manifolds. Our key\ncontributions in this work are: (i) A theorem stating that linear group\nequivariance systems are fully characterized by correlation of functions on the\ndomain manifold and vice-versa. This is fundamental to the characterization of\nall linear group equivariant systems and parallels the widely used result in\nlinear system theory for vector spaces. (ii) As a corrolary, we prove the\nequivariance of the correlation operation to group actions admitted by the\ninput domains which are Riemannian homogeneous manifolds. (iii) We present the\nfirst end-to-end deep network architecture for classification of diffusion\nmagnetic resonance image (dMRI) scans acquired from a cohort of 44 Parkinson\nDisease patients and 50 control/normal subjects. (iv) A proof of concept\nexperiment involving synthetic data generated on the manifold of symmetric\npositive definite matrices is presented to demonstrate the applicability of our\nnetwork to other types of domains.",
        "url": "http://arxiv.org/pdf/1805.05487v3.pdf"
    },
    {
        "title": "Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition",
        "abstract": "Unsupervised text embeddings extraction is crucial for text understanding in\nmachine learning. Word2Vec and its variants have received substantial success\nin mapping words with similar syntactic or semantic meaning to vectors close to\neach other. However, extracting context-aware word-sequence embedding remains a\nchallenging task. Training over large corpus is difficult as labels are\ndifficult to get. More importantly, it is challenging for pre-trained models to\nobtain word-sequence embeddings that are universally good for all downstream\ntasks or for any new datasets. We propose a two-phased ConvDic+DeconvDec\nframework to solve the problem by combining a word-sequence dictionary learning\nmodel with a word-sequence embedding decode model. We propose a convolutional\ntensor decomposition mechanism to learn good word-sequence phrase dictionary in\nthe learning phase. It is proved to be more accurate and much more efficient\nthan the popular alternating minimization method. In the decode phase, we\nintroduce a deconvolution framework that is immune to the problem of varying\nsentence lengths. The word-sequence embeddings we extracted using\nConvDic+DeconvDec are universally good for a few downstream tasks we test on.\nThe framework requires neither pre-training nor prior/outside information.",
        "url": "http://arxiv.org/pdf/1606.03153v3.pdf"
    },
    {
        "title": "Dataflow Matrix Machines as a Generalization of Recurrent Neural Networks",
        "abstract": "Dataflow matrix machines are a powerful generalization of recurrent neural\nnetworks. They work with multiple types of arbitrary linear streams, multiple\ntypes of powerful neurons, and allow to incorporate higher-order constructions.\nWe expect them to be useful in machine learning and probabilistic programming,\nand in the synthesis of dynamic systems and of deterministic and probabilistic\nprograms.",
        "url": "http://arxiv.org/pdf/1603.09002v2.pdf"
    },
    {
        "title": "Adversarial Examples in Remote Sensing",
        "abstract": "This paper considers attacks against machine learning algorithms used in\nremote sensing applications, a domain that presents a suite of challenges that\nare not fully addressed by current research focused on natural image data such\nas ImageNet. In particular, we present a new study of adversarial examples in\nthe context of satellite image classification problems. Using a recently\ncurated data set and associated classifier, we provide a preliminary analysis\nof adversarial examples in settings where the targeted classifier is permitted\nmultiple observations of the same location over time. While our experiments to\ndate are purely digital, our problem setup explicitly incorporates a number of\npractical considerations that a real-world attacker would need to take into\naccount when mounting a physical attack. We hope this work provides a useful\nstarting point for future studies of potential vulnerabilities in this setting.",
        "url": "http://arxiv.org/pdf/1805.10997v1.pdf"
    },
    {
        "title": "Machine learning for prediction of extreme statistics in modulation instability",
        "abstract": "A central area of research in nonlinear science is the study of instabilities\nthat drive the emergence of extreme events. Unfortunately, experimental\ntechniques for measuring such phenomena often provide only partial\ncharacterization. For example, real-time studies of instabilities in nonlinear\nfibre optics frequently use only spectral data, precluding detailed predictions\nabout the associated temporal properties. Here, we show how Machine Learning\ncan overcome this limitation by predicting statistics for the maximum intensity\nof temporal peaks in modulation instability based only on spectral\nmeasurements. Specifically, we train a neural network based Machine Learning\nmodel to correlate spectral and temporal properties of optical fibre modulation\ninstability using data from numerical simulations, and we then use this model\nto predict the temporal probability distribution based on high-dynamic range\nspectral data from experiments. These results open novel perspectives in all\nsystems exhibiting chaos and instability where direct time-domain observations\nare difficult.",
        "url": "http://arxiv.org/pdf/1806.06121v1.pdf"
    },
    {
        "title": "Granger-causal Attentive Mixtures of Experts: Learning Important Features with Neural Networks",
        "abstract": "Knowledge of the importance of input features towards decisions made by\nmachine-learning models is essential to increase our understanding of both the\nmodels and the underlying data. Here, we present a new approach to estimating\nfeature importance with neural networks based on the idea of distributing the\nfeatures of interest among experts in an attentive mixture of experts (AME).\nAMEs use attentive gating networks trained with a Granger-causal objective to\nlearn to jointly produce accurate predictions as well as estimates of feature\nimportance in a single model. Our experiments show (i) that the feature\nimportance estimates provided by AMEs compare favourably to those provided by\nstate-of-the-art methods, (ii) that AMEs are significantly faster at estimating\nfeature importance than existing methods, and (iii) that the associations\ndiscovered by AMEs are consistent with those reported by domain experts.",
        "url": "http://arxiv.org/pdf/1802.02195v6.pdf"
    },
    {
        "title": "Local Rule-Based Explanations of Black Box Decision Systems",
        "abstract": "The recent years have witnessed the rise of accurate but obscure decision\nsystems which hide the logic of their internal decision processes to the users.\nThe lack of explanations for the decisions of black box systems is a key\nethical issue, and a limitation to the adoption of machine learning components\nin socially sensitive and safety-critical contexts. %Therefore, we need\nexplanations that reveals the reasons why a predictor takes a certain decision.\nIn this paper we focus on the problem of black box outcome explanation, i.e.,\nexplaining the reasons of the decision taken on a specific instance. We propose\nLORE, an agnostic method able to provide interpretable and faithful\nexplanations. LORE first leans a local interpretable predictor on a synthetic\nneighborhood generated by a genetic algorithm. Then it derives from the logic\nof the local interpretable predictor a meaningful explanation consisting of: a\ndecision rule, which explains the reasons of the decision; and a set of\ncounterfactual rules, suggesting the changes in the instance's features that\nlead to a different outcome. Wide experiments show that LORE outperforms\nexisting methods and baselines both in the quality of explanations and in the\naccuracy in mimicking the black box.",
        "url": "http://arxiv.org/pdf/1805.10820v1.pdf"
    },
    {
        "title": "On Formalizing Fairness in Prediction with Machine Learning",
        "abstract": "Machine learning algorithms for prediction are increasingly being used in\ncritical decisions affecting human lives. Various fairness formalizations, with\nno firm consensus yet, are employed to prevent such algorithms from\nsystematically discriminating against people based on certain attributes\nprotected by law. The aim of this article is to survey how fairness is\nformalized in the machine learning literature for the task of prediction and\npresent these formalizations with their corresponding notions of distributive\njustice from the social sciences literature. We provide theoretical as well as\nempirical critiques of these notions from the social sciences literature and\nexplain how these critiques limit the suitability of the corresponding fairness\nformalizations to certain domains. We also suggest two notions of distributive\njustice which address some of these critiques and discuss avenues for\nprospective fairness formalizations.",
        "url": "http://arxiv.org/pdf/1710.03184v3.pdf"
    },
    {
        "title": "Deep Discriminative Latent Space for Clustering",
        "abstract": "Clustering is one of the most fundamental tasks in data analysis and machine\nlearning. It is central to many data-driven applications that aim to separate\nthe data into groups with similar patterns. Moreover, clustering is a complex\nprocedure that is affected significantly by the choice of the data\nrepresentation method. Recent research has demonstrated encouraging clustering\nresults by learning effectively these representations. In most of these works a\ndeep auto-encoder is initially pre-trained to minimize a reconstruction loss,\nand then jointly optimized with clustering centroids in order to improve the\nclustering objective. Those works focus mainly on the clustering phase of the\nprocedure, while not utilizing the potential benefit out of the initial phase.\nIn this paper we propose to optimize an auto-encoder with respect to a\ndiscriminative pairwise loss function during the auto-encoder pre-training\nphase. We demonstrate the high accuracy obtained by the proposed method as well\nas its rapid convergence (e.g. reaching above 92% accuracy on MNIST during the\npre-training phase, in less than 50 epochs), even with small networks.",
        "url": "http://arxiv.org/pdf/1805.10795v1.pdf"
    },
    {
        "title": "Keep and Learn: Continual Learning by Constraining the Latent Space for Knowledge Preservation in Neural Networks",
        "abstract": "Data is one of the most important factors in machine learning. However, even\nif we have high-quality data, there is a situation in which access to the data\nis restricted. For example, access to the medical data from outside is strictly\nlimited due to the privacy issues. In this case, we have to learn a model\nsequentially only with the data accessible in the corresponding stage. In this\nwork, we propose a new method for preserving learned knowledge by modeling the\nhigh-level feature space and the output space to be mutually informative, and\nconstraining feature vectors to lie in the modeled space during training. The\nproposed method is easy to implement as it can be applied by simply adding a\nreconstruction loss to an objective function. We evaluate the proposed method\non CIFAR-10/100 and a chest X-ray dataset, and show benefits in terms of\nknowledge preservation compared to previous approaches.",
        "url": "http://arxiv.org/pdf/1805.10784v1.pdf"
    },
    {
        "title": "Universality of Deep Convolutional Neural Networks",
        "abstract": "Deep learning has been widely applied and brought breakthroughs in speech\nrecognition, computer vision, and many other domains. The involved deep neural\nnetwork architectures and computational issues have been well studied in\nmachine learning. But there lacks a theoretical foundation for understanding\nthe approximation or generalization ability of deep learning methods generated\nby the network architectures such as deep convolutional neural networks having\nconvolutional structures. Here we show that a deep convolutional neural network\n(CNN) is universal, meaning that it can be used to approximate any continuous\nfunction to an arbitrary accuracy when the depth of the neural network is large\nenough. This answers an open question in learning theory. Our quantitative\nestimate, given tightly in terms of the number of free parameters to be\ncomputed, verifies the efficiency of deep CNNs in dealing with large\ndimensional data. Our study also demonstrates the role of convolutions in deep\nCNNs.",
        "url": "http://arxiv.org/pdf/1805.10769v2.pdf"
    },
    {
        "title": "Deep Trustworthy Knowledge Tracing",
        "abstract": "Knowledge tracing (KT), a key component of an intelligent tutoring system, is a machine learning technique that estimates the mastery level of a student based on his/her past performance. The objective of KT is to predict a student's response to the next question. Compared with traditional KT models, deep learning-based KT (DLKT) models show better predictive performance because of the representation power of deep neural networks. Various methods have been proposed to improve the performance of DLKT, but few studies have been conducted on the reliability of DLKT. In this work, we claim that the existing DLKTs are not reliable in real education environments. To substantiate the claim, we show limitations of DLKT from various perspectives such as knowledge state update failure, catastrophic forgetting, and non-interpretability. We then propose a novel regularization to address these problems. The proposed method allows us to achieve trustworthy DLKT. In addition, the proposed model which is trained on scenarios with forgetting can also be easily extended to scenarios without forgetting.",
        "url": "https://arxiv.org/pdf/1805.10768v3.pdf"
    },
    {
        "title": "Exponential convergence rates for Batch Normalization: The power of length-direction decoupling in non-convex optimization",
        "abstract": "Normalization techniques such as Batch Normalization have been applied\nsuccessfully for training deep neural networks. Yet, despite its apparent\nempirical benefits, the reasons behind the success of Batch Normalization are\nmostly hypothetical. We here aim to provide a more thorough theoretical\nunderstanding from a classical optimization perspective. Our main contribution\ntowards this goal is the identification of various problem instances in the\nrealm of machine learning where % -- under certain assumptions-- Batch\nNormalization can provably accelerate optimization. We argue that this\nacceleration is due to the fact that Batch Normalization splits the\noptimization task into optimizing length and direction of the parameters\nseparately. This allows gradient-based methods to leverage a favourable global\nstructure in the loss landscape that we prove to exist in Learning Halfspace\nproblems and neural network training with Gaussian inputs. We thereby turn\nBatch Normalization from an effective practical heuristic into a provably\nconverging algorithm for these settings. Furthermore, we substantiate our\nanalysis with empirical evidence that suggests the validity of our theoretical\nresults in a broader context.",
        "url": "http://arxiv.org/pdf/1805.10694v3.pdf"
    },
    {
        "title": "Strategyproof Linear Regression in High Dimensions",
        "abstract": "This paper is part of an emerging line of work at the intersection of machine\nlearning and mechanism design, which aims to avoid noise in training data by\ncorrectly aligning the incentives of data sources. Specifically, we focus on\nthe ubiquitous problem of linear regression, where strategyproof mechanisms\nhave previously been identified in two dimensions. In our setting, agents have\nsingle-peaked preferences and can manipulate only their response variables. Our\nmain contribution is the discovery of a family of group strategyproof linear\nregression mechanisms in any number of dimensions, which we call generalized\nresistant hyperplane mechanisms. The game-theoretic properties of these\nmechanisms -- and, in fact, their very existence -- are established through a\nconnection to a discrete version of the Ham Sandwich Theorem.",
        "url": "http://arxiv.org/pdf/1805.10693v1.pdf"
    },
    {
        "title": "Hierarchical correlation reconstruction with missing data, for example for biology-inspired neuron",
        "abstract": "Machine learning often needs to model density from a multidimensional data\nsample, including correlations between coordinates. Additionally, we often have\nmissing data case: that data points can miss values for some of coordinates.\nThis article adapts rapid parametric density estimation approach for this\npurpose: modelling density as a linear combination of orthonormal functions,\nfor which $L^2$ optimization says that (independently) estimated coefficient\nfor a given function is just average over the sample of value of this function.\nHierarchical correlation reconstruction first models probability density for\neach separate coordinate using all its appearances in data sample, then adds\ncorrections from independently modelled pairwise correlations using all samples\nhaving both coordinates, and so on independently adding correlations for\ngrowing numbers of variables using often decreasing evidence in data sample. A\nbasic application of such modelled multidimensional density can be imputation\nof missing coordinates: by inserting known coordinates to the density, and\ntaking expected values for the missing coordinates, or even their entire joint\nprobability distribution. Presented method can be compared with cascade\ncorrelations approach, offering several advantages in flexibility and accuracy.\nIt can be also used as artificial neuron: maximizing prediction capabilities\nfor only local behavior - modelling and predicting local connections.",
        "url": "http://arxiv.org/pdf/1804.06218v4.pdf"
    },
    {
        "title": "Distributed Deep Forest and its Application to Automatic Detection of Cash-out Fraud",
        "abstract": "Internet companies are facing the need for handling large-scale machine learning applications on a daily basis and distributed implementation of machine learning algorithms which can handle extra-large scale tasks with great performance is widely needed. Deep forest is a recently proposed deep learning framework which uses tree ensembles as its building blocks and it has achieved highly competitive results on various domains of tasks. However, it has not been tested on extremely large scale tasks. In this work, based on our parameter server system, we developed the distributed version of deep forest. To meet the need for real-world tasks, many improvements are introduced to the original deep forest model, including MART (Multiple Additive Regression Tree) as base learners for efficiency and effectiveness consideration, the cost-based method for handling prevalent class-imbalanced data, MART based feature selection for high dimension data and different evaluation metrics for automatically determining of the cascade level. We tested the deep forest model on an extra-large scale task, i.e., automatic detection of cash-out fraud, with more than 100 millions of training samples. Experimental results showed that the deep forest model has the best performance according to the evaluation metrics from different perspectives even with very little effort for parameter tuning. This model can block fraud transactions in a large amount of money each day. Even compared with the best-deployed model, the deep forest model can additionally bring into a significant decrease in economic loss each day.",
        "url": "https://arxiv.org/pdf/1805.04234v3.pdf"
    },
    {
        "title": "Semantic Explanations of Predictions",
        "abstract": "The main objective of explanations is to transmit knowledge to humans. This\nwork proposes to construct informative explanations for predictions made from\nmachine learning models. Motivated by the observations from social sciences,\nour approach selects data points from the training sample that exhibit special\ncharacteristics crucial for explanation, for instance, ones contrastive to the\nclassification prediction and ones representative of the models. Subsequently,\nsemantic concepts are derived from the selected data points through the use of\ndomain ontologies. These concepts are filtered and ranked to produce\ninformative explanations that improves human understanding. The main features\nof our approach are that (1) knowledge about explanations is captured in the\nform of ontological concepts, (2) explanations include contrastive evidences in\naddition to normal evidences, and (3) explanations are user relevant.",
        "url": "http://arxiv.org/pdf/1805.10587v1.pdf"
    },
    {
        "title": "Metric-Optimized Example Weights",
        "abstract": "Real-world machine learning applications often have complex test metrics, and may have training and test data that are not identically distributed. Motivated by known connections between complex test metrics and cost-weighted learning, we propose addressing these issues by using a weighted loss function with a standard loss, where the weights on the training examples are learned to optimize the test metric on a validation set. These metric-optimized example weights can be learned for any test metric, including black box and customized ones for specific applications. We illustrate the performance of the proposed method on diverse public benchmark datasets and real-world applications. We also provide a generalization bound for the method.",
        "url": "https://arxiv.org/pdf/1805.10582v3.pdf"
    },
    {
        "title": "Unsupervised Learning with Stein's Unbiased Risk Estimator",
        "abstract": "Learning from unlabeled and noisy data is one of the grand challenges of machine learning. As such, it has seen a flurry of research with new ideas proposed continuously. In this work, we revisit a classical idea: Stein's Unbiased Risk Estimator (SURE). We show that, in the context of image recovery, SURE and its generalizations can be used to train convolutional neural networks (CNNs) for a range of image denoising and recovery problems without any ground truth data. Specifically, our goal is to reconstruct an image $x$ from a noisy linear transformation (measurement) of the image. We consider two scenarios: one where no additional data is available and one where we have measurements of other images that are drawn from the same noisy distribution as $x$, but have no access to the clean images. Such is the case, for instance, in the context of medical imaging, microscopy, and astronomy, where noise-less ground truth data is rarely available. We show that in this situation, SURE can be used to estimate the mean-squared-error loss associated with an estimate of $x$. Using this estimate of the loss, we train networks to perform denoising and compressed sensing recovery. In addition, we also use the SURE framework to partially explain and improve upon an intriguing results presented by Ulyanov et al. in \"Deep Image Prior\": that a network initialized with random weights and fit to a single noisy image can effectively denoise that image. Public implementations of the networks and methods described in this paper can be found at https://github.com/ricedsp/D-AMP_Toolbox.",
        "url": "https://arxiv.org/pdf/1805.10531v3.pdf"
    },
    {
        "title": "Deep Reinforcement Learning in Ice Hockey for Context-Aware Player Evaluation",
        "abstract": "A variety of machine learning models have been proposed to assess the\nperformance of players in professional sports. However, they have only a\nlimited ability to model how player performance depends on the game context.\nThis paper proposes a new approach to capturing game context: we apply Deep\nReinforcement Learning (DRL) to learn an action-value Q function from 3M\nplay-by-play events in the National Hockey League (NHL). The neural network\nrepresentation integrates both continuous context signals and game history,\nusing a possession-based LSTM. The learned Q-function is used to value players'\nactions under different game contexts. To assess a player's overall\nperformance, we introduce a novel Game Impact Metric (GIM) that aggregates the\nvalues of the player's actions. Empirical Evaluation shows GIM is consistent\nthroughout a play season, and correlates highly with standard success measures\nand future salary.",
        "url": "http://arxiv.org/pdf/1805.11088v3.pdf"
    },
    {
        "title": "Adapted Deep Embeddings: A Synthesis of Methods for $k$-Shot Inductive Transfer Learning",
        "abstract": "The focus in machine learning has branched beyond training classifiers on a\nsingle task to investigating how previously acquired knowledge in a source\ndomain can be leveraged to facilitate learning in a related target domain,\nknown as inductive transfer learning. Three active lines of research have\nindependently explored transfer learning using neural networks. In weight\ntransfer, a model trained on the source domain is used as an initialization\npoint for a network to be trained on the target domain. In deep metric\nlearning, the source domain is used to construct an embedding that captures\nclass structure in both the source and target domains. In few-shot learning,\nthe focus is on generalizing well in the target domain based on a limited\nnumber of labeled examples. We compare state-of-the-art methods from these\nthree paradigms and also explore hybrid adapted-embedding methods that use\nlimited target-domain data to fine tune embeddings constructed from\nsource-domain data. We conduct a systematic comparison of methods in a variety\nof domains, varying the number of labeled instances available in the target\ndomain ($k$), as well as the number of target-domain classes. We reach three\nprincipal conclusions: (1) Deep embeddings are far superior, compared to weight\ntransfer, as a starting point for inter-domain transfer or model re-use (2) Our\nhybrid methods robustly outperform every few-shot learning and every deep\nmetric learning method previously proposed, with a mean error reduction of 34%\nover state-of-the-art. (3) Among loss functions for discovering embeddings, the\nhistogram loss (Ustinova & Lempitsky, 2016) is most robust. We hope our results\nwill motivate a unification of research in weight transfer, deep metric\nlearning, and few-shot learning.",
        "url": "http://arxiv.org/pdf/1805.08402v4.pdf"
    },
    {
        "title": "Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions",
        "abstract": "Embedding complex objects as vectors in low dimensional spaces is a\nlongstanding problem in machine learning. We propose in this work an extension\nof that approach, which consists in embedding objects as elliptical probability\ndistributions, namely distributions whose densities have elliptical level sets.\nWe endow these measures with the 2-Wasserstein metric, with two important\nbenefits: (i) For such measures, the squared 2-Wasserstein metric has a closed\nform, equal to a weighted sum of the squared Euclidean distance between means\nand the squared Bures metric between covariance matrices. The latter is a\nRiemannian metric between positive semi-definite matrices, which turns out to\nbe Euclidean on a suitable factor representation of such matrices, which is\nvalid on the entire geodesic between these matrices. (ii) The 2-Wasserstein\ndistance boils down to the usual Euclidean metric when comparing Diracs, and\ntherefore provides a natural framework to extend point embeddings. We show that\nfor these reasons Wasserstein elliptical embeddings are more intuitive and\nyield tools that are better behaved numerically than the alternative choice of\nGaussian embeddings with the Kullback-Leibler divergence. In particular, and\nunlike previous work based on the KL geometry, we learn elliptical\ndistributions that are not necessarily diagonal. We demonstrate the advantages\nof elliptical embeddings by using them for visualization, to compute embeddings\nof words, and to reflect entailment or hypernymy.",
        "url": "http://arxiv.org/pdf/1805.07594v5.pdf"
    },
    {
        "title": "Intensive Preprocessing of KDD Cup 99 for Network Intrusion Classification Using Machine Learning Techniques",
        "abstract": "Network security engineers work to keep services available all the time by\nhandling intruder attacks. Intrusion Detection System (IDS) is one of the\nobtainable mechanism that used to sense and classify any abnormal actions.\nTherefore, the IDS must be always up to date with the latest intruder attacks\nsignatures to preserve confidentiality, integrity and availability of the\nservices. The speed of the IDS is very important issue as well learning the new\nattacks. This research work illustrates how the Knowledge Discovery and Data\nMining (or Knowledge Discovery in Databases) KDD dataset is very handy for\ntesting and evaluating different Machine Learning Techniques. It mainly focuses\non the KDD preprocess part in order to prepare a decent and fair experimental\ndata set. The techniques J48, Random Forest, Random Tree, MLP, Na\\\"ive Bayes\nand Bayes Network classifiers have been chosen for this study. It has been\nproven that the Random forest classifier has achieved the highest accuracy rate\nfor detecting and classifying all KDD dataset attacks, which are of type (DOS,\nR2L, U2R, and PROBE)",
        "url": "http://arxiv.org/pdf/1805.10458v2.pdf"
    },
    {
        "title": "Nonlinear variable selection with continuous outcome: a nonparametric incremental forward stagewise approach",
        "abstract": "We present a method of variable selection for the sparse generalized additive\nmodel. The method doesn't assume any specific functional form, and can select\nfrom a large number of candidates. It takes the form of incremental forward\nstagewise regression. Given no functional form is assumed, we devised an\napproach termed roughening to adjust the residuals in the iterations. In\nsimulations, we show the new method is competitive against popular machine\nlearning approaches. We also demonstrate its performance using some real\ndatasets. The method is available as a part of the nlnet package on CRAN\nhttps://cran.r-project.org/package=nlnet.",
        "url": "http://arxiv.org/pdf/1601.05285v4.pdf"
    },
    {
        "title": "Splitting source code identifiers using Bidirectional LSTM Recurrent Neural Network",
        "abstract": "Programmers make rich use of natural language in the source code they write\nthrough identifiers and comments. Source code identifiers are selected from a\npool of tokens which are strongly related to the meaning, naming conventions,\nand context. These tokens are often combined to produce more precise and\nobvious designations. Such multi-part identifiers count for 97% of all naming\ntokens in the Public Git Archive - the largest dataset of Git repositories to\ndate. We introduce a bidirectional LSTM recurrent neural network to detect\nsubtokens in source code identifiers. We trained that network on 41.7 million\ndistinct splittable identifiers collected from 182,014 open source projects in\nPublic Git Archive, and show that it outperforms several other machine learning\nmodels. The proposed network can be used to improve the upstream models which\nare based on source code identifiers, as well as improving developer experience\nallowing writing code without switching the keyboard case.",
        "url": "http://arxiv.org/pdf/1805.11651v2.pdf"
    },
    {
        "title": "Model-based Pricing for Machine Learning in a Data Marketplace",
        "abstract": "Data analytics using machine learning (ML) has become ubiquitous in science,\nbusiness intelligence, journalism and many other domains. While a lot of work\nfocuses on reducing the training cost, inference runtime and storage cost of ML\nmodels, little work studies how to reduce the cost of data acquisition, which\npotentially leads to a loss of sellers' revenue and buyers' affordability and\nefficiency.\n  In this paper, we propose a model-based pricing (MBP) framework, which\ninstead of pricing the data, directly prices ML model instances. We first\nformally describe the desired properties of the MBP framework, with a focus on\navoiding arbitrage. Next, we show a concrete realization of the MBP framework\nvia a noise injection approach, which provably satisfies the desired formal\nproperties. Based on the proposed framework, we then provide algorithmic\nsolutions on how the seller can assign prices to models under different market\nscenarios (such as to maximize revenue). Finally, we conduct extensive\nexperiments, which validate that the MBP framework can provide high revenue to\nthe seller, high affordability to the buyer, and also operate on low runtime\ncost.",
        "url": "http://arxiv.org/pdf/1805.11450v1.pdf"
    },
    {
        "title": "Classification of crystallization outcomes using deep convolutional neural networks",
        "abstract": "The Machine Recognition of Crystallization Outcomes (MARCO) initiative has\nassembled roughly half a million annotated images of macromolecular\ncrystallization experiments from various sources and setups. Here,\nstate-of-the-art machine learning algorithms are trained and tested on\ndifferent parts of this data set. We find that more than 94% of the test images\ncan be correctly labeled, irrespective of their experimental origin. Because\ncrystal recognition is key to high-density screening and the systematic\nanalysis of crystallization experiments, this approach opens the door to both\nindustrial and fundamental research applications.",
        "url": "http://arxiv.org/pdf/1803.10342v2.pdf"
    },
    {
        "title": "A Mathematical Framework for Deep Learning in Elastic Source Imaging",
        "abstract": "An inverse elastic source problem with sparse measurements is of concern. A\ngeneric mathematical framework is proposed which incorporates a low-\ndimensional manifold regularization in the conventional source reconstruction\nalgorithms thereby enhancing their performance with sparse datasets. It is\nrigorously established that the proposed framework is equivalent to the\nso-called \\emph{deep convolutional framelet expansion} in machine learning\nliterature for inverse problems. Apposite numerical examples are furnished to\nsubstantiate the efficacy of the proposed framework.",
        "url": "http://arxiv.org/pdf/1802.10055v3.pdf"
    },
    {
        "title": "Gradient Coding via the Stochastic Block Model",
        "abstract": "Gradient descent and its many variants, including mini-batch stochastic\ngradient descent, form the algorithmic foundation of modern large-scale machine\nlearning. Due to the size and scale of modern data, gradient computations are\noften distributed across multiple compute nodes. Unfortunately, such\ndistributed implementations can face significant delays caused by straggler\nnodes, i.e., nodes that are much slower than average. Gradient coding is a new\ntechnique for mitigating the effect of stragglers via algorithmic redundancy.\nWhile effective, previously proposed gradient codes can be computationally\nexpensive to construct, inaccurate, or susceptible to adversarial stragglers.\nIn this work, we present the stochastic block code (SBC), a gradient code based\non the stochastic block model. We show that SBCs are efficient, accurate, and\nthat under certain settings, adversarial straggler selection becomes as hard as\ndetecting a community structure in the multiple community, block stochastic\ngraph model.",
        "url": "http://arxiv.org/pdf/1805.10378v1.pdf"
    },
    {
        "title": "Ergodic Inference: Accelerate Convergence by Optimisation",
        "abstract": "Statistical inference methods are fundamentally important in machine learning. Most state-of-the-art inference algorithms are variants of Markov chain Monte Carlo (MCMC) or variational inference (VI). However, both methods struggle with limitations in practice: MCMC methods can be computationally demanding; VI methods may have large bias. In this work, we aim to improve upon MCMC and VI by a novel hybrid method based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. We show that our method produces promising results on popular benchmarks when compared to recent hybrid methods of MCMC and VI.",
        "url": "https://arxiv.org/pdf/1805.10377v4.pdf"
    },
    {
        "title": "Detecting Deceptive Reviews using Generative Adversarial Networks",
        "abstract": "In the past few years, consumer review sites have become the main target of\ndeceptive opinion spam, where fictitious opinions or reviews are deliberately\nwritten to sound authentic. Most of the existing work to detect the deceptive\nreviews focus on building supervised classifiers based on syntactic and lexical\npatterns of an opinion. With the successful use of Neural Networks on various\nclassification applications, in this paper, we propose FakeGAN a system that\nfor the first time augments and adopts Generative Adversarial Networks (GANs)\nfor a text classification task, in particular, detecting deceptive reviews.\nUnlike standard GAN models which have a single Generator and Discriminator\nmodel, FakeGAN uses two discriminator models and one generative model. The\ngenerator is modeled as a stochastic policy agent in reinforcement learning\n(RL), and the discriminators use Monte Carlo search algorithm to estimate and\npass the intermediate action-value as the RL reward to the generator. Providing\nthe generator model with two discriminator models avoids the mod collapse issue\nby learning from both distributions of truthful and deceptive reviews. Indeed,\nour experiments show that using two discriminators provides FakeGAN high\nstability, which is a known issue for GAN architectures. While FakeGAN is built\nupon a semi-supervised classifier, known for less accuracy, our evaluation\nresults on a dataset of TripAdvisor hotel reviews show the same performance in\nterms of accuracy as of the state-of-the-art approaches that apply supervised\nmachine learning. These results indicate that GANs can be effective for text\nclassification tasks. Specifically, FakeGAN is effective at detecting deceptive\nreviews.",
        "url": "http://arxiv.org/pdf/1805.10364v1.pdf"
    },
    {
        "title": "What Face and Body Shapes Can Tell About Height",
        "abstract": "Recovering a person's height from a single image is important for virtual\ngarment fitting, autonomous driving and surveillance, however, it is also very\nchallenging due to the absence of absolute scale information. We tackle the\nrarely addressed case, where camera parameters and scene geometry is unknown.\nTo nevertheless resolve the inherent scale ambiguity, we infer height from\nstatistics that are intrinsic to human anatomy and can be estimated from images\ndirectly, such as articulated pose, bone length proportions, and facial\nfeatures. Our contribution is twofold. First, we experiment with different\nmachine learning models to capture the relation between image content and human\nheight. Second, we show that performance is predominantly limited by dataset\nsize and create a new dataset that is three magnitudes larger, by mining\nexplicit height labels and propagating them to additional images through face\nrecognition and assignment consistency. Our evaluation shows that monocular\nheight estimation is possible with a MAE of 5.56cm.",
        "url": "http://arxiv.org/pdf/1805.10355v1.pdf"
    },
    {
        "title": "Guaranteed Simultaneous Asymmetric Tensor Decomposition via Orthogonalized Alternating Least Squares",
        "abstract": "Tensor CANDECOMP/PARAFAC (CP) decomposition is an important tool that solves a wide class of machine learning problems. Existing popular approaches recover components one by one, not necessarily in the order of larger components first. Recently developed simultaneous power method obtains only a high probability recovery of top $r$ components even when the observed tensor is noiseless. We propose a Slicing Initialized Alternating Subspace Iteration (s-ASI) method that is guaranteed to recover top $r$ components ($\\epsilon$-close) simultaneously for (a)symmetric tensors almost surely under the noiseless case (with high probability for a bounded noise) using $O(\\log(\\log \\frac{1}{\\epsilon}))$ steps of tensor subspace iterations. Our s-ASI method introduces a Slice-Based Initialization that runs $O(1/\\log(\\frac{\\lambda_r}{\\lambda_{r+1}}))$ steps of matrix subspace iterations, where $\\lambda_r$ denotes the r-th top singular value of the tensor. We are the first to provide a theoretical guarantee on simultaneous orthogonal asymmetric tensor decomposition. Under the noiseless case, we are the first to provide an \\emph{almost sure} theoretical guarantee on simultaneous orthogonal tensor decomposition. When tensor is noisy, our algorithm for asymmetric tensor is robust to noise smaller than $\\min\\{O(\\frac{(\\lambda_r - \\lambda_{r+1})\\epsilon}{\\sqrt{r}}), O(\\delta_0\\frac{\\lambda_r -\\lambda_{r+1}}{\\sqrt{d}})\\}$, where $\\delta_0$ is a small constant proportional to the probability of bad initializations in the noisy setting.",
        "url": "https://arxiv.org/pdf/1805.10348v2.pdf"
    },
    {
        "title": "ORGaNICs: A Theory of Working Memory in Brains and Machines",
        "abstract": "Working memory is a cognitive process that is responsible for temporarily\nholding and manipulating information. Most of the empirical neuroscience\nresearch on working memory has focused on measuring sustained activity in\nprefrontal cortex (PFC) and/or parietal cortex during simple delayed-response\ntasks, and most of the models of working memory have been based on neural\nintegrators. But working memory means much more than just holding a piece of\ninformation online. We describe a new theory of working memory, based on a\nrecurrent neural circuit that we call ORGaNICs (Oscillatory Recurrent GAted\nNeural Integrator Circuits). ORGaNICs are a variety of Long Short Term Memory\nunits (LSTMs), imported from machine learning and artificial intelligence.\nORGaNICs can be used to explain the complex dynamics of delay-period activity\nin prefrontal cortex (PFC) during a working memory task. The theory is\nanalytically tractable so that we can characterize the dynamics, and the theory\nprovides a means for reading out information from the dynamically varying\nresponses at any point in time, in spite of the complex dynamics. ORGaNICs can\nbe implemented with a biophysical (electrical circuit) model of pyramidal\ncells, combined with shunting inhibition via a thalamocortical loop. Although\nintroduced as a computational theory of working memory, ORGaNICs are also\napplicable to models of sensory processing, motor preparation and motor\ncontrol. ORGaNICs offer computational advantages compared to other varieties of\nLSTMs that are commonly used in AI applications. Consequently, ORGaNICs are a\nframework for canonical computation in brains and machines.",
        "url": "http://arxiv.org/pdf/1803.06288v4.pdf"
    },
    {
        "title": "Qunatification of Metabolites in MR Spectroscopic Imaging using Machine Learning",
        "abstract": "Magnetic Resonance Spectroscopic Imaging (MRSI) is a clinical imaging\nmodality for measuring tissue metabolite levels in-vivo. An accurate estimation\nof spectral parameters allows for better assessment of spectral quality and\nmetabolite concentration levels. The current gold standard quantification\nmethod is the LCModel - a commercial fitting tool. However, this fails for\nspectra having poor signal-to-noise ratio (SNR) or a large number of artifacts.\nThis paper introduces a framework based on random forest regression for\naccurate estimation of the output parameters of a model based analysis of MR\nspectroscopy data. The goal of our proposed framework is to learn the spectral\nfeatures from a training set comprising of different variations of both\nsimulated and in-vivo brain spectra and then use this learning for the\nsubsequent metabolite quantification. Experiments involve training and testing\non simulated and in-vivo human brain spectra. We estimate parameters such as\nconcentration of metabolites and compare our results with that from the\nLCModel.",
        "url": "http://arxiv.org/pdf/1805.10201v1.pdf"
    },
    {
        "title": "Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces",
        "abstract": "This paper presents the machine learning architecture of the Snips Voice\nPlatform, a software solution to perform Spoken Language Understanding on\nmicroprocessors typical of IoT devices. The embedded inference is fast and\naccurate while enforcing privacy by design, as no personal user data is ever\ncollected. Focusing on Automatic Speech Recognition and Natural Language\nUnderstanding, we detail our approach to training high-performance Machine\nLearning models that are small enough to run in real-time on small devices.\nAdditionally, we describe a data generation procedure that provides sufficient,\nhigh-quality training data without compromising user privacy.",
        "url": "http://arxiv.org/pdf/1805.10190v3.pdf"
    },
    {
        "title": "Futuristic Classification with Dynamic Reference Frame Strategy",
        "abstract": "Classification is one of the widely used analytical techniques in data\nscience domain across different business to associate a pattern which\ncontribute to the occurrence of certain event which is predicted with some\nlikelihood. This Paper address a lacuna of creating some time window before the\nprediction actually happen to enable organizations some space to act on the\nprediction. There are some really good state of the art machine learning\ntechniques to optimally identify the possible churners in either customer base\nor employee base, similarly for fault prediction too if the prediction does not\ncome with some buffer time to act on the fault it is very difficult to provide\na seamless experience to the user. New concept of reference frame creation is\nintroduced to solve this problem in this paper",
        "url": "http://arxiv.org/pdf/1805.10168v1.pdf"
    },
    {
        "title": "UnibucKernel Reloaded: First Place in Arabic Dialect Identification for the Second Year in a Row",
        "abstract": "We present a machine learning approach that ranked on the first place in the\nArabic Dialect Identification (ADI) Closed Shared Tasks of the 2018 VarDial\nEvaluation Campaign. The proposed approach combines several kernels using\nmultiple kernel learning. While most of our kernels are based on character\np-grams (also known as n-grams) extracted from speech or phonetic transcripts,\nwe also use a kernel based on dialectal embeddings generated from audio\nrecordings by the organizers. In the learning stage, we independently employ\nKernel Discriminant Analysis (KDA) and Kernel Ridge Regression (KRR).\nPreliminary experiments indicate that KRR provides better classification\nresults. Our approach is shallow and simple, but the empirical results obtained\nin the 2018 ADI Closed Shared Task prove that it achieves the best performance.\nFurthermore, our top macro-F1 score (58.92%) is significantly better than the\nsecond best score (57.59%) in the 2018 ADI Shared Task, according to the\nstatistical significance test performed by the organizers. Nevertheless, we\nobtain even better post-competition results (a macro-F1 score of 62.28%) using\nthe audio embeddings released by the organizers after the competition. With a\nvery similar approach (that did not include phonetic features), we also ranked\nfirst in the ADI Closed Shared Tasks of the 2017 VarDial Evaluation Campaign,\nsurpassing the second best method by 4.62%. We therefore conclude that our\nmultiple kernel learning method is the best approach to date for Arabic dialect\nidentification.",
        "url": "http://arxiv.org/pdf/1805.04876v4.pdf"
    },
    {
        "title": "Double Quantization for Communication-Efficient Distributed Optimization",
        "abstract": "Modern distributed training of machine learning models suffers from high communication overhead for synchronizing stochastic gradients and model parameters. In this paper, to reduce the communication complexity, we propose \\emph{double quantization}, a general scheme for quantizing both model parameters and gradients. Three communication-efficient algorithms are proposed under this general scheme. Specifically, (i) we propose a low-precision algorithm AsyLPG with asynchronous parallelism, (ii) we explore integrating gradient sparsification with double quantization and develop Sparse-AsyLPG, (iii) we show that double quantization can also be accelerated by momentum technique and design accelerated AsyLPG. We establish rigorous performance guarantees for the algorithms, and conduct experiments on a multi-server test-bed to demonstrate that our algorithms can effectively save transmitted bits without performance degradation.",
        "url": "https://arxiv.org/pdf/1805.10111v4.pdf"
    },
    {
        "title": "Stochastic algorithms with descent guarantees for ICA",
        "abstract": "Independent component analysis (ICA) is a widespread data exploration technique, where observed signals are modeled as linear mixtures of independent components. From a machine learning point of view, it amounts to a matrix factorization problem with a statistical independence criterion. Infomax is one of the most used ICA algorithms. It is based on a loss function which is a non-convex log-likelihood. We develop a new majorization-minimization framework adapted to this loss function. We derive an online algorithm for the streaming setting, and an incremental algorithm for the finite sum setting, with the following benefits. First, unlike most algorithms found in the literature, the proposed methods do not rely on any critical hyper-parameter like a step size, nor do they require a line-search technique. Second, the algorithm for the finite sum setting, although stochastic, guarantees a decrease of the loss function at each iteration. Experiments demonstrate progress on the state-of-the-art for large scale datasets, without the necessity for any manual parameter tuning.",
        "url": "https://arxiv.org/pdf/1805.10054v2.pdf"
    },
    {
        "title": "Zeno: Distributed Stochastic Gradient Descent with Suspicion-based Fault-tolerance",
        "abstract": "We present Zeno, a technique to make distributed machine learning, particularly Stochastic Gradient Descent (SGD), tolerant to an arbitrary number of faulty workers. Zeno generalizes previous results that assumed a majority of non-faulty nodes; we need assume only one non-faulty worker. Our key idea is to suspect workers that are potentially defective. Since this is likely to lead to false positives, we use a ranking-based preference mechanism. We prove the convergence of SGD for non-convex problems under these scenarios. Experimental results show that Zeno outperforms existing approaches.",
        "url": "https://arxiv.org/pdf/1805.10032v3.pdf"
    },
    {
        "title": "Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence Diagrams",
        "abstract": "Algebraic topology methods have recently played an important role for\nstatistical analysis with complicated geometric structured data such as shapes,\nlinked twist maps, and material data. Among them, \\textit{persistent homology}\nis a well-known tool to extract robust topological features, and outputs as\n\\textit{persistence diagrams} (PDs). However, PDs are point multi-sets which\ncan not be used in machine learning algorithms for vector data. To deal with\nit, an emerged approach is to use kernel methods, and an appropriate geometry\nfor PDs is an important factor to measure the similarity of PDs. A popular\ngeometry for PDs is the \\textit{Wasserstein metric}. However, Wasserstein\ndistance is not \\textit{negative definite}. Thus, it is limited to build\npositive definite kernels upon the Wasserstein distance \\textit{without\napproximation}. In this work, we rely upon the alternative \\textit{Fisher\ninformation geometry} to propose a positive definite kernel for PDs\n\\textit{without approximation}, namely the Persistence Fisher (PF) kernel.\nThen, we analyze eigensystem of the integral operator induced by the proposed\nkernel for kernel machines. Based on that, we derive generalization error\nbounds via covering numbers and Rademacher averages for kernel machines with\nthe PF kernel. Additionally, we show some nice properties such as stability and\ninfinite divisibility for the proposed kernel. Furthermore, we also propose a\nlinear time complexity over the number of points in PDs for an approximation of\nour proposed kernel with a bounded error. Throughout experiments with many\ndifferent tasks on various benchmark datasets, we illustrate that the PF kernel\ncompares favorably with other baseline kernels for PDs.",
        "url": "http://arxiv.org/pdf/1802.03569v5.pdf"
    },
    {
        "title": "Graph2Seq: Graph to Sequence Learning with Attention-based Neural Networks",
        "abstract": "The celebrated Sequence to Sequence learning (Seq2Seq) technique and its\nnumerous variants achieve excellent performance on many tasks. However, many\nmachine learning tasks have inputs naturally represented as graphs; existing\nSeq2Seq models face a significant challenge in achieving accurate conversion\nfrom graph form to the appropriate sequence. To address this challenge, we\nintroduce a novel general end-to-end graph-to-sequence neural encoder-decoder\nmodel that maps an input graph to a sequence of vectors and uses an\nattention-based LSTM method to decode the target sequence from these vectors.\nOur method first generates the node and graph embeddings using an improved\ngraph-based neural network with a novel aggregation strategy to incorporate\nedge direction information in the node embeddings. We further introduce an\nattention mechanism that aligns node embeddings and the decoding sequence to\nbetter cope with large graphs. Experimental results on bAbI, Shortest Path, and\nNatural Language Generation tasks demonstrate that our model achieves\nstate-of-the-art performance and significantly outperforms existing graph\nneural networks, Seq2Seq, and Tree2Seq models; using the proposed\nbi-directional node embedding aggregation strategy, the model can converge\nrapidly to the optimal performance.",
        "url": "http://arxiv.org/pdf/1804.00823v4.pdf"
    },
    {
        "title": "D2KE: From Distance to Kernel and Embedding",
        "abstract": "For many machine learning problem settings, particularly with structured\ninputs such as sequences or sets of objects, a distance measure between inputs\ncan be specified more naturally than a feature representation. However, most\nstandard machine models are designed for inputs with a vector feature\nrepresentation. In this work, we consider the estimation of a function\n$f:\\mathcal{X} \\rightarrow \\R$ based solely on a dissimilarity measure\n$d:\\mathcal{X}\\times\\mathcal{X} \\rightarrow \\R$ between inputs. In particular,\nwe propose a general framework to derive a family of \\emph{positive definite\nkernels} from a given dissimilarity measure, which subsumes the widely-used\n\\emph{representative-set method} as a special case, and relates to the\nwell-known \\emph{distance substitution kernel} in a limiting case. We show that\nfunctions in the corresponding Reproducing Kernel Hilbert Space (RKHS) are\nLipschitz-continuous w.r.t. the given distance metric. We provide a tractable\nalgorithm to estimate a function from this RKHS, and show that it enjoys better\ngeneralizability than Nearest-Neighbor estimates. Our approach draws from the\nliterature of Random Features, but instead of deriving feature maps from an\nexisting kernel, we construct novel kernels from a random feature map, that we\nspecify given the distance measure. We conduct classification experiments with\nsuch disparate domains as strings, time series, and sets of vectors, where our\nproposed framework compares favorably to existing distance-based learning\nmethods such as $k$-nearest-neighbors, distance-substitution kernels,\npseudo-Euclidean embedding, and the representative-set method.",
        "url": "http://arxiv.org/pdf/1802.04956v4.pdf"
    },
    {
        "title": "A Sentiment Analysis of Breast Cancer Treatment Experiences and Healthcare Perceptions Across Twitter",
        "abstract": "Background: Social media has the capacity to afford the healthcare industry\nwith valuable feedback from patients who reveal and express their medical\ndecision-making process, as well as self-reported quality of life indicators\nboth during and post treatment. In prior work, [Crannell et. al.], we have\nstudied an active cancer patient population on Twitter and compiled a set of\ntweets describing their experience with this disease. We refer to these online\npublic testimonies as \"Invisible Patient Reported Outcomes\" (iPROs), because\nthey carry relevant indicators, yet are difficult to capture by conventional\nmeans of self-report. Methods: Our present study aims to identify tweets\nrelated to the patient experience as an additional informative tool for\nmonitoring public health. Using Twitter's public streaming API, we compiled\nover 5.3 million \"breast cancer\" related tweets spanning September 2016 until\nmid December 2017. We combined supervised machine learning methods with natural\nlanguage processing to sift tweets relevant to breast cancer patient\nexperiences. We analyzed a sample of 845 breast cancer patient and survivor\naccounts, responsible for over 48,000 posts. We investigated tweet content with\na hedonometric sentiment analysis to quantitatively extract emotionally charged\ntopics. Results: We found that positive experiences were shared regarding\npatient treatment, raising support, and spreading awareness. Further\ndiscussions related to healthcare were prevalent and largely negative focusing\non fear of political legislation that could result in loss of coverage.\nConclusions: Social media can provide a positive outlet for patients to discuss\ntheir needs and concerns regarding their healthcare coverage and treatment\nneeds. Capturing iPROs from online communication can help inform healthcare\nprofessionals and lead to more connected and personalized treatment regimens.",
        "url": "http://arxiv.org/pdf/1805.09959v2.pdf"
    },
    {
        "title": "Training of photonic neural networks through in situ backpropagation",
        "abstract": "Recently, integrated optics has gained interest as a hardware platform for\nimplementing machine learning algorithms. Of particular interest are artificial\nneural networks, since matrix-vector multi- plications, which are used heavily\nin artificial neural networks, can be done efficiently in photonic circuits.\nThe training of an artificial neural network is a crucial step in its\napplication. However, currently on the integrated photonics platform there is\nno efficient protocol for the training of these networks. In this work, we\nintroduce a method that enables highly efficient, in situ training of a\nphotonic neural network. We use adjoint variable methods to derive the photonic\nanalogue of the backpropagation algorithm, which is the standard method for\ncomputing gradients of conventional neural networks. We further show how these\ngradients may be obtained exactly by performing intensity measurements within\nthe device. As an application, we demonstrate the training of a numerically\nsimulated photonic artificial neural network. Beyond the training of photonic\nmachine learning implementations, our method may also be of broad interest to\nexperimental sensitivity analysis of photonic systems and the optimization of\nreconfigurable optics platforms.",
        "url": "http://arxiv.org/pdf/1805.09943v1.pdf"
    },
    {
        "title": "Automated Verification of Neural Networks: Advances, Challenges and Perspectives",
        "abstract": "Neural networks are one of the most investigated and widely used techniques\nin Machine Learning. In spite of their success, they still find limited\napplication in safety- and security-related contexts, wherein assurance about\nnetworks' performances must be provided. In the recent past, automated\nreasoning techniques have been proposed by several researchers to close the gap\nbetween neural networks and applications requiring formal guarantees about\ntheir behavior. In this work, we propose a primer of such techniques and a\ncomprehensive categorization of existing approaches for the automated\nverification of neural networks. A discussion about current limitations and\ndirections for future investigation is provided to foster research on this\ntopic at the crossroads of Machine Learning and Automated Reasoning.",
        "url": "http://arxiv.org/pdf/1805.09938v1.pdf"
    },
    {
        "title": "Multi-Task Determinantal Point Processes for Recommendation",
        "abstract": "Determinantal point processes (DPPs) have received significant attention in\nthe recent years as an elegant model for a variety of machine learning tasks,\ndue to their ability to elegantly model set diversity and item quality or\npopularity. Recent work has shown that DPPs can be effective models for product\nrecommendation and basket completion tasks. We present an enhanced DPP model\nthat is specialized for the task of basket completion, the multi-task DPP. We\nview the basket completion problem as a multi-class classification problem, and\nleverage ideas from tensor factorization and multi-class classification to\ndesign the multi-task DPP model. We evaluate our model on several real-world\ndatasets, and find that the multi-task DPP provides significantly better\npredictive quality than a number of state-of-the-art models.",
        "url": "http://arxiv.org/pdf/1805.09916v2.pdf"
    },
    {
        "title": "Pooling of Causal Models under Counterfactual Fairness via Causal Judgement Aggregation",
        "abstract": "In this paper we consider the problem of combining multiple probabilistic\ncausal models, provided by different experts, under the requirement that the\naggregated model satisfy the criterion of counterfactual fairness. We build\nupon the work on causal models and fairness in machine learning, and we express\nthe problem of combining multiple models within the framework of opinion\npooling. We propose two simple algorithms, grounded in the theory of\ncounterfactual fairness and causal judgment aggregation, that are guaranteed to\ngenerate aggregated probabilistic causal models respecting the criterion of\nfairness, and we compare their behaviors on a toy case study.",
        "url": "http://arxiv.org/pdf/1805.09866v2.pdf"
    },
    {
        "title": "Mining Procedures from Technical Support Documents",
        "abstract": "Guided troubleshooting is an inherent task in the domain of technical support\nservices. When a customer experiences an issue with the functioning of a\ntechnical service or a product, an expert user helps guide the customer through\na set of steps comprising a troubleshooting procedure. The objective is to\nidentify the source of the problem through a set of diagnostic steps and\nobservations, and arrive at a resolution. Procedures containing these set of\ndiagnostic steps and observations in response to different problems are common\nartifacts in the body of technical support documentation. The ability to use\nmachine learning and linguistics to understand and leverage these procedures\nfor applications like intelligent chatbots or robotic process automation, is\ncrucial. Existing research on question answering or intelligent chatbots does\nnot look within procedures or deep-understand them. In this paper, we outline a\nsystem for mining procedures from technical support documents. We create models\nfor solving important subproblems like extraction of procedures, identifying\ndecision points within procedures, identifying blocks of instructions\ncorresponding to these decision points and mapping instructions within a\ndecision block. We also release a dataset containing our manual annotations on\npublicly available support documents, to promote further research on the\nproblem.",
        "url": "http://arxiv.org/pdf/1805.09780v1.pdf"
    },
    {
        "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale",
        "abstract": "Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make the\ncontained knowledge discoverable. Unfortunately, both the format of these\ndocuments (e.g. the PDF format or bitmap images) as well as the presentation of\nthe data (e.g. complex tables) make the extraction of qualitative and\nquantitive data extremely challenging. In this paper, we present a modular,\ncloud-based platform to ingest documents at scale. This platform, called the\nCorpus Conversion Service (CCS), implements a pipeline which allows users to\nparse and annotate documents (i.e. collect ground-truth), train\nmachine-learning classification algorithms and ultimately convert any type of\nPDF or bitmap-documents to a structured content representation format. We will\nshow that each of the modules is scalable due to an asynchronous microservice\narchitecture and can therefore handle massive amounts of documents.\nFurthermore, we will show that our capability to gather ground-truth is\naccelerated by machine-learning algorithms by at least one order of magnitude.\nThis allows us to both gather large amounts of ground-truth in very little time\nand obtain very good precision/recall metrics in the range of 99\\% with regard\nto content conversion to structured output. The CCS platform is currently\ndeployed on IBM internal infrastructure and serving more than 250 active users\nfor knowledge-engineering project engagements.",
        "url": "http://arxiv.org/pdf/1806.02284v1.pdf"
    },
    {
        "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport",
        "abstract": "Many tasks in machine learning and signal processing can be solved by\nminimizing a convex function of a measure. This includes sparse spikes\ndeconvolution or training a neural network with a single hidden layer. For\nthese problems, we study a simple minimization method: the unknown measure is\ndiscretized into a mixture of particles and a continuous-time gradient descent\nis performed on their weights and positions. This is an idealization of the\nusual way to train neural networks with a large hidden layer. We show that,\nwhen initialized correctly and in the many-particle limit, this gradient flow,\nalthough non-convex, converges to global minimizers. The proof involves\nWasserstein gradient flows, a by-product of optimal transport theory. Numerical\nexperiments show that this asymptotic behavior is already at play for a\nreasonable number of particles, even in high dimension.",
        "url": "http://arxiv.org/pdf/1805.09545v2.pdf"
    },
    {
        "title": "Optimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular Maximization",
        "abstract": "In this paper we study the fundamental problems of maximizing a continuous\nnon-monotone submodular function over the hypercube, both with and without\ncoordinate-wise concavity. This family of optimization problems has several\napplications in machine learning, economics, and communication systems. Our\nmain result is the first $\\frac{1}{2}$-approximation algorithm for continuous\nsubmodular function maximization; this approximation factor of $\\frac{1}{2}$ is\nthe best possible for algorithms that only query the objective function at\npolynomially many points. For the special case of DR-submodular maximization,\ni.e. when the submodular functions is also coordinate wise concave along all\ncoordinates, we provide a different $\\frac{1}{2}$-approximation algorithm that\nruns in quasilinear time. Both of these results improve upon prior work [Bian\net al, 2017, Soma and Yoshida, 2017].\n  Our first algorithm uses novel ideas such as reducing the guaranteed\napproximation problem to analyzing a zero-sum game for each coordinate, and\nincorporates the geometry of this zero-sum game to fix the value at this\ncoordinate. Our second algorithm exploits coordinate-wise concavity to identify\na monotone equilibrium condition sufficient for getting the required\napproximation guarantee, and hunts for the equilibrium point using binary\nsearch. We further run experiments to verify the performance of our proposed\nalgorithms in related machine learning applications.",
        "url": "http://arxiv.org/pdf/1805.09480v1.pdf"
    },
    {
        "title": "VisualBackProp for learning using privileged information with CNNs",
        "abstract": "In many machine learning applications, from medical diagnostics to autonomous\ndriving, the availability of prior knowledge can be used to improve the\npredictive performance of learning algorithms and incorporate `physical,'\n`domain knowledge,' or `common sense' concepts into training of machine\nlearning systems as well as verify constraints/properties of the systems. We\nexplore the learning using privileged information paradigm and show how to\nincorporate the privileged information, such as segmentation mask available\nalong with the classification label of each example, into the training stage of\nconvolutional neural networks. This is done by augmenting the CNN model with an\narchitectural component that effectively focuses model's attention on the\ndesired region of the input image during the training process and that is\ntransparent to the network's label prediction mechanism at testing. This\ncomponent effectively corresponds to the visualization strategy for identifying\nthe parts of the input, often referred to as visualization mask, that most\ncontribute to the prediction, yet uses this strategy in reverse to the\nclassical setting in order to enforce the desired visualization mask instead.\nWe verify our proposed algorithms through exhaustive experiments on benchmark\nImageNet and PASCAL VOC data sets and achieve improvements in the performance\nof $2.4\\%$ and $2.7\\%$ over standard single-supervision model training.\nFinally, we confirm the effectiveness of our approach on skin lesion\nclassification problem.",
        "url": "http://arxiv.org/pdf/1805.09474v1.pdf"
    },
    {
        "title": "Taming Convergence for Asynchronous Stochastic Gradient Descent with Unbounded Delay in Non-Convex Learning",
        "abstract": "Understanding the convergence performance of asynchronous stochastic gradient descent method (Async-SGD) has received increasing attention in recent years due to their foundational role in machine learning. To date, however, most of the existing works are restricted to either bounded gradient delays or convex settings. In this paper, we focus on Async-SGD and its variant Async-SGDI (which uses increasing batch size) for non-convex optimization problems with unbounded gradient delays. We prove $o(1/\\sqrt{k})$ convergence rate for Async-SGD and $o(1/k)$ for Async-SGDI. Also, a unifying sufficient condition for Async-SGD's convergence is established, which includes two major gradient delay models in the literature as special cases and yields a new delay model not considered thus far.",
        "url": "https://arxiv.org/pdf/1805.09470v2.pdf"
    },
    {
        "title": "Learning Determinantal Point Processes by Corrective Negative Sampling",
        "abstract": "Determinantal Point Processes (DPPs) have attracted significant interest from\nthe machine-learning community due to their ability to elegantly and tractably\nmodel the delicate balance between quality and diversity of sets. DPPs are\ncommonly learned from data using maximum likelihood estimation (MLE). While\nfitting observed sets well, MLE for DPPs may also assign high likelihoods to\nunobserved sets that are far from the true generative distribution of the data.\nTo address this issue, which reduces the quality of the learned model, we\nintroduce a novel optimization problem, Contrastive Estimation (CE), which\nencodes information about \"negative\" samples into the basic learning model. CE\nis grounded in the successful use of negative information in machine-vision and\nlanguage modeling. Depending on the chosen negative distribution (which may be\nstatic or evolve during optimization), CE assumes two different forms, which we\nanalyze theoretically and experimentally. We evaluate our new model on\nreal-world datasets; on a challenging dataset, CE learning delivers a\nconsiderable improvement in predictive performance over a DPP learned without\nusing contrastive information.",
        "url": "http://arxiv.org/pdf/1802.05649v4.pdf"
    },
    {
        "title": "Markov Chain Importance Sampling -- a highly efficient estimator for MCMC",
        "abstract": "Markov chain (MC) algorithms are ubiquitous in machine learning and statistics and many other disciplines. Typically, these algorithms can be formulated as acceptance rejection methods. In this work we present a novel estimator applicable to these methods, dubbed Markov chain importance sampling (MCIS), which efficiently makes use of rejected proposals. For the unadjusted Langevin algorithm, it provides a novel way of correcting the discretization error. Our estimator satisfies a central limit theorem and improves on error per CPU cycle, often to a large extent. As a by-product it enables estimating the normalizing constant, an important quantity in Bayesian machine learning and statistics.",
        "url": "https://arxiv.org/pdf/1805.07179v4.pdf"
    },
    {
        "title": "Quantifying the visual concreteness of words and topics in multimodal datasets",
        "abstract": "Multimodal machine learning algorithms aim to learn visual-textual\ncorrespondences. Previous work suggests that concepts with concrete visual\nmanifestations may be easier to learn than concepts with abstract ones. We give\nan algorithm for automatically computing the visual concreteness of words and\ntopics within multimodal datasets. We apply the approach in four settings,\nranging from image captions to images/text scraped from historical books. In\naddition to enabling explorations of concepts in multimodal datasets, our\nconcreteness scores predict the capacity of machine learning algorithms to\nlearn textual/visual relationships. We find that 1) concrete concepts are\nindeed easier to learn; 2) the large number of algorithms we consider have\nsimilar failure cases; 3) the precise positive relationship between\nconcreteness and performance varies between datasets. We conclude with\nrecommendations for using concreteness scores to facilitate future multimodal\nresearch.",
        "url": "http://arxiv.org/pdf/1804.06786v2.pdf"
    },
    {
        "title": "Anonymizing k-Facial Attributes via Adversarial Perturbations",
        "abstract": "A face image not only provides details about the identity of a subject but\nalso reveals several attributes such as gender, race, sexual orientation, and\nage. Advancements in machine learning algorithms and popularity of sharing\nimages on the World Wide Web, including social media websites, have increased\nthe scope of data analytics and information profiling from photo collections.\nThis poses a serious privacy threat for individuals who do not want to be\nprofiled. This research presents a novel algorithm for anonymizing selective\nattributes which an individual does not want to share without affecting the\nvisual quality of images. Using the proposed algorithm, a user can select\nsingle or multiple attributes to be surpassed while preserving identity\ninformation and visual content. The proposed adversarial perturbation based\nalgorithm embeds imperceptible noise in an image such that attribute prediction\nalgorithm for the selected attribute yields incorrect classification result,\nthereby preserving the information according to user's choice. Experiments on\nthree popular databases i.e. MUCT, LFWcrop, and CelebA show that the proposed\nalgorithm not only anonymizes k-attributes, but also preserves image quality\nand identity information.",
        "url": "http://arxiv.org/pdf/1805.09380v2.pdf"
    },
    {
        "title": "Tell Me Something New: A New Framework for Asynchronous Parallel Learning",
        "abstract": "We present a novel approach for parallel computation in the context of\nmachine learning that we call \"Tell Me Something New\" (TMSN). This approach\ninvolves a set of independent workers that use broadcast to update each other\nwhen they observe \"something new\". TMSN does not require synchronization or a\nhead node and is highly resilient against failing machines or laggards. We\ndemonstrate the utility of TMSN by applying it to learning boosted trees. We\nshow that our implementation is 10 times faster than XGBoost and LightGBM on\nthe splice-site prediction problem.",
        "url": "http://arxiv.org/pdf/1805.07483v2.pdf"
    },
    {
        "title": "Learning to Optimize Contextually Constrained Problems for Real-Time Decision-Generation",
        "abstract": "The topic of learning to solve optimization problems has received interest from both the operations research and machine learning communities. In this work, we combine techniques from both fields to address the problem of learning to generate decisions to instances of continuous optimization problems where the feasible set varies with contextual features. We propose a novel framework for training a generative model to estimate optimal decisions by combining interior point methods and adversarial learning, which we further embed within an data generation algorithm. Decisions generated by our model satisfy in-sample and out-of-sample optimality guarantees. Finally, we investigate case studies in portfolio optimization and personalized treatment design, demonstrating that our approach yields advantages over predict-then-optimize and supervised deep learning techniques, respectively.",
        "url": "https://arxiv.org/pdf/1805.09293v4.pdf"
    },
    {
        "title": "On Nesting Monte Carlo Estimators",
        "abstract": "Many problems in machine learning and statistics involve nested expectations\nand thus do not permit conventional Monte Carlo (MC) estimation. For such\nproblems, one must nest estimators, such that terms in an outer estimator\nthemselves involve calculation of a separate, nested, estimation. We\ninvestigate the statistical implications of nesting MC estimators, including\ncases of multiple levels of nesting, and establish the conditions under which\nthey converge. We derive corresponding rates of convergence and provide\nempirical evidence that these rates are observed in practice. We further\nestablish a number of pitfalls that can arise from naive nesting of MC\nestimators, provide guidelines about how these can be avoided, and lay out\nnovel methods for reformulating certain classes of nested expectation problems\ninto single expectations, leading to improved convergence rates. We demonstrate\nthe applicability of our work by using our results to develop a new estimator\nfor discrete Bayesian experimental design problems and derive error bounds for\na class of variational objectives.",
        "url": "http://arxiv.org/pdf/1709.06181v4.pdf"
    },
    {
        "title": "Machine-learning inference of fluid variables from data using reservoir computing",
        "abstract": "We infer both microscopic and macroscopic behaviors of a three-dimensional\nchaotic fluid flow using reservoir computing. In our procedure of the\ninference, we assume no prior knowledge of a physical process of a fluid flow\nexcept that its behavior is complex but deterministic. We present two ways of\ninference of the complex behavior; the first called partial-inference requires\ncontinued knowledge of partial time-series data during the inference as well as\npast time-series data, while the second called full-inference requires only\npast time-series data as training data. For the first case, we are able to\ninfer long-time motion of microscopic fluid variables. For the second case, we\nshow that the reservoir dynamics constructed from only past data of energy\nfunctions can infer the future behavior of energy functions and reproduce the\nenergy spectrum. It is also shown that we can infer a time-series data from\nonly one measurement by using the delay coordinates. These implies that the\nobtained two reservoir systems constructed without the knowledge of microscopic\ndata are equivalent to the dynamical systems describing macroscopic behavior of\nenergy functions.",
        "url": "http://arxiv.org/pdf/1805.09917v3.pdf"
    },
    {
        "title": "Collective Online Learning of Gaussian Processes in Massive Multi-Agent Systems",
        "abstract": "Distributed machine learning (ML) is a modern computation paradigm that\ndivides its workload into independent tasks that can be simultaneously achieved\nby multiple machines (i.e., agents) for better scalability. However, a typical\ndistributed system is usually implemented with a central server that collects\ndata statistics from multiple independent machines operating on different\nsubsets of data to build a global analytic model. This centralized\ncommunication architecture however exposes a single choke point for operational\nfailure and places severe bottlenecks on the server's communication and\ncomputation capacities as it has to process a growing volume of communication\nfrom a crowd of learning agents. To mitigate these bottlenecks, this paper\nintroduces a novel Collective Online Learning Gaussian Process framework for\nmassive distributed systems that allows each agent to build its local model,\nwhich can be exchanged and combined efficiently with others via peer-to-peer\ncommunication to converge on a global model of higher quality. Finally, our\nempirical results consistently demonstrate the efficiency of our framework on\nboth synthetic and real-world datasets.",
        "url": "http://arxiv.org/pdf/1805.09266v2.pdf"
    },
    {
        "title": "Hyperbolic Neural Networks",
        "abstract": "Hyperbolic spaces have recently gained momentum in the context of machine\nlearning due to their high capacity and tree-likeliness properties. However,\nthe representational power of hyperbolic geometry is not yet on par with\nEuclidean geometry, mostly because of the absence of corresponding hyperbolic\nneural network layers. This makes it hard to use hyperbolic embeddings in\ndownstream tasks. Here, we bridge this gap in a principled manner by combining\nthe formalism of M\\\"obius gyrovector spaces with the Riemannian geometry of the\nPoincar\\'e model of hyperbolic spaces. As a result, we derive hyperbolic\nversions of important deep learning tools: multinomial logistic regression,\nfeed-forward and recurrent neural networks such as gated recurrent units. This\nallows to embed sequential data and perform classification in the hyperbolic\nspace. Empirically, we show that, even if hyperbolic optimization tools are\nlimited, hyperbolic sentence embeddings either outperform or are on par with\ntheir Euclidean variants on textual entailment and noisy-prefix recognition\ntasks.",
        "url": "http://arxiv.org/pdf/1805.09112v2.pdf"
    },
    {
        "title": "Empirical Analysis of Foundational Distinctions in Linked Open Data",
        "abstract": "The Web and its Semantic extension (i.e. Linked Open Data) contain open\nglobal-scale knowledge and make it available to potentially intelligent\nmachines that want to benefit from it. Nevertheless, most of Linked Open Data\nlack ontological distinctions and have sparse axiomatisation. For example,\ndistinctions such as whether an entity is inherently a class or an individual,\nor whether it is a physical object or not, are hardly expressed in the data,\nalthough they have been largely studied and formalised by foundational\nontologies (e.g. DOLCE, SUMO). These distinctions belong to common sense too,\nwhich is relevant for many artificial intelligence tasks such as natural\nlanguage understanding, scene recognition, and the like. There is a gap between\nfoundational ontologies, that often formalise or are inspired by pre-existing\nphilosophical theories and are developed with a top-down approach, and Linked\nOpen Data that mostly derive from existing databases or crowd-based effort\n(e.g. DBpedia, Wikidata). We investigate whether machines can learn\nfoundational distinctions over Linked Open Data entities, and if they match\ncommon sense. We want to answer questions such as \"does the DBpedia entity for\ndog refer to a class or to an instance?\". We report on a set of experiments\nbased on machine learning and crowdsourcing that show promising results.",
        "url": "http://arxiv.org/pdf/1803.09840v2.pdf"
    },
    {
        "title": "A Brand-level Ranking System with the Customized Attention-GRU Model",
        "abstract": "In e-commerce websites like Taobao, brand is playing a more important role in\ninfluencing users' decision of click/purchase, partly because users are now\nattaching more importance to the quality of products and brand is an indicator\nof quality. However, existing ranking systems are not specifically designed to\nsatisfy this kind of demand. Some design tricks may partially alleviate this\nproblem, but still cannot provide satisfactory results or may create additional\ninteraction cost. In this paper, we design the first brand-level ranking system\nto address this problem. The key challenge of this system is how to\nsufficiently exploit users' rich behavior in e-commerce websites to rank the\nbrands. In our solution, we firstly conduct the feature engineering\nspecifically tailored for the personalized brand ranking problem and then rank\nthe brands by an adapted Attention-GRU model containing three important\nmodifications. Note that our proposed modifications can also apply to many\nother machine learning models on various tasks. We conduct a series of\nexperiments to evaluate the effectiveness of our proposed ranking model and\ntest the response to the brand-level ranking system from real users on a\nlarge-scale e-commerce platform, i.e. Taobao.",
        "url": "http://arxiv.org/pdf/1805.08958v2.pdf"
    },
    {
        "title": "Global Model Interpretation via Recursive Partitioning",
        "abstract": "In this work, we propose a simple but effective method to interpret black-box\nmachine learning models globally. That is, we use a compact binary tree, the\ninterpretation tree, to explicitly represent the most important decision rules\nthat are implicitly contained in the black-box machine learning models. This\ntree is learned from the contribution matrix which consists of the\ncontributions of input variables to predicted scores for each single\nprediction. To generate the interpretation tree, a unified process recursively\npartitions the input variable space by maximizing the difference in the average\ncontribution of the split variable between the divided spaces. We demonstrate\nthe effectiveness of our method in diagnosing machine learning models on\nmultiple tasks. Also, it is useful for new knowledge discovery as such insights\nare not easily identifiable when only looking at single predictions. In\ngeneral, our work makes it easier and more efficient for human beings to\nunderstand machine learning models.",
        "url": "http://arxiv.org/pdf/1802.04253v2.pdf"
    },
    {
        "title": "Minimax Distribution Estimation in Wasserstein Distance",
        "abstract": "The Wasserstein metric is an important measure of distance between probability distributions, with applications in machine learning, statistics, probability theory, and data analysis. This paper provides upper and lower bounds on statistical minimax rates for the problem of estimating a probability distribution under Wasserstein loss, using only metric properties, such as covering and packing numbers, of the sample space, and weak moment assumptions on the probability distributions.",
        "url": "https://arxiv.org/pdf/1802.08855v3.pdf"
    },
    {
        "title": "Approximate Newton-based statistical inference using only stochastic gradients",
        "abstract": "We present a novel statistical inference framework for convex empirical risk\nminimization, using approximate stochastic Newton steps. The proposed algorithm\nis based on the notion of finite differences and allows the approximation of a\nHessian-vector product from first-order information. In theory, our method\nefficiently computes the statistical error covariance in $M$-estimation, both\nfor unregularized convex learning problems and high-dimensional LASSO\nregression, without using exact second order information, or resampling the\nentire data set. We also present a stochastic gradient sampling scheme for\nstatistical inference in non-i.i.d. time series analysis, where we sample\ncontiguous blocks of indices. In practice, we demonstrate the effectiveness of\nour framework on large-scale machine learning problems, that go even beyond\nconvexity: as a highlight, our work can be used to detect certain adversarial\nattacks on neural networks.",
        "url": "http://arxiv.org/pdf/1805.08920v2.pdf"
    },
    {
        "title": "Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training",
        "abstract": "The Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNNs) are a popular class of machine learning models for analyzing sequential data. Their training on modern GPUs, however, is limited by the GPU memory capacity. Our profiling results of the LSTM RNN-based Neural Machine Translation (NMT) model reveal that feature maps of the attention and RNN layers form the memory bottleneck and runtime is unevenly distributed across different layers when training on GPUs. Based on these two observations, we propose to recompute the feature maps rather than stashing them persistently in the GPU memory. While the idea of feature map recomputation has been considered before, existing solutions fail to deliver satisfactory footprint reduction, as they do not address two key challenges. For each feature map recomputation to be effective and efficient, its effect on (1) the total memory footprint, and (2) the total execution time has to be carefully estimated. To this end, we propose *Echo*, a new compiler-based optimization scheme that addresses the first challenge with a practical mechanism that estimates the memory benefits of recomputation over the entire computation graph, and the second challenge by non-conservatively estimating the recomputation overhead leveraging layer specifics. *Echo* reduces the GPU memory footprint automatically and transparently without any changes required to the training source code, and is effective for models beyond LSTM RNNs. We evaluate *Echo* on numerous state-of-the-art machine learning workloads on real systems with modern GPUs and observe footprint reduction ratios of 1.89X on average and 3.13X maximum. Such reduction can be converted into faster training with a larger batch size, savings in GPU energy consumption (e.g., training with one GPU as fast as with four), and/or an increase in the maximum number of layers under the same GPU memory budget.",
        "url": "https://arxiv.org/pdf/1805.08899v5.pdf"
    },
    {
        "title": "Optimal Transport for Multi-source Domain Adaptation under Target Shift",
        "abstract": "In this paper, we propose to tackle the problem of reducing discrepancies\nbetween multiple domains referred to as multi-source domain adaptation and\nconsider it under the target shift assumption: in all domains we aim to solve a\nclassification problem with the same output classes, but with labels'\nproportions differing across them. This problem, generally ignored in the vast\nmajority papers on domain adaptation papers, is nevertheless critical in\nreal-world applications, and we theoretically show its impact on the adaptation\nsuccess. To address this issue, we design a method based on optimal transport,\na theory that has been successfully used to tackle adaptation problems in\nmachine learning. Our method performs multi-source adaptation and target shift\ncorrection simultaneously by learning the class probabilities of the unlabeled\ntarget sample and the coupling allowing to align two (or more) probability\ndistributions. Experiments on both synthetic and real-world data related to\nsatellite image segmentation task show the superiority of the proposed method\nover the state-of-the-art.",
        "url": "http://arxiv.org/pdf/1803.04899v3.pdf"
    },
    {
        "title": "Clustering - What Both Theoreticians and Practitioners are Doing Wrong",
        "abstract": "Unsupervised learning is widely recognized as one of the most important\nchallenges facing machine learning nowa- days. However, in spite of hundreds of\npapers on the topic being published every year, current theoretical\nunderstanding and practical implementations of such tasks, in particular of\nclustering, is very rudimentary. This note focuses on clustering. I claim that\nthe most signif- icant challenge for clustering is model selection. In contrast\nwith other common computational tasks, for clustering, dif- ferent algorithms\noften yield drastically different outcomes. Therefore, the choice of a\nclustering algorithm, and their pa- rameters (like the number of clusters) may\nplay a crucial role in the usefulness of an output clustering solution.\nHowever, currently there exists no methodical guidance for clustering\ntool-selection for a given clustering task. Practitioners pick the algorithms\nthey use without awareness to the implications of their choices and the vast\nmajority of theory of clustering papers focus on providing savings to the\nresources needed to solve optimization problems that arise from picking some\nconcrete clustering objective. Saving that pale in com- parison to the costs of\nmismatch between those objectives and the intended use of clustering results. I\nargue the severity of this problem and describe some recent proposals aiming to\naddress this crucial lacuna.",
        "url": "http://arxiv.org/pdf/1805.08838v1.pdf"
    },
    {
        "title": "Infinite-Task Learning with RKHSs",
        "abstract": "Machine learning has witnessed tremendous success in solving tasks depending\non a single hyperparameter. When considering simultaneously a finite number of\ntasks, multi-task learning enables one to account for the similarities of the\ntasks via appropriate regularizers. A step further consists of learning a\ncontinuum of tasks for various loss functions. A promising approach, called\n\\emph{Parametric Task Learning}, has paved the way in the continuum setting for\naffine models and piecewise-linear loss functions. In this work, we introduce a\nnovel approach called \\emph{Infinite Task Learning} whose goal is to learn a\nfunction whose output is a function over the hyperparameter space. We leverage\ntools from operator-valued kernels and the associated vector-valued RKHSs that\nprovide an explicit control over the role of the hyperparameters, and also\nallows us to consider new type of constraints. We provide generalization\nguarantees to the suggested scheme and illustrate its efficiency in\ncost-sensitive classification, quantile regression and density level set\nestimation.",
        "url": "http://arxiv.org/pdf/1805.08809v3.pdf"
    },
    {
        "title": "Super learning in the SAS system",
        "abstract": "Background and objective: Stacking is an ensemble machine learning method that averages predictions from multiple other algorithms, such as generalized linear models and regression trees. An implementation of stacking, called super learning, has been developed as a general approach to supervised learning and has seen frequent usage, in part due to the availability of an R package. We develop super learning in the SAS software system using a new macro, and demonstrate its performance relative to the R package. Methods: Following previous work using the R SuperLearner package we assess the performance of super learning in a number of domains. We compare the R package with the new SAS macro in a small set of simulations assessing curve fitting in a predictive model as well in a set of 14 publicly available datasets to assess cross-validated accuracy. Results: Across the simulated data and the publicly available data, the SAS macro performed similarly to the R package, despite a different set of potential algorithms available natively in R and SAS. Conclusions: Our super learner macro performs as well as the R package at a number of tasks. Further, by extending the macro to include the use of R packages, the macro can leverage both the robust, enterprise oriented procedures in SAS and the nimble, cutting edge packages in R. In the spirit of ensemble learning, this macro extends the potential library of algorithms beyond a single software system and provides a simple avenue into machine learning in SAS.",
        "url": "https://arxiv.org/pdf/1805.08058v3.pdf"
    },
    {
        "title": "Efficient Stochastic Gradient Descent for Learning with Distributionally Robust Optimization",
        "abstract": "Distributionally robust optimization (DRO) problems are increasingly seen as a viable method to train machine learning models for improved model generalization. These min-max formulations, however, are more difficult to solve. We therefore provide a new stochastic gradient descent algorithm to efficiently solve this DRO formulation. Our approach applies gradient descent to the outer minimization formulation and estimates the gradient of the inner maximization based on a sample average approximation. The latter uses a subset of the data in each iteration, progressively increasing the subset size to ensure convergence. Theoretical results include establishing the optimal manner for growing the support size to balance a fundamental tradeoff between stochastic error and computational effort. Empirical results demonstrate the significant benefits of our approach over previous work, and also illustrate how learning with DRO can improve generalization.",
        "url": "https://arxiv.org/pdf/1805.08728v2.pdf"
    },
    {
        "title": "Adversarial Training of Word2Vec for Basket Completion",
        "abstract": "In recent years, the Word2Vec model trained with the Negative Sampling loss\nfunction has shown state-of-the-art results in a number of machine learning\ntasks, including language modeling tasks, such as word analogy and word\nsimilarity, and in recommendation tasks, through Prod2Vec, an extension that\napplies to modeling user shopping activity and user preferences. Several\nmethods that aim to improve upon the standard Negative Sampling loss have been\nproposed. In our paper we pursue more sophisticated Negative Sampling, by\nleveraging ideas from the field of Generative Adversarial Networks (GANs), and\npropose Adversarial Negative Sampling. We build upon the recent progress made\nin stabilizing the training objective of GANs in the discrete data setting, and\nintroduce a new GAN-Word2Vec model.We evaluate our model on the task of basket\ncompletion, and show significant improvements in performance over Word2Vec\ntrained using standard loss functions, including Noise Contrastive Estimation\nand Negative Sampling.",
        "url": "http://arxiv.org/pdf/1805.08720v1.pdf"
    },
    {
        "title": "Information Constraints on Auto-Encoding Variational Bayes",
        "abstract": "Parameterizing the approximate posterior of a generative model with neural\nnetworks has become a common theme in recent machine learning research. While\nproviding appealing flexibility, this approach makes it difficult to impose or\nassess structural constraints such as conditional independence. We propose a\nframework for learning representations that relies on Auto-Encoding Variational\nBayes and whose search space is constrained via kernel-based measures of\nindependence. In particular, our method employs the $d$-variable\nHilbert-Schmidt Independence Criterion (dHSIC) to enforce independence between\nthe latent representations and arbitrary nuisance factors. We show how to apply\nthis method to a range of problems, including the problems of learning\ninvariant representations and the learning of interpretable representations. We\nalso present a full-fledged application to single-cell RNA sequencing\n(scRNA-seq). In this setting the biological signal is mixed in complex ways\nwith sequencing errors and sampling effects. We show that our method\nout-performs the state-of-the-art in this domain.",
        "url": "http://arxiv.org/pdf/1805.08672v4.pdf"
    },
    {
        "title": "Universal discriminative quantum neural networks",
        "abstract": "Quantum mechanics fundamentally forbids deterministic discrimination of\nquantum states and processes. However, the ability to optimally distinguish\nvarious classes of quantum data is an important primitive in quantum\ninformation science. In this work, we train near-term quantum circuits to\nclassify data represented by non-orthogonal quantum probability distributions\nusing the Adam stochastic optimization algorithm. This is achieved by iterative\ninteractions of a classical device with a quantum processor to discover the\nparameters of an unknown non-unitary quantum circuit. This circuit learns to\nsimulates the unknown structure of a generalized quantum measurement, or\nPositive-Operator-Value-Measure (POVM), that is required to optimally\ndistinguish possible distributions of quantum inputs. Notably we use universal\ncircuit topologies, with a theoretically motivated circuit design, which\nguarantees that our circuits can in principle learn to perform arbitrary\ninput-output mappings. Our numerical simulations show that shallow quantum\ncircuits could be trained to discriminate among various pure and mixed quantum\nstates exhibiting a trade-off between minimizing erroneous and inconclusive\noutcomes with comparable performance to theoretically optimal POVMs. We train\nthe circuit on different classes of quantum data and evaluate the\ngeneralization error on unseen mixed quantum states. This generalization power\nhence distinguishes our work from standard circuit optimization and provides an\nexample of quantum machine learning for a task that has inherently no classical\nanalogue.",
        "url": "http://arxiv.org/pdf/1805.08654v1.pdf"
    },
    {
        "title": "Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach",
        "abstract": "Neural networks, a central tool in machine learning, have demonstrated remarkable, high fidelity performance on image recognition and classification tasks. These successes evince an ability to accurately represent high dimensional functions, but rigorous results about the approximation error of neural networks after training are few. Here we establish conditions for global convergence of the standard optimization algorithm used in machine learning applications, stochastic gradient descent (SGD), and quantify the scaling of its error with the size of the network. This is done by reinterpreting SGD as the evolution of a particle system with interactions governed by a potential related to the objective or \"loss\" function used to train the network. We show that, when the number $n$ of units is large, the empirical distribution of the particles descends on a convex landscape towards the global minimum at a rate independent of $n$, with a resulting approximation error that universally scales as $O(n^{-1})$. These properties are established in the form of a Law of Large Numbers and a Central Limit Theorem for the empirical distribution. Our analysis also quantifies the scale and nature of the noise introduced by SGD and provides guidelines for the step size and batch size to use when training a neural network. We illustrate our findings on examples in which we train neural networks to learn the energy function of the continuous 3-spin model on the sphere. The approximation error scales as our analysis predicts in as high a dimension as $d=25$.",
        "url": "https://arxiv.org/pdf/1805.00915v3.pdf"
    },
    {
        "title": "A 2D laser rangefinder scans dataset of standard EUR pallets",
        "abstract": "In the past few years, the technology of automated guided vehicles (AGVs) has\nnotably advanced. In particular, in the context of factory and warehouse\nautomation, different approaches have been presented for detecting and\nlocalizing pallets inside warehouses and shop-floor environments. In a related\nresearch paper [1], we show that an AGVs can detect, localize, and track\npallets using machine learning techniques based only on the data of an on-board\n2D laser rangefinder. Such sensor is very common in industrial scenarios due to\nits simplicity and robustness, but it can only provide a limited amount of\ndata. Therefore, it has been neglected in the past in favor of more complex\nsolutions. In this paper, we release to the community the data we collected in\n[1] for further research activities in the field of pallet localization and\ntracking. The dataset comprises a collection of 565 2D scans from real-world\nenvironments, which are divided into 340 samples where pallets are present, and\n225 samples where they are not. The data have been manually labelled and are\nprovided in different formats.",
        "url": "http://arxiv.org/pdf/1805.08564v2.pdf"
    },
    {
        "title": "Fully Understanding the Hashing Trick",
        "abstract": "Feature hashing, also known as {\\em the hashing trick}, introduced by\nWeinberger et al. (2009), is one of the key techniques used in scaling-up\nmachine learning algorithms. Loosely speaking, feature hashing uses a random\nsparse projection matrix $A : \\mathbb{R}^n \\to \\mathbb{R}^m$ (where $m \\ll n$)\nin order to reduce the dimension of the data from $n$ to $m$ while\napproximately preserving the Euclidean norm. Every column of $A$ contains\nexactly one non-zero entry, equals to either $-1$ or $1$.\n  Weinberger et al. showed tail bounds on $\\|Ax\\|_2^2$. Specifically they\nshowed that for every $\\varepsilon, \\delta$, if $\\|x\\|_{\\infty} / \\|x\\|_2$ is\nsufficiently small, and $m$ is sufficiently large, then $$\\Pr[ \\; |\n\\;\\|Ax\\|_2^2 - \\|x\\|_2^2\\; | < \\varepsilon \\|x\\|_2^2 \\;] \\ge 1 - \\delta \\;.$$\nThese bounds were later extended by Dasgupta \\etal (2010) and most recently\nrefined by Dahlgaard et al. (2017), however, the true nature of the performance\nof this key technique, and specifically the correct tradeoff between the\npivotal parameters $\\|x\\|_{\\infty} / \\|x\\|_2, m, \\varepsilon, \\delta$ remained\nan open question.\n  We settle this question by giving tight asymptotic bounds on the exact\ntradeoff between the central parameters, thus providing a complete\nunderstanding of the performance of feature hashing. We complement the\nasymptotic bound with empirical data, which shows that the constants \"hiding\"\nin the asymptotic notation are, in fact, very close to $1$, thus further\nillustrating the tightness of the presented bounds in practice.",
        "url": "http://arxiv.org/pdf/1805.08539v1.pdf"
    },
    {
        "title": "Learning Topic Models by Neighborhood Aggregation",
        "abstract": "Topic models are frequently used in machine learning owing to their high interpretability and modular structure. However, extending a topic model to include a supervisory signal, to incorporate pre-trained word embedding vectors and to include a nonlinear output function is not an easy task because one has to resort to a highly intricate approximate inference procedure. The present paper shows that topic modeling with pre-trained word embedding vectors can be viewed as implementing a neighborhood aggregation algorithm where messages are passed through a network defined over words. From the network view of topic models, nodes correspond to words in a document and edges correspond to either a relationship describing co-occurring words in a document or a relationship describing the same word in the corpus. The network view allows us to extend the model to include supervisory signals, incorporate pre-trained word embedding vectors and include a nonlinear output function in a simple manner. In experiments, we show that our approach outperforms the state-of-the-art supervised Latent Dirichlet Allocation implementation in terms of held-out document classification tasks.",
        "url": "https://arxiv.org/pdf/1802.08012v6.pdf"
    },
    {
        "title": "Wikipedia for Smart Machines and Double Deep Machine Learning",
        "abstract": "Very important breakthroughs in data centric deep learning algorithms led to\nimpressive performance in transactional point applications of Artificial\nIntelligence (AI) such as Face Recognition, or EKG classification. With all due\nappreciation, however, knowledge blind data only machine learning algorithms\nhave severe limitations for non-transactional AI applications, such as medical\ndiagnosis beyond the EKG results. Such applications require deeper and broader\nknowledge in their problem solving capabilities, e.g. integrating anatomy and\nphysiology knowledge with EKG results and other patient findings. Following a\nreview and illustrations of such limitations for several real life AI\napplications, we point at ways to overcome them. The proposed Wikipedia for\nSmart Machines initiative aims at building repositories of software structures\nthat represent humanity science & technology knowledge in various parts of\nlife; knowledge that we all learn in schools, universities and during our\nprofessional life. Target readers for these repositories are smart machines;\nnot human. AI software developers will have these Reusable Knowledge structures\nreadily available, hence, the proposed name ReKopedia. Big Data is by now a\nmature technology, it is time to focus on Big Knowledge. Some will be derived\nfrom data, some will be obtained from mankind gigantic repository of knowledge.\nWikipedia for smart machines along with the new Double Deep Learning approach\noffer a paradigm for integrating datacentric deep learning algorithms with\nalgorithms that leverage deep knowledge, e.g. evidential reasoning and\ncausality reasoning. For illustration, a project is described to produce\nReKopedia knowledge modules for medical diagnosis of about 1,000 disorders.\nData is important, but knowledge deep, basic, and commonsense is equally\nimportant.",
        "url": "http://arxiv.org/pdf/1711.06517v2.pdf"
    },
    {
        "title": "Adversarial Examples that Fool both Computer Vision and Time-Limited Humans",
        "abstract": "Machine learning models are vulnerable to adversarial examples: small changes\nto images can cause computer vision models to make mistakes such as identifying\na school bus as an ostrich. However, it is still an open question whether\nhumans are prone to similar mistakes. Here, we address this question by\nleveraging recent techniques that transfer adversarial examples from computer\nvision models with known parameters and architecture to other models with\nunknown parameters and architecture, and by matching the initial processing of\nthe human visual system. We find that adversarial examples that strongly\ntransfer across computer vision models influence the classifications made by\ntime-limited human observers.",
        "url": "http://arxiv.org/pdf/1802.08195v3.pdf"
    },
    {
        "title": "Domain Adaptation Using Adversarial Learning for Autonomous Navigation",
        "abstract": "Autonomous navigation has become an increasingly popular machine learning\napplication. Recent advances in deep learning have also resulted in great\nimprovements to autonomous navigation. However, prior outdoor autonomous\nnavigation depends on various expensive sensors or large amounts of real\nlabeled data which is difficult to acquire and sometimes erroneous. The\nobjective of this study is to train an autonomous navigation model that uses a\nsimulator (instead of real labeled data) and an inexpensive monocular camera.\nIn order to exploit the simulator satisfactorily, our proposed method is based\non domain adaptation with adversarial learning. Specifically, we propose our\nmodel with 1) a dilated residual block in the generator, 2) cycle loss, and 3)\nstyle loss to improve the adversarial learning performance for satisfactory\ndomain adaptation. In addition, we perform a theoretical analysis that supports\nthe justification of our proposed method. We present empirical results of\nnavigation in outdoor courses with various intersections using a commercial\nradio controlled car. We observe that our proposed method allows us to learn a\nfavorable navigation model by generating images with realistic textures. To the\nbest of our knowledge, this is the first work to apply domain adaptation with\nadversarial learning to autonomous navigation in real outdoor environments. Our\nproposed method can also be applied to precise image generation or other\nrobotic tasks.",
        "url": "http://arxiv.org/pdf/1712.03742v6.pdf"
    },
    {
        "title": "Interpreting Blackbox Models via Model Extraction",
        "abstract": "Interpretability has become incredibly important as machine learning is\nincreasingly used to inform consequential decisions. We propose to construct\nglobal explanations of complex, blackbox models in the form of a decision tree\napproximating the original model---as long as the decision tree is a good\napproximation, then it mirrors the computation performed by the blackbox model.\nWe devise a novel algorithm for extracting decision tree explanations that\nactively samples new training points to avoid overfitting. We evaluate our\nalgorithm on a random forest to predict diabetes risk and a learned controller\nfor cart-pole. Compared to several baselines, our decision trees are both\nsubstantially more accurate and equally or more interpretable based on a user\nstudy. Finally, we describe several insights provided by our interpretations,\nincluding a causal issue validated by a physician.",
        "url": "http://arxiv.org/pdf/1705.08504v6.pdf"
    },
    {
        "title": "AdGraph: A Graph-Based Approach to Ad and Tracker Blocking",
        "abstract": "User demand for blocking advertising and tracking online is large and growing. Existing tools, both deployed and described in research, have proven useful, but lack either the completeness or robustness needed for a general solution. Existing detection approaches generally focus on only one aspect of advertising or tracking (e.g. URL patterns, code structure), making existing approaches susceptible to evasion. In this work we present AdGraph, a novel graph-based machine learning approach for detecting advertising and tracking resources on the web. AdGraph differs from existing approaches by building a graph representation of the HTML structure, network requests, and JavaScript behavior of a webpage, and using this unique representation to train a classifier for identifying advertising and tracking resources. Because AdGraph considers many aspects of the context a network request takes place in, it is less susceptible to the single-factor evasion techniques that flummox existing approaches. We evaluate AdGraph on the Alexa top-10K websites, and find that it is highly accurate, able to replicate the labels of human-generated filter lists with 95.33% accuracy, and can even identify many mistakes in filter lists. We implement AdGraph as a modification to Chromium. AdGraph adds only minor overhead to page loading and execution, and is actually faster than stock Chromium on 42% of websites and AdBlock Plus on 78% of websites. Overall, we conclude that AdGraph is both accurate enough and performant enough for online use, breaking comparable or fewer websites than popular filter list based approaches.",
        "url": "https://arxiv.org/pdf/1805.09155v2.pdf"
    },
    {
        "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning",
        "abstract": "Adaptive optimization methods, which perform local optimization with a metric\nconstructed from the history of iterates, are becoming increasingly popular for\ntraining deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We\nshow that for simple overparameterized problems, adaptive methods often find\ndrastically different solutions than gradient descent (GD) or stochastic\ngradient descent (SGD). We construct an illustrative binary classification\nproblem where the data is linearly separable, GD and SGD achieve zero test\nerror, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to\nhalf. We additionally study the empirical generalization capability of adaptive\nmethods on several state-of-the-art deep learning models. We observe that the\nsolutions found by adaptive methods generalize worse (often significantly\nworse) than SGD, even when these solutions have better training performance.\nThese results suggest that practitioners should reconsider the use of adaptive\nmethods to train neural networks.",
        "url": "http://arxiv.org/pdf/1705.08292v2.pdf"
    },
    {
        "title": "geomstats: a Python Package for Riemannian Geometry in Machine Learning",
        "abstract": "We introduce geomstats, a python package that performs computations on\nmanifolds such as hyperspheres, hyperbolic spaces, spaces of symmetric positive\ndefinite matrices and Lie groups of transformations. We provide efficient and\nextensively unit-tested implementations of these manifolds, together with\nuseful Riemannian metrics and associated Exponential and Logarithm maps. The\ncorresponding geodesic distances provide a range of intuitive choices of\nMachine Learning loss functions. We also give the corresponding Riemannian\ngradients. The operations implemented in geomstats are available with different\ncomputing backends such as numpy, tensorflow and keras. We have enabled GPU\nimplementation and integrated geomstats manifold computations into keras deep\nlearning framework. This paper also presents a review of manifolds in machine\nlearning and an overview of the geomstats package with examples demonstrating\nits use for efficient and user-friendly Riemannian geometry.",
        "url": "http://arxiv.org/pdf/1805.08308v2.pdf"
    },
    {
        "title": "Robust Decentralized Learning Using ADMM with Unreliable Agents",
        "abstract": "Many machine learning problems can be formulated as consensus optimization\nproblems which can be solved efficiently via a cooperative multi-agent system.\nHowever, the agents in the system can be unreliable due to a variety of\nreasons: noise, faults and attacks. Providing erroneous updates leads the\noptimization process in a wrong direction, and degrades the performance of\ndistributed machine learning algorithms. This paper considers the problem of\ndecentralized learning using ADMM in the presence of unreliable agents. First,\nwe rigorously analyze the effect of erroneous updates (in ADMM learning\niterations) on the convergence behavior of multi-agent system. We show that the\nalgorithm linearly converges to a neighborhood of the optimal solution under\ncertain conditions and characterize the neighborhood size analytically. Next,\nwe provide guidelines for network design to achieve a faster convergence. We\nalso provide conditions on the erroneous updates for exact convergence to the\noptimal solution. Finally, to mitigate the influence of unreliable agents, we\npropose \\textsf{ROAD}, a robust variant of ADMM, and show its resilience to\nunreliable agents with an exact convergence to the optimum.",
        "url": "http://arxiv.org/pdf/1710.05241v3.pdf"
    },
    {
        "title": "The Roles of Supervised Machine Learning in Systems Neuroscience",
        "abstract": "Over the last several years, the use of machine learning (ML) in neuroscience\nhas been rapidly increasing. Here, we review ML's contributions, both realized\nand potential, across several areas of systems neuroscience. We describe four\nprimary roles of ML within neuroscience: 1) creating solutions to engineering\nproblems, 2) identifying predictive variables, 3) setting benchmarks for simple\nmodels of the brain, and 4) serving itself as a model for the brain. The\nbreadth and ease of its applicability suggests that machine learning should be\nin the toolbox of most systems neuroscientists.",
        "url": "http://arxiv.org/pdf/1805.08239v2.pdf"
    },
    {
        "title": "Meta-learning with differentiable closed-form solvers",
        "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures. Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.",
        "url": "https://arxiv.org/pdf/1805.08136v3.pdf"
    },
    {
        "title": "Understanding Self-Paced Learning under Concave Conjugacy Theory",
        "abstract": "By simulating the easy-to-hard learning manners of humans/animals, the\nlearning regimes called curriculum learning~(CL) and self-paced learning~(SPL)\nhave been recently investigated and invoked broad interests. However, the\nintrinsic mechanism for analyzing why such learning regimes can work has not\nbeen comprehensively investigated. To this issue, this paper proposes a concave\nconjugacy theory for looking into the insight of CL/SPL. Specifically, by using\nthis theory, we prove the equivalence of the SPL regime and a latent concave\nobjective, which is closely related to the known non-convex regularized penalty\nwidely used in statistics and machine learning. Beyond the previous theory for\nexplaining CL/SPL insights, this new theoretical framework on one hand\nfacilitates two direct approaches for designing new SPL models for certain\ntasks, and on the other hand can help conduct the latent objective of\nself-paced curriculum learning, which is the advanced version of both CL/SPL\nand possess advantages of both learning regimes to a certain extent. This\nfurther facilitates a theoretical understanding for SPCL, instead of only\nCL/SPL as conventional. Under this theory, we attempt to attain intrinsic\nlatent objectives of two curriculum forms, the partial order and group\ncurriculums, which easily follow the theoretical understanding of the\ncorresponding SPCL regimes.",
        "url": "http://arxiv.org/pdf/1805.08096v1.pdf"
    },
    {
        "title": "GANE: A Generative Adversarial Network Embedding",
        "abstract": "Network embedding has become a hot research topic recently which can provide\nlow-dimensional feature representations for many machine learning applications.\nCurrent work focuses on either (1) whether the embedding is designed as an\nunsupervised learning task by explicitly preserving the structural connectivity\nin the network, or (2) whether the embedding is a by-product during the\nsupervised learning of a specific discriminative task in a deep neural network.\nIn this paper, we focus on bridging the gap of the two lines of the research.\nWe propose to adapt the Generative Adversarial model to perform network\nembedding, in which the generator is trying to generate vertex pairs, while the\ndiscriminator tries to distinguish the generated vertex pairs from real\nconnections (edges) in the network. Wasserstein-1 distance is adopted to train\nthe generator to gain better stability. We develop three variations of models,\nincluding GANE which applies cosine similarity, GANE-O1 which preserves the\nfirst-order proximity, and GANE-O2 which tries to preserves the second-order\nproximity of the network in the low-dimensional embedded vector space. We later\nprove that GANE-O2 has the same objective function as GANE-O1 when negative\nsampling is applied to simplify the training process in GANE-O2. Experiments\nwith real-world network datasets demonstrate that our models constantly\noutperform state-of-the-art solutions with significant improvements on\nprecision in link prediction, as well as on visualizations and accuracy in\nclustering tasks.",
        "url": "http://arxiv.org/pdf/1805.07324v2.pdf"
    },
    {
        "title": "Benchmarking Decoupled Neural Interfaces with Synthetic Gradients",
        "abstract": "Artifical Neural Networks are a particular class of learning systems modeled\nafter biological neural functions with an interesting penchant for Hebbian\nlearning, that is \"neurons that wire together, fire together\". However, unlike\ntheir natural counterparts, artificial neural networks have a close and\nstringent coupling between the modules of neurons in the network. This coupling\nor locking imposes upon the network a strict and inflexible structure that\nprevent layers in the network from updating their weights until a full\nfeed-forward and backward pass has occurred. Such a constraint though may have\nsufficed for a while, is now no longer feasible in the era of very-large-scale\nmachine learning, coupled with the increased desire for parallelization of the\nlearning process across multiple computing infrastructures. To solve this\nproblem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are\nintroduced as a viable alternative to the backpropagation algorithm. This paper\nperforms a speed benchmark to compare the speed and accuracy capabilities of\nSG-DNI as opposed to a standard neural interface using multilayer perceptron\nMLP. SG-DNI shows good promise, in that it not only captures the learning\nproblem, it is also over 3-fold faster due to it asynchronous learning\ncapabilities.",
        "url": "http://arxiv.org/pdf/1712.08314v3.pdf"
    },
    {
        "title": "Predicting Electricity Outages Caused by Convective Storms",
        "abstract": "We consider the problem of predicting power outages in an electrical power\ngrid due to hazards produced by convective storms. These storms produce extreme\nweather phenomena such as intense wind, tornadoes and lightning over a small\narea. In this paper, we discuss the application of state-of-the-art machine\nlearning techniques, such as random forest classifiers and deep neural\nnetworks, to predict the amount of damage caused by storms. We cast this\napplication as a classification problem where the goal is to classify storm\ncells into a finite number of classes, each corresponding to a certain amount\nof expected damage. The classification method use as input features estimates\nfor storm cell location and movement which has to be extracted from the raw\ndata.\n  A main challenge of this application is that the training data is heavily\nimbalanced as the occurrence of extreme weather events is rare. In order to\naddress this issue, we applied SMOTE technique.",
        "url": "http://arxiv.org/pdf/1805.07897v1.pdf"
    },
    {
        "title": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples",
        "abstract": "Adversarial perturbations of normal images are usually imperceptible to\nhumans, but they can seriously confuse state-of-the-art machine learning\nmodels. What makes them so special in the eyes of image classifiers? In this\npaper, we show empirically that adversarial examples mainly lie in the low\nprobability regions of the training distribution, regardless of attack types\nand targeted models. Using statistical hypothesis testing, we find that modern\nneural density models are surprisingly good at detecting imperceptible image\nperturbations. Based on this discovery, we devised PixelDefend, a new approach\nthat purifies a maliciously perturbed image by moving it back towards the\ndistribution seen in the training data. The purified image is then run through\nan unmodified classifier, making our method agnostic to both the classifier and\nthe attacking method. As a result, PixelDefend can be used to protect already\ndeployed models and be combined with other model-specific defenses. Experiments\nshow that our method greatly improves resilience across a wide variety of\nstate-of-the-art attacking methods, increasing accuracy on the strongest attack\nfrom 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.",
        "url": "http://arxiv.org/pdf/1710.10766v3.pdf"
    },
    {
        "title": "Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing",
        "abstract": "Machine Learning has been the quintessential solution for many AI problems,\nbut learning is still heavily dependent on the specific training data. Some\nlearning models can be incorporated with a prior knowledge in the Bayesian set\nup, but these learning models do not have the ability to access any organised\nworld knowledge on demand. In this work, we propose to enhance learning models\nwith world knowledge in the form of Knowledge Graph (KG) fact triples for\nNatural Language Processing (NLP) tasks. Our aim is to develop a deep learning\nmodel that can extract relevant prior support facts from knowledge graphs\ndepending on the task using attention mechanism. We introduce a\nconvolution-based model for learning representations of knowledge graph entity\nand relation clusters in order to reduce the attention space. We show that the\nproposed method is highly scalable to the amount of prior information that has\nto be processed and can be applied to any generic NLP task. Using this method\nwe show significant improvement in performance for text classification with\nNews20, DBPedia datasets and natural language inference with Stanford Natural\nLanguage Inference (SNLI) dataset. We also demonstrate that a deep learning\nmodel can be trained well with substantially less amount of labeled training\ndata, when it has access to organised world knowledge in the form of knowledge\ngraph.",
        "url": "http://arxiv.org/pdf/1802.05930v2.pdf"
    },
    {
        "title": "GSAE: an autoencoder with embedded gene-set nodes for genomics functional characterization",
        "abstract": "Bioinformatics tools have been developed to interpret gene expression data at\nthe gene set level, and these gene set based analyses improve the biologists'\ncapability to discover functional relevance of their experiment design. While\nelucidating gene set individually, inter gene sets association is rarely taken\ninto consideration. Deep learning, an emerging machine learning technique in\ncomputational biology, can be used to generate an unbiased combination of gene\nset, and to determine the biological relevance and analysis consistency of\nthese combining gene sets by leveraging large genomic data sets. In this study,\nwe proposed a gene superset autoencoder (GSAE), a multi-layer autoencoder model\nwith the incorporation of a priori defined gene sets that retain the crucial\nbiological features in the latent layer. We introduced the concept of the gene\nsuperset, an unbiased combination of gene sets with weights trained by the\nautoencoder, where each node in the latent layer is a superset. Trained with\ngenomic data from TCGA and evaluated with their accompanying clinical\nparameters, we showed gene supersets' ability of discriminating tumor subtypes\nand their prognostic capability. We further demonstrated the biological\nrelevance of the top component gene sets in the significant supersets. Using\nautoencoder model and gene superset at its latent layer, we demonstrated that\ngene supersets retain sufficient biological information with respect to tumor\nsubtypes and clinical prognostic significance. Superset also provides high\nreproducibility on survival analysis and accurate prediction for cancer\nsubtypes.",
        "url": "http://arxiv.org/pdf/1805.07874v2.pdf"
    },
    {
        "title": "Knowledge Discovery from Layered Neural Networks based on Non-negative Task Decomposition",
        "abstract": "Interpretability has become an important issue in the machine learning field,\nalong with the success of layered neural networks in various practical tasks.\nSince a trained layered neural network consists of a complex nonlinear\nrelationship between large number of parameters, we failed to understand how\nthey could achieve input-output mappings with a given data set. In this paper,\nwe propose the non-negative task decomposition method, which applies\nnon-negative matrix factorization to a trained layered neural network. This\nenables us to decompose the inference mechanism of a trained layered neural\nnetwork into multiple principal tasks of input-output mapping, and reveal the\nroles of hidden units in terms of their contribution to each principal task.",
        "url": "http://arxiv.org/pdf/1805.07137v2.pdf"
    },
    {
        "title": "Adversarial Attacks Against Medical Deep Learning Systems",
        "abstract": "The discovery of adversarial examples has raised concerns about the practical\ndeployment of deep learning systems. In this paper, we demonstrate that\nadversarial examples are capable of manipulating deep learning systems across\nthree clinical domains. For each of our representative medical deep learning\nclassifiers, both white and black box attacks were highly successful. Our\nmodels are representative of the current state of the art in medical computer\nvision and, in some cases, directly reflect architectures already seeing\ndeployment in real world clinical settings. In addition to the technical\ncontribution of our paper, we synthesize a large body of knowledge about the\nhealthcare system to argue that medicine may be uniquely susceptible to\nadversarial attacks, both in terms of monetary incentives and technical\nvulnerability. To this end, we outline the healthcare economy and the\nincentives it creates for fraud and provide concrete examples of how and why\nsuch attacks could be realistically carried out. We urge practitioners to be\naware of current vulnerabilities when deploying deep learning systems in\nclinical settings, and encourage the machine learning community to further\ninvestigate the domain-specific characteristics of medical learning systems.",
        "url": "http://arxiv.org/pdf/1804.05296v3.pdf"
    },
    {
        "title": "Projection-Free Algorithms in Statistical Estimation",
        "abstract": "Frank-Wolfe algorithm (FW) and its variants have gained a surge of interests\nin machine learning community due to its projection-free property. Recently\npeople have reduced the gradient evaluation complexity of FW algorithm to\n$\\log(\\frac{1}{\\epsilon})$ for the smooth and strongly convex objective. This\ncomplexity result is especially significant in learning problem, as the\noverwhelming data size makes a single evluation of gradient computational\nexpensive. However, in high-dimensional statistical estimation problems, the\nobjective is typically not strongly convex, and sometimes even non-convex. In\nthis paper, we extend the state-of-the-art FW type algorithms for the\nlarge-scale, high-dimensional estimation problem. We show that as long as the\nobjective satisfies {\\em restricted strong convexity}, and we are not\noptimizing over statistical limit of the model, the $\\log(\\frac{1}{\\epsilon})$\ngradient evaluation complexity could still be attained.",
        "url": "http://arxiv.org/pdf/1805.07844v1.pdf"
    },
    {
        "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning",
        "abstract": "There is an increasing need to bring machine learning to a wide diversity of\nhardware devices. Current frameworks rely on vendor-specific operator libraries\nand optimize for a narrow range of server-class GPUs. Deploying workloads to\nnew platforms -- such as mobile phones, embedded devices, and accelerators\n(e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a\ncompiler that exposes graph-level and operator-level optimizations to provide\nperformance portability to deep learning workloads across diverse hardware\nback-ends. TVM solves optimization challenges specific to deep learning, such\nas high-level operator fusion, mapping to arbitrary hardware primitives, and\nmemory latency hiding. It also automates optimization of low-level programs to\nhardware characteristics by employing a novel, learning-based cost modeling\nmethod for rapid exploration of code optimizations. Experimental results show\nthat TVM delivers performance across hardware back-ends that are competitive\nwith state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and\nserver-class GPUs. We also demonstrate TVM's ability to target new accelerator\nback-ends, such as the FPGA-based generic deep learning accelerator. The system\nis open sourced and in production use inside several major companies.",
        "url": "http://arxiv.org/pdf/1802.04799v3.pdf"
    },
    {
        "title": "Machine Learning: Basic Principles",
        "abstract": "This tutorial is based on the lecture notes for, and the plentiful student\nfeedback received from, the courses \"Machine Learning: Basic Principles\" and\n\"Artificial Intelligence\", which I have co-taught since 2015 at Aalto\nUniversity. The aim is to provide an accessible introduction to some of the\nmain concepts and methods within machine learning. Many of the current systems\nwhich are considered as (artificially) intelligent are based on combinations of\nfew basic machine learning methods. After formalizing the main building blocks\nof a machine learning problem, some popular algorithmic design patterns for\nmachine learning methods are discussed in some detail.",
        "url": "http://arxiv.org/pdf/1805.05052v9.pdf"
    },
    {
        "title": "Model Aggregation via Good-Enough Model Spaces",
        "abstract": "In many applications, the training data for a machine learning task is partitioned across multiple nodes, and aggregating this data may be infeasible due to communication, privacy, or storage constraints. Existing distributed optimization methods for learning global models in these settings typically aggregate local updates from each node in an iterative fashion. However, these approaches require many rounds of communication between nodes, and assume that updates can be synchronously shared across a connected network. In this work, we present Good-Enough Model Spaces (GEMS), a novel framework for learning a global model by carefully intersecting the sets of \"good-enough\" models across each node. Our approach utilizes minimal communication and does not require sharing of data between nodes. We present methods for learning both convex models and neural networks within this framework and discuss how small samples of held-out data can be used for post-learning fine-tuning. In experiments on image and medical datasets, our approach on average improves upon other baseline aggregation techniques such as ensembling or model averaging by as much as 15 points (accuracy).",
        "url": "https://arxiv.org/pdf/1805.07782v3.pdf"
    },
    {
        "title": "Edge Attention-based Multi-Relational Graph Convolutional Networks",
        "abstract": "Graph convolutional network (GCN) is generalization of convolutional neural\nnetwork (CNN) to work with arbitrarily structured graphs. A binary adjacency\nmatrix is commonly used in training a GCN. Recently, the attention mechanism\nallows the network to learn a dynamic and adaptive aggregation of the\nneighborhood. We propose a new GCN model on the graphs where edges are\ncharacterized in multiple views or precisely in terms of multiple\nrelationships. For instance, in chemical graph theory, compound structures are\noften represented by the hydrogen-depleted molecular graph where nodes\ncorrespond to atoms and edges correspond to chemical bonds. Multiple attributes\ncan be important to characterize chemical bonds, such as atom pair (the types\nof atoms that a bond connects), aromaticity, and whether a bond is in a ring.\nThe different attributes lead to different graph representations for the same\nmolecule. There is growing interests in both chemistry and machine learning\nfields to directly learn molecular properties of compounds from the molecular\ngraph, instead of from fingerprints predefined by chemists. The proposed GCN\nmodel, which we call edge attention-based multi-relational GCN (EAGCN), jointly\nlearns attention weights and node features in graph convolution. For each bond\nattribute, a real-valued attention matrix is used to replace the binary\nadjacency matrix. By designing a dictionary for the edge attention, and forming\nthe attention matrix of each molecule by looking up the dictionary, the EAGCN\nexploits correspondence between bonds in different molecules. The prediction of\ncompound properties is based on the aggregated node features, which is\nindependent of the varying molecule (graph) size. We demonstrate the efficacy\nof the EAGCN on multiple chemical datasets: Tox21, HIV, Freesolv, and\nLipophilicity, and interpret the resultant attention weights.",
        "url": "http://arxiv.org/pdf/1802.04944v2.pdf"
    },
    {
        "title": "Minimax Lower Bounds for Cost Sensitive Classification",
        "abstract": "The cost-sensitive classification problem plays a crucial role in\nmission-critical machine learning applications, and differs with traditional\nclassification by taking the misclassification costs into consideration.\nAlthough being studied extensively in the literature, the fundamental limits of\nthis problem are still not well understood. We investigate the hardness of this\nproblem by extending the standard minimax lower bound of balanced binary\nclassification problem (due to \\cite{massart2006risk}), and emphasize the\nimpact of cost terms on the hardness.",
        "url": "http://arxiv.org/pdf/1805.07723v1.pdf"
    },
    {
        "title": "Machine Learning for Predictive Analytics of Compute Cluster Jobs",
        "abstract": "We address the problem of predicting whether sufficient memory and CPU\nresources have been requested for jobs at submission time. For this purpose, we\nexamine the task of training a supervised machine learning system to predict\nthe outcome - whether the job will fail specifically due to insufficient\nresources - as a classification task. Sufficiently high accuracy, precision,\nand recall at this task facilitates more anticipatory decision support\napplications in the domain of HPC resource allocation. Our preliminary results\nusing a new test bed show that the probability of failed jobs is associated\nwith information freely available at job submission time and may thus be usable\nby a learning system for user modeling that gives personalized feedback to\nusers.",
        "url": "http://arxiv.org/pdf/1806.01116v1.pdf"
    },
    {
        "title": "Consistent Estimation of Propensity Score Functions with Oversampled Exposed Subjects",
        "abstract": "Observational cohort studies with oversampled exposed subjects are typically\nimplemented to understand the causal effect of a rare exposure. Because the\ndistribution of exposed subjects in the sample differs from the source\npopulation, estimation of a propensity score function (i.e., probability of\nexposure given baseline covariates) targets a nonparametrically nonidentifiable\nparameter. Consistent estimation of propensity score functions is an important\ncomponent of various causal inference estimators, including double robust\nmachine learning and inverse probability weighted estimators. This paper\ndevelops the use of the probability of exposure from the source population in a\nflexible computational implementation that can be used with any algorithm that\nallows observation weighting to produce consistent estimators of propensity\nscore functions. Simulation studies and a hypothetical health policy\nintervention data analysis demonstrate low empirical bias and variance for\nthese propensity score function estimators with observation weights in finite\nsamples.",
        "url": "http://arxiv.org/pdf/1805.07684v2.pdf"
    },
    {
        "title": "Learning Graph-Level Representations with Recurrent Neural Networks",
        "abstract": "Recently a variety of methods have been developed to encode graphs into\nlow-dimensional vectors that can be easily exploited by machine learning\nalgorithms. The majority of these methods start by embedding the graph nodes\ninto a low-dimensional vector space, followed by using some scheme to aggregate\nthe node embeddings. In this work, we develop a new approach to learn\ngraph-level representations, which includes a combination of unsupervised and\nsupervised learning components. We start by learning a set of node\nrepresentations in an unsupervised fashion. Graph nodes are mapped into node\nsequences sampled from random walk approaches approximated by the\nGumbel-Softmax distribution. Recurrent neural network (RNN) units are modified\nto accommodate both the node representations as well as their neighborhood\ninformation. Experiments on standard graph classification benchmarks\ndemonstrate that our proposed approach achieves superior or comparable\nperformance relative to the state-of-the-art algorithms in terms of convergence\nspeed and classification accuracy. We further illustrate the effectiveness of\nthe different components used by our approach.",
        "url": "http://arxiv.org/pdf/1805.07683v4.pdf"
    },
    {
        "title": "Direct Runge-Kutta Discretization Achieves Acceleration",
        "abstract": "We study gradient-based optimization methods obtained by directly\ndiscretizing a second-order ordinary differential equation (ODE) related to the\ncontinuous limit of Nesterov's accelerated gradient method. When the function\nis smooth enough, we show that acceleration can be achieved by a stable\ndiscretization of this ODE using standard Runge-Kutta integrators.\nSpecifically, we prove that under Lipschitz-gradient, convexity and\norder-$(s+2)$ differentiability assumptions, the sequence of iterates generated\nby discretizing the proposed second-order ODE converges to the optimal solution\nat a rate of $\\mathcal{O}({N^{-2\\frac{s}{s+1}}})$, where $s$ is the order of\nthe Runge-Kutta numerical integrator. Furthermore, we introduce a new local\nflatness condition on the objective, under which rates even faster than\n$\\mathcal{O}(N^{-2})$ can be achieved with low-order integrators and only\ngradient information. Notably, this flatness condition is satisfied by several\nstandard loss functions used in machine learning. We provide numerical\nexperiments that verify the theoretical rates predicted by our results.",
        "url": "http://arxiv.org/pdf/1805.00521v5.pdf"
    },
    {
        "title": "Distributed Stochastic Optimization via Adaptive SGD",
        "abstract": "Stochastic convex optimization algorithms are the most popular way to train\nmachine learning models on large-scale data. Scaling up the training process of\nthese models is crucial, but the most popular algorithm, Stochastic Gradient\nDescent (SGD), is a serial method that is surprisingly hard to parallelize. In\nthis paper, we propose an efficient distributed stochastic optimization method\nby combining adaptivity with variance reduction techniques. Our analysis yields\na linear speedup in the number of machines, constant memory footprint, and only\na logarithmic number of communication rounds. Critically, our approach is a\nblack-box reduction that parallelizes any serial online learning algorithm,\nstreamlining prior analysis and allowing us to leverage the significant\nprogress that has been made in designing adaptive algorithms. In particular, we\nachieve optimal convergence rates without any prior knowledge of smoothness\nparameters, yielding a more robust algorithm that reduces the need for\nhyperparameter tuning. We implement our algorithm in the Spark distributed\nframework and exhibit dramatic performance gains on large-scale logistic\nregression problems.",
        "url": "http://arxiv.org/pdf/1802.05811v3.pdf"
    },
    {
        "title": "Capturing human category representations by sampling in deep feature spaces",
        "abstract": "Understanding how people represent categories is a core problem in cognitive\nscience. Decades of research have yielded a variety of formal theories of\ncategories, but validating them with naturalistic stimuli is difficult. The\nchallenge is that human category representations cannot be directly observed\nand running informative experiments with naturalistic stimuli such as images\nrequires a workable representation of these stimuli. Deep neural networks have\nrecently been successful in solving a range of computer vision tasks and\nprovide a way to compactly represent image features. Here, we introduce a\nmethod to estimate the structure of human categories that combines ideas from\ncognitive science and machine learning, blending human-based algorithms with\nstate-of-the-art deep image generators. We provide qualitative and quantitative\nresults as a proof-of-concept for the method's feasibility. Samples drawn from\nhuman distributions rival those from state-of-the-art generative models in\nquality and outperform alternative methods for estimating the structure of\nhuman categories.",
        "url": "http://arxiv.org/pdf/1805.07644v1.pdf"
    },
    {
        "title": "NtMalDetect: A Machine Learning Approach to Malware Detection Using Native API System Calls",
        "abstract": "As computing systems become increasingly advanced and as users increasingly\nengage themselves in technology, security has never been a greater concern. In\nmalware detection, static analysis, the method of analyzing potentially\nmalicious files, has been the prominent approach. This approach, however,\nquickly falls short as malicious programs become more advanced and adopt the\ncapabilities of obfuscating its binaries to execute the same malicious\nfunctions, making static analysis extremely difficult for newer variants. The\napproach assessed in this paper is a novel dynamic malware analysis method,\nwhich may generalize better than static analysis to newer variants. Inspired by\nrecent successes in Natural Language Processing (NLP), widely used document\nclassification techniques were assessed in detecting malware by doing such\nanalysis on system calls, which contain useful information about the operation\nof a program as requests that the program makes of the kernel. Features\nconsidered are extracted from system call traces of benign and malicious\nprograms, and the task to classify these traces is treated as a binary document\nclassification task of system call traces. The system call traces were\nprocessed to remove the parameters to only leave the system call function\nnames. The features were grouped into various n-grams and weighted with Term\nFrequency-Inverse Document Frequency. This paper shows that Linear Support\nVector Machines (SVM) optimized by Stochastic Gradient Descent and the\ntraditional Coordinate Descent on the Wolfe Dual form of the SVM are effective\nin this approach, achieving a highest of 96% accuracy with 95% recall score.\nAdditional contributions include the identification of significant system call\nsequences that could be avenues for further research.",
        "url": "http://arxiv.org/pdf/1802.05412v2.pdf"
    },
    {
        "title": "Robust Optimization over Multiple Domains",
        "abstract": "In this work, we study the problem of learning a single model for multiple\ndomains. Unlike the conventional machine learning scenario where each domain\ncan have the corresponding model, multiple domains (i.e., applications/users)\nmay share the same machine learning model due to maintenance loads in cloud\ncomputing services. For example, a digit-recognition model should be applicable\nto hand-written digits, house numbers, car plates, etc. Therefore, an ideal\nmodel for cloud computing has to perform well at each applicable domain. To\naddress this new challenge from cloud computing, we develop a framework of\nrobust optimization over multiple domains. In lieu of minimizing the empirical\nrisk, we aim to learn a model optimized to the adversarial distribution over\nmultiple domains. Hence, we propose to learn the model and the adversarial\ndistribution simultaneously with the stochastic algorithm for efficiency.\nTheoretically, we analyze the convergence rate for convex and non-convex\nmodels. To our best knowledge, we first study the convergence rate of learning\na robust non-convex model with a practical algorithm. Furthermore, we\ndemonstrate that the robustness of the framework and the convergence rate can\nbe further enhanced by appropriate regularizers over the adversarial\ndistribution. The empirical study on real-world fine-grained visual\ncategorization and digits recognition tasks verifies the effectiveness and\nefficiency of the proposed framework.",
        "url": "http://arxiv.org/pdf/1805.07588v2.pdf"
    },
    {
        "title": "Chief complaint classification with recurrent neural networks",
        "abstract": "Syndromic surveillance detects and monitors individual and population health\nindicators through sources such as emergency department records. Automated\nclassification of these records can improve outbreak detection speed and\ndiagnosis accuracy. Current syndromic systems rely on hand-coded keyword-based\nmethods to parse written fields and may benefit from the use of modern\nsupervised-learning classifier models. In this paper we implement two recurrent\nneural network models based on long short-term memory (LSTM) and gated\nrecurrent unit (GRU) cells and compare them to two traditional bag-of-words\nclassifiers: multinomial naive Bayes (MNB) and a support vector machine (SVM).\nThe MNB classifier is one of only two machine learning algorithms currently\nbeing used for syndromic surveillance. All four models are trained to predict\ndiagnostic code groups as defined by Clinical Classification Software, first to\npredict from discharge diagnosis, then from chief complaint fields. The\nclassifiers are trained on 3.6 million de-identified emergency department\nrecords from a single United States jurisdiction. We compare performance of\nthese models primarily using the F1 score. Using discharge diagnoses, the LSTM\nclassifier performs best, though all models exhibit an F1 score above 96.00.\nThe GRU performs best on chief complaints (F1=47.38), and MNB with bigrams\nperforms worst (F1=39.40). Certain syndrome types are easier to detect than\nothers. For examples, chief complaints using the GRU model predicts\nalcohol-related disorders well (F1=78.91) but predicts influenza poorly\n(F1=14.80). In all instances, the RNN models outperformed the bag-of-word\nclassifiers, suggesting deep learning models could substantially improve the\nautomatic classification of unstructured text for syndromic surveillance.",
        "url": "http://arxiv.org/pdf/1805.07574v2.pdf"
    },
    {
        "title": "Reconciled Polynomial Machine: A Unified Representation of Shallow and Deep Learning Models",
        "abstract": "In this paper, we aim at introducing a new machine learning model, namely\nreconciled polynomial machine, which can provide a unified representation of\nexisting shallow and deep machine learning models. Reconciled polynomial\nmachine predicts the output by computing the inner product of the feature\nkernel function and variable reconciling function. Analysis of several concrete\nmodels, including Linear Models, FM, MVM, Perceptron, MLP and Deep Neural\nNetworks, will be provided in this paper, which can all be reduced to the\nreconciled polynomial machine representations. Detailed analysis of the\nlearning error by these models will also be illustrated in this paper based on\ntheir reduced representations from the function approximation perspective.",
        "url": "http://arxiv.org/pdf/1805.07507v1.pdf"
    },
    {
        "title": "Datasheets for Datasets",
        "abstract": "The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.",
        "url": "https://arxiv.org/pdf/1803.09010v8.pdf"
    },
    {
        "title": "Number Sequence Prediction Problems for Evaluating Computational Powers of Neural Networks",
        "abstract": "Inspired by number series tests to measure human intelligence, we suggest\nnumber sequence prediction tasks to assess neural network models' computational\npowers for solving algorithmic problems. We define the complexity and\ndifficulty of a number sequence prediction task with the structure of the\nsmallest automaton that can generate the sequence. We suggest two types of\nnumber sequence prediction problems: the number-level and the digit-level\nproblems. The number-level problems format sequences as 2-dimensional grids of\ndigits and the digit-level problems provide a single digit input per a time\nstep. The complexity of a number-level sequence prediction can be defined with\nthe depth of an equivalent combinatorial logic, and the complexity of a\ndigit-level sequence prediction can be defined with an equivalent state\nautomaton for the generation rule. Experiments with number-level sequences\nsuggest that CNN models are capable of learning the compound operations of\nsequence generation rules, but the depths of the compound operations are\nlimited. For the digit-level problems, simple GRU and LSTM models can solve\nsome problems with the complexity of finite state automata. Memory augmented\nmodels such as Stack-RNN, Attention, and Neural Turing Machines can solve the\nreverse-order task which has the complexity of simple pushdown automaton.\nHowever, all of above cannot solve general Fibonacci, Arithmetic or Geometric\nsequence generation problems that represent the complexity of queue automata or\nTuring machines. The results show that our number sequence prediction problems\neffectively evaluate machine learning models' computational capabilities.",
        "url": "http://arxiv.org/pdf/1805.07494v2.pdf"
    },
    {
        "title": "A General Framework for Abstention Under Label Shift",
        "abstract": "In safety-critical applications of machine learning, it is often important to abstain from making predictions on low confidence examples. Standard abstention methods tend to be focused on optimizing top-k accuracy, but in many applications, accuracy is not the metric of interest. Further, label shift (a shift in class proportions between training time and prediction time) is ubiquitous in practical settings, and existing abstention methods do not handle label shift well. In this work, we present a general framework for abstention that can be applied to optimize any metric of interest, that is adaptable to label shift at test time, and that works out-of-the-box with any classifier that can be calibrated. Our approach leverages recent reports that calibrated probability estimates can be used as a proxy for the true class labels, thereby allowing us to estimate the change in an arbitrary metric if an example were abstained on. We present computationally efficient algorithms under our framework to optimize sensitivity at a target specificity, auROC, and the weighted Cohen's Kappa, and introduce a novel strong baseline based on JS divergence from prior class probabilities. Experiments on synthetic, biological, and clinical data support our findings.",
        "url": "https://arxiv.org/pdf/1802.07024v5.pdf"
    },
    {
        "title": "Butterfly-Net: Optimal Function Representation Based on Convolutional Neural Networks",
        "abstract": "Deep networks, especially convolutional neural networks (CNNs), have been successfully applied in various areas of machine learning as well as to challenging problems in other scientific and engineering fields. This paper introduces Butterfly-Net, a low-complexity CNN with structured and sparse cross-channel connections, together with a Butterfly initialization strategy for a family of networks. Theoretical analysis of the approximation power of Butterfly-Net to the Fourier representation of input data shows that the error decays exponentially as the depth increases. Combining Butterfly-Net with a fully connected neural network, a large class of problems are proved to be well approximated with network complexity depending on the effective frequency bandwidth instead of the input dimension. Regular CNN is covered as a special case in our analysis. Numerical experiments validate the analytical results on the approximation of Fourier kernels and energy functionals of Poisson's equations. Moreover, all experiments support that training from Butterfly initialization outperforms training from random initialization. Also, adding the remaining cross-channel connections, although significantly increase the parameter number, does not much improve the post-training accuracy and is more sensitive to data distribution.",
        "url": "https://arxiv.org/pdf/1805.07451v4.pdf"
    },
    {
        "title": "DeepLogic: Towards End-to-End Differentiable Logical Reasoning",
        "abstract": "Combining machine learning with logic-based expert systems in order to get\nthe best of both worlds are becoming increasingly popular. However, to what\nextent machine learning can already learn to reason over rule-based knowledge\nis still an open problem. In this paper, we explore how symbolic logic, defined\nas logic programs at a character level, is learned to be represented in a\nhigh-dimensional vector space using RNN-based iterative neural networks to\nperform reasoning. We create a new dataset that defines 12 classes of logic\nprograms exemplifying increased level of complexity of logical reasoning and\ntrain the networks in an end-to-end fashion to learn whether a logic program\nentails a given query. We analyse how learning the inference algorithm gives\nrise to representations of atoms, literals and rules within logic programs and\nevaluate against increasing lengths of predicate and constant symbols as well\nas increasing steps of multi-hop reasoning.",
        "url": "http://arxiv.org/pdf/1805.07433v3.pdf"
    },
    {
        "title": "RF-PUF: Enhancing IoT Security through Authentication of Wireless Nodes using In-situ Machine Learning",
        "abstract": "Traditional authentication in radio-frequency (RF) systems enable secure data\ncommunication within a network through techniques such as digital signatures\nand hash-based message authentication codes (HMAC), which suffer from key\nrecovery attacks. State-of-the-art IoT networks such as Nest also use Open\nAuthentication (OAuth 2.0) protocols that are vulnerable to cross-site-recovery\nforgery (CSRF), which shows that these techniques may not prevent an adversary\nfrom copying or modeling the secret IDs or encryption keys using invasive, side\nchannel, learning or software attacks. Physical unclonable functions (PUF), on\nthe other hand, can exploit manufacturing process variations to uniquely\nidentify silicon chips which makes a PUF-based system extremely robust and\nsecure at low cost, as it is practically impossible to replicate the same\nsilicon characteristics across dies. Taking inspiration from human\ncommunication, which utilizes inherent variations in the voice signatures to\nidentify a certain speaker, we present RF- PUF: a deep neural network-based\nframework that allows real-time authentication of wireless nodes, using the\neffects of inherent process variation on RF properties of the wireless\ntransmitters (Tx), detected through in-situ machine learning at the receiver\n(Rx) end. The proposed method utilizes the already-existing asymmetric RF\ncommunication framework and does not require any additional circuitry for PUF\ngeneration or feature extraction. Simulation results involving the process\nvariations in a standard 65 nm technology node, and features such as LO offset\nand I-Q imbalance detected with a neural network having 50 neurons in the\nhidden layer indicate that the framework can distinguish up to 4800\ntransmitters with an accuracy of 99.9% (~ 99% for 10,000 transmitters) under\nvarying channel conditions, and without the need for traditional preambles.",
        "url": "http://arxiv.org/pdf/1805.01374v3.pdf"
    },
    {
        "title": "Semantic Adversarial Deep Learning",
        "abstract": "Fueled by massive amounts of data, models produced by machine-learning (ML)\nalgorithms, especially deep neural networks, are being used in diverse domains\nwhere trustworthiness is a concern, including automotive systems, finance,\nhealth care, natural language processing, and malware detection. Of particular\nconcern is the use of ML algorithms in cyber-physical systems (CPS), such as\nself-driving cars and aviation, where an adversary can cause serious\nconsequences. However, existing approaches to generating adversarial examples\nand devising robust ML algorithms mostly ignore the semantics and context of\nthe overall system containing the ML component. For example, in an autonomous\nvehicle using deep learning for perception, not every adversarial example for\nthe neural network might lead to a harmful consequence. Moreover, one may want\nto prioritize the search for adversarial examples towards those that\nsignificantly modify the desired semantics of the overall system. Along the\nsame lines, existing algorithms for constructing robust ML algorithms ignore\nthe specification of the overall system. In this paper, we argue that the\nsemantics and specification of the overall system has a crucial role to play in\nthis line of research. We present preliminary research results that support\nthis claim.",
        "url": "http://arxiv.org/pdf/1804.07045v2.pdf"
    },
    {
        "title": "Probing hidden spin order with interpretable machine learning",
        "abstract": "The search of unconventional magnetic and nonmagnetic states is a major topic\nin the study of frustrated magnetism. Canonical examples of those states\ninclude various spin liquids and spin nematics. However, discerning their\nexistence and the correct characterization is usually challenging. Here we\nintroduce a machine-learning protocol that can identify general nematic order\nand their order parameter from seemingly featureless spin configurations, thus\nproviding comprehensive insight on the presence or absence of hidden orders. We\ndemonstrate the capabilities of our method by extracting the analytical form of\nnematic order parameter tensors up to rank 6. This may prove useful in the\nsearch for novel spin states and for ruling out spurious spin liquid\ncandidates.",
        "url": "http://arxiv.org/pdf/1804.08557v5.pdf"
    },
    {
        "title": "Deep learning improved by biological activation functions",
        "abstract": "`Biologically inspired' activation functions, such as the logistic sigmoid,\nhave been instrumental in the historical advancement of machine learning.\nHowever in the field of deep learning, they have been largely displaced by\nrectified linear units (ReLU) or similar functions, such as its exponential\nlinear unit (ELU) variant, to mitigate the effects of vanishing gradients\nassociated with error back-propagation. The logistic sigmoid however does not\nrepresent the true input-output relation in neuronal cells under physiological\nconditions. Here, bionodal root unit (BRU) activation functions are introduced,\nexhibiting input-output non-linearities that are substantially more\nbiologically plausible since their functional form is based on known\nbiophysical properties of neuronal cells.\n  In order to evaluate the learning performance of BRU activations, deep\nnetworks are constructed with identical architectures except differing in their\ntransfer functions (ReLU, ELU, and BRU). Multilayer perceptrons, stacked\nauto-encoders, and convolutional networks are used to test supervised and\nunsupervised learning based on the MNIST and CIFAR-10/100 datasets. Comparisons\nof learning performance, quantified using loss and error measurements,\ndemonstrate that bionodal networks both train faster than their ReLU and ELU\ncounterparts and result in the best generalised models even in the absence of\nformal regularisation. These results therefore suggest that revisiting the\ndetailed properties of biological neurones and their circuitry might prove\ninvaluable in the field of deep learning for the future.",
        "url": "http://arxiv.org/pdf/1804.11237v2.pdf"
    },
    {
        "title": "Optimizing for Generalization in Machine Learning with Cross-Validation Gradients",
        "abstract": "Cross-validation is the workhorse of modern applied statistics and machine\nlearning, as it provides a principled framework for selecting the model that\nmaximizes generalization performance. In this paper, we show that the\ncross-validation risk is differentiable with respect to the hyperparameters and\ntraining data for many common machine learning algorithms, including logistic\nregression, elastic-net regression, and support vector machines. Leveraging\nthis property of differentiability, we propose a cross-validation gradient\nmethod (CVGM) for hyperparameter optimization. Our method enables efficient\noptimization in high-dimensional hyperparameter spaces of the cross-validation\nrisk, the best surrogate of the true generalization ability of our learning\nalgorithm.",
        "url": "http://arxiv.org/pdf/1805.07072v1.pdf"
    },
    {
        "title": "Multifunction Cognitive Radar Task Scheduling Using Monte Carlo Tree Search and Policy Networks",
        "abstract": "A modern radar may be designed to perform multiple functions, such as\nsurveillance, tracking, and fire control. Each function requires the radar to\nexecute a number of transmit-receive tasks. A radar resource management (RRM)\nmodule makes decisions on parameter selection, prioritization, and scheduling\nof such tasks. RRM becomes especially challenging in overload situations, where\nsome tasks may need to be delayed or even dropped. In general, task scheduling\nis an NP-hard problem. In this work, we develop the branch-and-bound (B&B)\nmethod which obtains the optimal solution but at exponential computational\ncomplexity. On the other hand, heuristic methods have low complexity but\nprovide relatively poor performance. We resort to machine learning-based\ntechniques to address this issue; specifically we propose an approximate\nalgorithm based on the Monte Carlo tree search method. Along with using bound\nand dominance rules to eliminate nodes from the search tree, we use a policy\nnetwork to help to reduce the width of the search. Such a network can be\ntrained using solutions obtained by running the B&B method offline on problems\nwith feasible complexity. We show that the proposed method provides\nnear-optimal performance, but with computational complexity orders of magnitude\nsmaller than the B&B algorithm.",
        "url": "http://arxiv.org/pdf/1805.07069v1.pdf"
    },
    {
        "title": "Asynch-SGBDT: Asynchronous Parallel Stochastic Gradient Boosting Decision Tree based on Parameters Server",
        "abstract": "In AI research and industry, machine learning is the most widely used tool. One of the most important machine learning algorithms is Gradient Boosting Decision Tree, i.e. GBDT whose training process needs considerable computational resources and time. To shorten GBDT training time, many works tried to apply GBDT on Parameter Server. However, those GBDT algorithms are synchronous parallel algorithms which fail to make full use of Parameter Server. In this paper, we examine the possibility of using asynchronous parallel methods to train GBDT model and name this algorithm as asynch-SGBDT (asynchronous parallel stochastic gradient boosting decision tree). Our theoretical and experimental results indicate that the scalability of asynch-SGBDT is influenced by the sample diversity of datasets, sampling rate, step length and the setting of GBDT tree. Experimental results also show asynch-SGBDT training process reaches a linear speedup in asynchronous parallel manner when datasets and GBDT trees meet high scalability requirements.",
        "url": "https://arxiv.org/pdf/1804.04659v4.pdf"
    },
    {
        "title": "Supervising Nystr\u00f6m Methods via Negative Margin Support Vector Selection",
        "abstract": "The Nystr\\\"om methods have been popular techniques for scalable kernel based\nlearning. They approximate explicit, low-dimensional feature mappings for\nkernel functions from the pairwise comparisons with the training data. However,\nNystr\\\"om methods are generally applied without the supervision provided by the\ntraining labels in the classification/regression problems. This leads to\npairwise comparisons with randomly chosen training samples in the model.\nConversely, this work studies a supervised Nystr\\\"om method that chooses the\ncritical subsets of samples for the success of the Machine Learning model.\nParticularly, we select the Nystr\\\"om support vectors via the negative margin\ncriterion, and create explicit feature maps that are more suitable for the\nclassification task on the data. Experimental results on six datasets show\nthat, without increasing the complexity over unsupervised techniques, our\nmethod can significantly improve the classification performance achieved via\nkernel approximation methods and reduce the number of features needed to reach\nor exceed the performance of the full-dimensional kernel machines.",
        "url": "http://arxiv.org/pdf/1805.04018v2.pdf"
    },
    {
        "title": "Random active path model of deep neural networks with diluted binary synapses",
        "abstract": "Deep learning has become a powerful and popular tool for a variety of machine\nlearning tasks. However, it is challenging to understand the mechanism of deep\nlearning from a theoretical perspective. In this work, we propose a random\nactive path model to study collective properties of deep neural networks with\nbinary synapses, under the removal perturbation of connections between layers.\nIn the model, the path from input to output is randomly activated, and the\ncorresponding input unit constrains the weights along the path into the form of\na $p$-weight interaction glass model. A critical value of the perturbation is\nobserved to separate a spin glass regime from a paramagnetic regime, with the\ntransition being of the first order. The paramagnetic phase is conjectured to\nhave a poor generalization performance.",
        "url": "http://arxiv.org/pdf/1705.00850v3.pdf"
    },
    {
        "title": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models",
        "abstract": "In recent years, deep neural network approaches have been widely adopted for\nmachine learning tasks, including classification. However, they were shown to\nbe vulnerable to adversarial perturbations: carefully crafted small\nperturbations can cause misclassification of legitimate images. We propose\nDefense-GAN, a new framework leveraging the expressive capability of generative\nmodels to defend deep neural networks against such attacks. Defense-GAN is\ntrained to model the distribution of unperturbed images. At inference time, it\nfinds a close output to a given image which does not contain the adversarial\nchanges. This output is then fed to the classifier. Our proposed method can be\nused with any classification model and does not modify the classifier structure\nor training procedure. It can also be used as a defense against any attack as\nit does not assume knowledge of the process for generating the adversarial\nexamples. We empirically show that Defense-GAN is consistently effective\nagainst different attack methods and improves on existing defense strategies.\nOur code has been made publicly available at\nhttps://github.com/kabkabm/defensegan",
        "url": "http://arxiv.org/pdf/1805.06605v2.pdf"
    },
    {
        "title": "Practical Algorithms for STV and Ranked Pairs with Parallel Universes Tiebreaking",
        "abstract": "STV and ranked pairs (RP) are two well-studied voting rules for group\ndecision-making. They proceed in multiple rounds, and are affected by how ties\nare broken in each round. However, the literature is surprisingly vague about\nhow ties should be broken. We propose the first algorithms for computing the\nset of alternatives that are winners under some tiebreaking mechanism under STV\nand RP, which is also known as parallel-universes tiebreaking (PUT).\nUnfortunately, PUT-winners are NP-complete to compute under STV and RP, and\nstandard search algorithms from AI do not apply. We propose multiple DFS-based\nalgorithms along with pruning strategies and heuristics to prioritize search\ndirection to significantly improve the performance using machine learning. We\nalso propose novel ILP formulations for PUT-winners under STV and RP,\nrespectively. Experiments on synthetic and real-world data show that our\nalgorithms are overall significantly faster than ILP, while there are a few\ncases where ILP is significantly faster for RP.",
        "url": "http://arxiv.org/pdf/1805.06992v1.pdf"
    },
    {
        "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the Applied Machine Learning Literature",
        "abstract": "Machine learning workflow development is anecdotally regarded to be an\niterative process of trial-and-error with humans-in-the-loop. However, we are\nnot aware of quantitative evidence corroborating this popular belief. A\nquantitative characterization of iteration can serve as a benchmark for machine\nlearning workflow development in practice, and can aid the development of\nhuman-in-the-loop machine learning systems. To this end, we conduct a\nsmall-scale survey of the applied machine learning literature from five\ndistinct application domains. We collect and distill statistics on the role of\niteration within machine learning workflow development, and report preliminary\ntrends and insights from our investigation, as a starting point towards this\nbenchmark. Based on our findings, we finally describe desiderata for effective\nand versatile human-in-the-loop machine learning systems that can cater to\nusers in diverse domains.",
        "url": "http://arxiv.org/pdf/1803.10311v2.pdf"
    },
    {
        "title": "Counterexample-Guided Data Augmentation",
        "abstract": "We present a novel framework for augmenting data sets for machine learning\nbased on counterexamples. Counterexamples are misclassified examples that have\nimportant properties for retraining and improving the model. Key components of\nour framework include a counterexample generator, which produces data items\nthat are misclassified by the model and error tables, a novel data structure\nthat stores information pertaining to misclassifications. Error tables can be\nused to explain the model's vulnerabilities and are used to efficiently\ngenerate counterexamples for augmentation. We show the efficacy of the proposed\nframework by comparing it to classical augmentation techniques on a case study\nof object detection in autonomous driving based on deep neural networks.",
        "url": "http://arxiv.org/pdf/1805.06962v1.pdf"
    },
    {
        "title": "Learning to detect chest radiographs containing lung nodules using visual attention networks",
        "abstract": "Machine learning approaches hold great potential for the automated detection\nof lung nodules in chest radiographs, but training the algorithms requires vary\nlarge amounts of manually annotated images, which are difficult to obtain. Weak\nlabels indicating whether a radiograph is likely to contain pulmonary nodules\nare typically easier to obtain at scale by parsing historical free-text\nradiological reports associated to the radiographs. Using a repositotory of\nover 700,000 chest radiographs, in this study we demonstrate that promising\nnodule detection performance can be achieved using weak labels through\nconvolutional neural networks for radiograph classification. We propose two\nnetwork architectures for the classification of images likely to contain\npulmonary nodules using both weak labels and manually-delineated bounding\nboxes, when these are available. Annotated nodules are used at training time to\ndeliver a visual attention mechanism informing the model about its localisation\nperformance. The first architecture extracts saliency maps from high-level\nconvolutional layers and compares the estimated position of a nodule against\nthe ground truth, when this is available. A corresponding localisation error is\nthen back-propagated along with the softmax classification error. The second\napproach consists of a recurrent attention model that learns to observe a short\nsequence of smaller image portions through reinforcement learning. When a\nnodule annotation is available at training time, the reward function is\nmodified accordingly so that exploring portions of the radiographs away from a\nnodule incurs a larger penalty. Our empirical results demonstrate the potential\nadvantages of these architectures in comparison to competing methodologies.",
        "url": "http://arxiv.org/pdf/1712.00996v3.pdf"
    },
    {
        "title": "A Brief Introduction to Machine Learning for Engineers",
        "abstract": "This monograph aims at providing an introduction to key concepts, algorithms,\nand theoretical results in machine learning. The treatment concentrates on\nprobabilistic models for supervised and unsupervised learning problems. It\nintroduces fundamental concepts and algorithms by building on first principles,\nwhile also exposing the reader to more advanced topics with extensive pointers\nto the literature, within a unified notation and mathematical framework. The\nmaterial is organized according to clearly defined categories, such as\ndiscriminative and generative models, frequentist and Bayesian approaches,\nexact and approximate inference, as well as directed and undirected models.\nThis monograph is meant as an entry point for researchers with a background in\nprobability and linear algebra.",
        "url": "http://arxiv.org/pdf/1709.02840v3.pdf"
    },
    {
        "title": "It's all Relative: Monocular 3D Human Pose Estimation from Weakly Supervised Data",
        "abstract": "We address the problem of 3D human pose estimation from 2D input images using\nonly weakly supervised training data. Despite showing considerable success for\n2D pose estimation, the application of supervised machine learning to 3D pose\nestimation in real world images is currently hampered by the lack of varied\ntraining images with corresponding 3D poses. Most existing 3D pose estimation\nalgorithms train on data that has either been collected in carefully controlled\nstudio settings or has been generated synthetically. Instead, we take a\ndifferent approach, and propose a 3D human pose estimation algorithm that only\nrequires relative estimates of depth at training time. Such training signal,\nalthough noisy, can be easily collected from crowd annotators, and is of\nsufficient quality for enabling successful training and evaluation of 3D pose\nalgorithms. Our results are competitive with fully supervised regression based\napproaches on the Human3.6M dataset, despite using significantly weaker\ntraining data. Our proposed algorithm opens the door to using existing\nwidespread 2D datasets for 3D pose estimation by allowing fine-tuning with\nnoisy relative constraints, resulting in more accurate 3D poses.",
        "url": "http://arxiv.org/pdf/1805.06880v2.pdf"
    },
    {
        "title": "ScaffoldNet: Detecting and Classifying Biomedical Polymer-Based Scaffolds via a Convolutional Neural Network",
        "abstract": "We developed a Convolutional Neural Network model to identify and classify\nAirbrushed (alternatively known as Blow-spun), Electrospun and Steel Wire\nscaffolds. Our model ScaffoldNet is a 6-layer Convolutional Neural Network\ntrained and tested on 3,043 images of Airbrushed, Electrospun and Steel Wire\nscaffolds. The model takes in as input an imaged scaffold and then outputs the\nscaffold type (Airbrushed, Electrospun or Steel Wire) as predicted\nprobabilities for the 3 classes. Our model scored a 99.44% Accuracy,\ndemonstrating potential for adaptation to investigating and solving complex\nmachine learning problems aimed at abstract spatial contexts, or in screening\ncomplex, biological, fibrous structures seen in cortical bone and fibrous\nshells.",
        "url": "http://arxiv.org/pdf/1805.08702v1.pdf"
    },
    {
        "title": "Supervised classification of Dermatological diseases by Deep learning",
        "abstract": "This paper introduces a deep-learning based efficient classifier for common\ndermatological conditions, aimed at people without easy access to skin\nspecialists. We report approximately 80% accuracy, in a situation where primary\ncare doctors have attained 57% success rate, according to recent literature.\nThe rationale of its design is centered on deploying and updating it on\nhandheld devices in near future. Dermatological diseases are common in every\npopulation and have a wide spectrum in severity. With a shortage of\ndermatological expertise being observed in several countries, machine learning\nsolutions can augment medical services and advise regarding existence of common\ndiseases. The paper implements supervised classification of nine distinct\nconditions which have high occurrence in East Asian countries. Our current\nattempt establishes that deep learning based techniques are viable avenues for\npreliminary information to aid patients.",
        "url": "http://arxiv.org/pdf/1802.03752v3.pdf"
    },
    {
        "title": "The Blessings of Multiple Causes",
        "abstract": "Causal inference from observational data often assumes \"ignorability,\" that\nall confounders are observed. This assumption is standard yet untestable.\nHowever, many scientific studies involve multiple causes, different variables\nwhose effects are simultaneously of interest. We propose the deconfounder, an\nalgorithm that combines unsupervised machine learning and predictive model\nchecking to perform causal inference in multiple-cause settings. The\ndeconfounder infers a latent variable as a substitute for unobserved\nconfounders and then uses that substitute to perform causal inference. We\ndevelop theory for the deconfounder, and show that it requires weaker\nassumptions than classical causal inference. We analyze its performance in\nthree types of studies: semi-simulated data around smoking and lung cancer,\nsemi-simulated data around genome-wide association studies, and a real dataset\nabout actors and movie revenue. The deconfounder provides a checkable approach\nto estimating closer-to-truth causal effects.",
        "url": "http://arxiv.org/pdf/1805.06826v3.pdf"
    },
    {
        "title": "Annotating Electronic Medical Records for Question Answering",
        "abstract": "Our research is in the relatively unexplored area of question answering\ntechnologies for patient-specific questions over their electronic health\nrecords. A large dataset of human expert curated question and answer pairs is\nan important pre-requisite for developing, training and evaluating any question\nanswering system that is powered by machine learning. In this paper, we\ndescribe a process for creating such a dataset of questions and answers. Our\nmethodology is replicable, can be conducted by medical students as annotators,\nand results in high inter-annotator agreement (0.71 Cohen's kappa). Over the\ncourse of 11 months, 11 medical students followed our annotation methodology,\nresulting in a question answering dataset of 5696 questions over 71 patient\nrecords, of which 1747 questions have corresponding answers generated by the\nmedical students.",
        "url": "http://arxiv.org/pdf/1805.06816v1.pdf"
    },
    {
        "title": "Structural Feature Selection for Event Logs",
        "abstract": "We consider the problem of classifying business process instances based on\nstructural features derived from event logs. The main motivation is to provide\nmachine learning based techniques with quick response times for interactive\ncomputer assisted root cause analysis. In particular, we create structural\nfeatures from process mining such as activity and transition occurrence counts,\nand ordering of activities to be evaluated as potential features for\nclassification. We show that adding such structural features increases the\namount of information thus potentially increasing classification accuracy.\nHowever, there is an inherent trade-off as using too many features leads to too\nlong run-times for machine learning classification models. One way to improve\nthe machine learning algorithms' run-time is to only select a small number of\nfeatures by a feature selection algorithm. However, the run-time required by\nthe feature selection algorithm must also be taken into account. Also, the\nclassification accuracy should not suffer too much from the feature selection.\nThe main contributions of this paper are as follows: First, we propose and\ncompare six different feature selection algorithms by means of an experimental\nsetup comparing their classification accuracy and achievable response times.\nSecond, we discuss the potential use of feature selection results for computer\nassisted root cause analysis as well as the properties of different types of\nstructural features in the context of feature selection.",
        "url": "http://arxiv.org/pdf/1710.02823v2.pdf"
    },
    {
        "title": "On the convergence properties of a $K$-step averaging stochastic gradient descent algorithm for nonconvex optimization",
        "abstract": "Despite their popularity, the practical performance of asynchronous\nstochastic gradient descent methods (ASGD) for solving large scale machine\nlearning problems are not as good as theoretical results indicate. We adopt and\nanalyze a synchronous K-step averaging stochastic gradient descent algorithm\nwhich we call K-AVG. We establish the convergence results of K-AVG for\nnonconvex objectives and explain why the K-step delay is necessary and leads to\nbetter performance than traditional parallel stochastic gradient descent which\nis a special case of K-AVG with $K=1$. We also show that K-AVG scales better\nthan ASGD. Another advantage of K-AVG over ASGD is that it allows larger\nstepsizes. On a cluster of $128$ GPUs, K-AVG is faster than ASGD\nimplementations and achieves better accuracies and faster convergence for\n\\cifar dataset.",
        "url": "http://arxiv.org/pdf/1708.01012v3.pdf"
    },
    {
        "title": "Learning to Branch",
        "abstract": "Tree search algorithms, such as branch-and-bound, are the most widely used\ntools for solving combinatorial and nonconvex problems. For example, they are\nthe foremost method for solving (mixed) integer programs and constraint\nsatisfaction problems. Tree search algorithms recursively partition the search\nspace to find an optimal solution. In order to keep the tree size small, it is\ncrucial to carefully decide, when expanding a tree node, which question\n(typically variable) to branch on at that node in order to partition the\nremaining space. Numerous partitioning techniques (e.g., variable selection)\nhave been proposed, but there is no theory describing which technique is\noptimal. We show how to use machine learning to determine an optimal weighting\nof any set of partitioning procedures for the instance distribution at hand\nusing samples from the distribution. We provide the first sample complexity\nguarantees for tree search algorithm configuration. These guarantees bound the\nnumber of samples sufficient to ensure that the empirical performance of an\nalgorithm over the samples nearly matches its expected performance on the\nunknown instance distribution. This thorough theoretical investigation\nnaturally gives rise to our learning algorithm. Via experiments, we show that\nlearning an optimal weighting of partitioning procedures can dramatically\nreduce tree size, and we prove that this reduction can even be exponential.\nThrough theory and experiments, we show that learning to branch is both\npractical and hugely beneficial.",
        "url": "http://arxiv.org/pdf/1803.10150v2.pdf"
    },
    {
        "title": "Composite Semantic Relation Classification",
        "abstract": "Different semantic interpretation tasks such as text entailment and question\nanswering require the classification of semantic relations between terms or\nentities within text. However, in most cases it is not possible to assign a\ndirect semantic relation between entities/terms. This paper proposes an\napproach for composite semantic relation classification, extending the\ntraditional semantic relation classification task. Different from existing\napproaches, which use machine learning models built over lexical and\ndistributional word vector features, the proposed model uses the combination of\na large commonsense knowledge base of binary relations, a distributional\nnavigational algorithm and sequence classification to provide a solution for\nthe composite semantic relation classification problem.",
        "url": "http://arxiv.org/pdf/1805.06521v1.pdf"
    },
    {
        "title": "SAFE: Spectral Evolution Analysis Feature Extraction for Non-Stationary Time Series Prediction",
        "abstract": "This paper presents a practical approach for detecting non-stationarity in\ntime series prediction. This method is called SAFE and works by monitoring the\nevolution of the spectral contents of time series through a distance function.\nThis method is designed to work in combination with state-of-the-art machine\nlearning methods in real time by informing the online predictors to perform\nnecessary adaptation when a non-stationarity presents. We also propose an\nalgorithm to proportionally include some past data in the adaption process to\novercome the Catastrophic Forgetting problem. To validate our hypothesis and\ntest the effectiveness of our approach, we present comprehensive experiments in\ndifferent elements of the approach involving artificial and real-world\ndatasets. The experiments show that the proposed method is able to\nsignificantly save computational resources in term of processor or GPU cycles\nwhile maintaining high prediction performances.",
        "url": "http://arxiv.org/pdf/1803.01364v2.pdf"
    },
    {
        "title": "Approximating the Void: Learning Stochastic Channel Models from Observation with Variational Generative Adversarial Networks",
        "abstract": "Channel modeling is a critical topic when considering designing, learning, or\nevaluating the performance of any communications system. Most prior work in\ndesigning or learning new modulation schemes has focused on using highly\nsimplified analytic channel models such as additive white Gaussian noise\n(AWGN), Rayleigh fading channels or similar. Recently, we proposed the usage of\na generative adversarial networks (GANs) to jointly approximate a wireless\nchannel response model (e.g. from real black box measurements) and optimize for\nan efficient modulation scheme over it using machine learning. This approach\nworked to some degree, but was unable to produce accurate probability\ndistribution functions (PDFs) representing the stochastic channel response. In\nthis paper, we focus specifically on the problem of accurately learning a\nchannel PDF using a variational GAN, introducing an architecture and loss\nfunction which can accurately capture stochastic behavior. We illustrate where\nour prior method failed and share results capturing the performance of such as\nsystem over a range of realistic channel distributions.",
        "url": "http://arxiv.org/pdf/1805.06350v2.pdf"
    },
    {
        "title": "Entanglement-guided architectures of machine learning by quantum tensor network",
        "abstract": "It is a fundamental, but still elusive question whether the schemes based on\nquantum mechanics, in particular on quantum entanglement, can be used for\nclassical information processing and machine learning. Even partial answer to\nthis question would bring important insights to both fields of machine learning\nand quantum mechanics. In this work, we implement simple numerical experiments,\nrelated to pattern/images classification, in which we represent the classifiers\nby many-qubit quantum states written in the matrix product states (MPS).\nClassical machine learning algorithm is applied to these quantum states to\nlearn the classical data. We explicitly show how quantum entanglement (i.e.,\nsingle-site and bipartite entanglement) can emerge in such represented images.\nEntanglement characterizes here the importance of data, and such information\nare practically used to guide the architecture of MPS, and improve the\nefficiency. The number of needed qubits can be reduced to less than 1/10 of the\noriginal number, which is within the access of the state-of-the-art quantum\ncomputers. We expect such numerical experiments could open new paths in\ncharactering classical machine learning algorithms, and at the same time shed\nlights on the generic quantum simulations/computations of machine learning\ntasks.",
        "url": "http://arxiv.org/pdf/1803.09111v3.pdf"
    },
    {
        "title": "Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert Spaces",
        "abstract": "Transfer operators such as the Perron--Frobenius or Koopman operator play an important role in the global analysis of complex dynamical systems. The eigenfunctions of these operators can be used to detect metastable sets, to project the dynamics onto the dominant slow processes, or to separate superimposed signals. We extend transfer operator theory to reproducing kernel Hilbert spaces and show that these operators are related to Hilbert space representations of conditional distributions, known as conditional mean embeddings in the machine learning community. Moreover, numerical methods to compute empirical estimates of these embeddings are akin to data-driven methods for the approximation of transfer operators such as extended dynamic mode decomposition and its variants. One main benefit of the presented kernel-based approaches is that these methods can be applied to any domain where a similarity measure given by a kernel is available. We illustrate the results with the aid of guiding examples and highlight potential applications in molecular dynamics as well as video and text data analysis.",
        "url": "https://arxiv.org/pdf/1712.01572v3.pdf"
    },
    {
        "title": "Image Classification Based on Quantum KNN Algorithm",
        "abstract": "Image classification is an important task in the field of machine learning\nand image processing. However, the usually used classification method --- the K\nNearest-Neighbor algorithm has high complexity, because its two main processes:\nsimilarity computing and searching are time-consuming. Especially in the era of\nbig data, the problem is prominent when the amount of images to be classified\nis large. In this paper, we try to use the powerful parallel computing ability\nof quantum computers to optimize the efficiency of image classification. The\nscheme is based on quantum K Nearest-Neighbor algorithm. Firstly, the feature\nvectors of images are extracted on classical computers. Then the feature\nvectors are inputted into a quantum superposition state, which is used to\nachieve parallel computing of similarity. Next, the quantum minimum search\nalgorithm is used to speed up searching process for similarity. Finally, the\nimage is classified by quantum measurement. The complexity of the quantum\nalgorithm is only O((kM)^(1/2)), which is superior to the classical algorithms.\nMoreover, the measurement step is executed only once to ensure the validity of\nthe scheme. The experimental results show that, the classification accuracy is\n83.1% on Graz-01 dataset and 78% on Caltech-101 dataset, which is close to\nexisting classical algorithms. Hence, our quantum scheme has a good\nclassification performance while greatly improving the efficiency.",
        "url": "http://arxiv.org/pdf/1805.06260v1.pdf"
    },
    {
        "title": "Towards Explaining Anomalies: A Deep Taylor Decomposition of One-Class Models",
        "abstract": "A common machine learning task is to discriminate between normal and\nanomalous data points. In practice, it is not always sufficient to reach high\naccuracy at this task, one also would like to understand why a given data point\nhas been predicted in a certain way. We present a new principled approach for\none-class SVMs that decomposes outlier predictions in terms of input variables.\nThe method first recomposes the one-class model as a neural network with\ndistance functions and min-pooling, and then performs a deep Taylor\ndecomposition (DTD) of the model output. The proposed One-Class DTD is\napplicable to a number of common distance-based SVM kernels and is able to\nreliably explain a wide set of data anomalies. Furthermore, it outperforms\nbaselines such as sensitivity analysis, nearest neighbor, or simple edge\ndetection.",
        "url": "http://arxiv.org/pdf/1805.06230v1.pdf"
    },
    {
        "title": "Analyzing high-dimensional time-series data using kernel transfer operator eigenfunctions",
        "abstract": "Kernel transfer operators, which can be regarded as approximations of\ntransfer operators such as the Perron-Frobenius or Koopman operator in\nreproducing kernel Hilbert spaces, are defined in terms of covariance and\ncross-covariance operators and have been shown to be closely related to the\nconditional mean embedding framework developed by the machine learning\ncommunity. The goal of this paper is to show how the dominant eigenfunctions of\nthese operators in combination with gradient-based optimization techniques can\nbe used to detect long-lived coherent patterns in high-dimensional time-series\ndata. The results will be illustrated using video data and a fluid flow\nexample.",
        "url": "http://arxiv.org/pdf/1805.10118v1.pdf"
    },
    {
        "title": "A Survey on Compiler Autotuning using Machine Learning",
        "abstract": "Since the mid-1990s, researchers have been trying to use machine-learning\nbased approaches to solve a number of different compiler optimization problems.\nThese techniques primarily enhance the quality of the obtained results and,\nmore importantly, make it feasible to tackle two main compiler optimization\nproblems: optimization selection (choosing which optimizations to apply) and\nphase-ordering (choosing the order of applying optimizations). The compiler\noptimization space continues to grow due to the advancement of applications,\nincreasing number of compiler optimizations, and new target architectures.\nGeneric optimization passes in compilers cannot fully leverage newly introduced\noptimizations and, therefore, cannot keep up with the pace of increasing\noptions. This survey summarizes and classifies the recent advances in using\nmachine learning for the compiler optimization field, particularly on the two\nmajor problems of (1) selecting the best optimizations and (2) the\nphase-ordering of optimizations. The survey highlights the approaches taken so\nfar, the obtained results, the fine-grain classification among different\napproaches and finally, the influential papers of the field.",
        "url": "http://arxiv.org/pdf/1801.04405v5.pdf"
    },
    {
        "title": "Unsupervised Machine Learning Based on Non-Negative Tensor Factorization for Analyzing Reactive-Mixing",
        "abstract": "Analysis of reactive-diffusion simulations requires a large number of\nindependent model runs. For each high-fidelity simulation, inputs are varied\nand the predicted mixing behavior is represented by changes in species\nconcentration. It is then required to discern how the model inputs impact the\nmixing process. This task is challenging and typically involves interpretation\nof large model outputs. However, the task can be automated and substantially\nsimplified by applying Machine Learning (ML) methods. In this paper, we present\nan application of an unsupervised ML method (called NTFk) using Non-negative\nTensor Factorization (NTF) coupled with a custom clustering procedure based on\nk-means to reveal hidden features in product concentration. An attractive\naspect of the proposed ML method is that it ensures the extracted features are\nnon-negative, which are important to obtain a meaningful deconstruction of the\nmixing processes. The ML method is applied to a large set of high-resolution\nFEM simulations representing reaction-diffusion processes in perturbed\nvortex-based velocity fields. The applied FEM ensures that species\nconcentration are always non-negative. The simulated reaction is a fast\nirreversible bimolecular reaction. The reactive-diffusion model input\nparameters that control mixing include properties of velocity field,\nanisotropic dispersion, and molecular diffusion. We demonstrate the\napplicability of the ML method to produce a meaningful deconstruction of model\noutputs to discriminate between different physical processes impacting the\nreactants, their mixing, and the spatial distribution of the product. The\npresented ML analysis allowed us to identify additive features that\ncharacterize mixing behavior.",
        "url": "http://arxiv.org/pdf/1805.06454v2.pdf"
    },
    {
        "title": "CLINIQA: A Machine Intelligence Based Clinical Question Answering System",
        "abstract": "The recent developments in the field of biomedicine have made large volumes\nof biomedical literature available to the medical practitioners. Due to the\nlarge size and lack of efficient searching strategies, medical practitioners\nstruggle to obtain necessary information available in the biomedical\nliterature. Moreover, the most sophisticated search engines of age are not\nintelligent enough to interpret the clinicians' questions. These facts reflect\nthe urgent need of an information retrieval system that accepts the queries\nfrom medical practitioners' in natural language and returns the answers quickly\nand efficiently. In this paper, we present an implementation of a machine\nintelligence based CLINIcal Question Answering system (CLINIQA) to answer\nmedical practitioner's questions. The system was rigorously evaluated on\ndifferent text mining algorithms and the best components for the system were\nselected. The system makes use of Unified Medical Language System for semantic\nanalysis of both questions and medical documents. In addition, the system\nemploys supervised machine learning algorithms for classification of the\ndocuments, identifying the focus of the question and answer selection.\nEffective domain-specific heuristics are designed for answer ranking. The\nperformance evaluation on hundred clinical questions shows the effectiveness of\nour approach.",
        "url": "http://arxiv.org/pdf/1805.05927v1.pdf"
    },
    {
        "title": "Gradient-Leaks: Understanding and Controlling Deanonymization in Federated Learning",
        "abstract": "Federated Learning (FL) systems are gaining popularity as a solution to training Machine Learning (ML) models from large-scale user data collected on personal devices (e.g., smartphones) without their raw data leaving the device. At the core of FL is a network of anonymous user devices sharing training information (model parameter updates) computed locally on personal data. However, the type and degree to which user-specific information is encoded in the model updates is poorly understood. In this paper, we identify model updates encode subtle variations in which users capture and generate data. The variations provide a strong statistical signal, allowing an adversary to effectively deanonymize participating devices using a limited set of auxiliary data. We analyze resulting deanonymization attacks on diverse tasks on real-world (anonymized) user-generated data across a range of closed- and open-world scenarios. We study various strategies to mitigate the risks of deanonymization. As random perturbation methods do not offer convincing operating points, we propose data-augmentation strategies which introduces adversarial biases in device data and thereby, offer substantial protection against deanonymization threats with little effect on utility.",
        "url": "https://arxiv.org/pdf/1805.05838v3.pdf"
    },
    {
        "title": "Intrinsic dimension and its application to association rules",
        "abstract": "The curse of dimensionality in the realm of association rules is twofold.\nFirstly, we have the well known exponential increase in computational\ncomplexity with increasing item set size. Secondly, there is a \\emph{related\ncurse} concerned with the distribution of (spare) data itself in high\ndimension. The former problem is often coped with by projection, i.e., feature\nselection, whereas the best known strategy for the latter is avoidance. This\nwork summarizes the first attempt to provide a computationally feasible method\nfor measuring the extent of dimension curse present in a data set with respect\nto a particular class machine of learning procedures. This recent development\nenables the application of various other methods from geometric analysis to be\ninvestigated and applied in machine learning procedures in the presence of high\ndimension.",
        "url": "http://arxiv.org/pdf/1805.05714v1.pdf"
    },
    {
        "title": "The Hierarchical Adaptive Forgetting Variational Filter",
        "abstract": "A common problem in Machine Learning and statistics consists in detecting\nwhether the current sample in a stream of data belongs to the same distribution\nas previous ones, is an isolated outlier or inaugurates a new distribution of\ndata. We present a hierarchical Bayesian algorithm that aims at learning a\ntime-specific approximate posterior distribution of the parameters describing\nthe distribution of the data observed. We derive the update equations of the\nvariational parameters of the approximate posterior at each time step for\nmodels from the exponential family, and show that these updates find\ninteresting correspondents in Reinforcement Learning (RL). In this perspective,\nour model can be seen as a hierarchical RL algorithm that learns a posterior\ndistribution according to a certain stability confidence that is, in turn,\nlearned according to its own stability confidence. Finally, we show some\napplications of our generic model, first in a RL context, next with an adaptive\nBayesian Autoregressive model, and finally in the context of Stochastic\nGradient Descent optimization.",
        "url": "http://arxiv.org/pdf/1805.05703v1.pdf"
    },
    {
        "title": "Generating Continuous Representations of Medical Texts",
        "abstract": "We present an architecture that generates medical texts while learning an\ninformative, continuous representation with discriminative features. During\ntraining the input to the system is a dataset of captions for medical X-Rays.\nThe acquired continuous representations are of particular interest for use in\nmany machine learning techniques where the discrete and high-dimensional nature\nof textual input is an obstacle. We use an Adversarially Regularized\nAutoencoder to create realistic text in both an unconditional and conditional\nsetting. We show that this technique is applicable to medical texts which often\ncontain syntactic and domain-specific shorthands. A quantitative evaluation\nshows that we achieve a lower model perplexity than a traditional LSTM\ngenerator.",
        "url": "http://arxiv.org/pdf/1805.05691v1.pdf"
    },
    {
        "title": "Distribution-based Label Space Transformation for Multi-label Learning",
        "abstract": "Multi-label learning problems have manifested themselves in various machine\nlearning applications. The key to successful multi-label learning algorithms\nlies in the exploration of inter-label correlations, which usually incur great\ncomputational cost. Another notable factor in multi-label learning is that the\nlabel vectors are usually extremely sparse, especially when the candidate label\nvocabulary is very large and only a few instances are assigned to each\ncategory. Recently, a label space transformation (LST) framework has been\nproposed targeting these challenges. However, current methods based on LST\nusually suffer from information loss in the label space dimension reduction\nprocess and fail to address the sparsity problem effectively. In this paper, we\npropose a distribution-based label space transformation (DLST) model. By\ndefining the distribution based on the similarity of label vectors, a more\ncomprehensive label structure can be captured. Then, by minimizing\nKL-divergence of two distributions, the information of the original label space\ncan be approximately preserved in the latent space. Consequently, multi-label\nclassifier trained using the dense latent codes yields better performance. The\nleverage of distribution enables DLST to fill out additional information about\nthe label correlations. This endows DLST the capability to handle label set\nsparsity and training data sparsity in multi-label learning problems. With the\noptimal latent code, a kernel logistic regression function is learned for the\nmapping from feature space to the latent space. Then ML-KNN is employed to\nrecover the original label vector from the transformed latent code. Extensive\nexperiments on several benchmark datasets demonstrate that DLST not only\nachieves high classification performance but also is computationally more\nefficient.",
        "url": "http://arxiv.org/pdf/1805.05687v1.pdf"
    },
    {
        "title": "Corpus Conversion Service: A machine learning platform to ingest documents at scale [Poster abstract]",
        "abstract": "Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make their\ncontent discoverable. Unfortunately, both the format of these documents (e.g.\nthe PDF format or bitmap images) as well as the presentation of the data (e.g.\ncomplex tables) make the extraction of qualitative and quantitive data\nextremely challenging. We present a platform to ingest documents at scale which\nis powered by Machine Learning techniques and allows the user to train custom\nmodels on document collections. We show precision/recall results greater than\n97% with regard to conversion to structured formats, as well as scaling\nevidence for each of the microservices constituting the platform.",
        "url": "http://arxiv.org/pdf/1805.09687v1.pdf"
    },
    {
        "title": "Prediction of Facebook Post Metrics using Machine Learning",
        "abstract": "In this short paper, we evaluate the performance of three well-known Machine\nLearning techniques for predicting the impact of a post in Facebook. Social\nmedias have a huge influence in the social behaviour. Therefore to develop an\nautomatic model for predicting the impact of posts in social medias can be\nuseful to the society. In this article, we analyze the efficiency for\npredicting the post impact of three popular techniques: Support Vector\nRegression (SVR), Echo State Network (ESN) and Adaptive Network Fuzzy Inject\nSystem (ANFIS). The evaluation was done over a public and well-known benchmark\ndataset.",
        "url": "http://arxiv.org/pdf/1805.05579v1.pdf"
    },
    {
        "title": "A Dynamic Neural Network Approach to Generating Robot's Novel Actions: A Simulation Experiment",
        "abstract": "In this study, we investigate how a robot can generate novel and creative\nactions from its own experience of learning basic actions. Inspired by a\nmachine learning approach to computational creativity, we propose a dynamic\nneural network model that can learn and generate robot's actions. We conducted\na set of simulation experiments with a humanoid robot. The results showed that\nthe proposed model was able to learn the basic actions and also to generate\nnovel actions by modulating and combining those learned actions. The analysis\non the neural activities illustrated that the ability to generate creative\nactions emerged from the model's nonlinear memory structure self-organized\nduring training. The results also showed that the different way of learning the\nbasic actions induced the self-organization of the memory structure with the\ndifferent characteristics, resulting in the generation of different levels of\ncreative actions. Our approach can be utilized in human-robot interaction in\nwhich a user can interactively explore the robot's memory to control its\nbehavior and also discover other novel actions.",
        "url": "http://arxiv.org/pdf/1805.05537v1.pdf"
    },
    {
        "title": "Deep Learning in Spiking Neural Networks",
        "abstract": "In recent years, deep learning has been a revolution in the field of machine\nlearning, for computer vision in particular. In this approach, a deep\n(multilayer) artificial neural network (ANN) is trained in a supervised manner\nusing backpropagation. Huge amounts of labeled examples are required, but the\nresulting classification accuracy is truly impressive, sometimes outperforming\nhumans. Neurons in an ANN are characterized by a single, static,\ncontinuous-valued activation. Yet biological neurons use discrete spikes to\ncompute and transmit information, and the spike times, in addition to the spike\nrates, matter. Spiking neural networks (SNNs) are thus more biologically\nrealistic than ANNs, and arguably the only viable option if one wants to\nunderstand how the brain computes. SNNs are also more hardware friendly and\nenergy-efficient than ANNs, and are thus appealing for technology, especially\nfor portable devices. However, training deep SNNs remains a challenge. Spiking\nneurons' transfer function is usually non-differentiable, which prevents using\nbackpropagation. Here we review recent supervised and unsupervised methods to\ntrain deep SNNs, and compare them in terms of accuracy, but also computational\ncost and hardware friendliness. The emerging picture is that SNNs still lag\nbehind ANNs in terms of accuracy, but the gap is decreasing, and can even\nvanish on some tasks, while the SNNs typically require much fewer operations.",
        "url": "http://arxiv.org/pdf/1804.08150v4.pdf"
    },
    {
        "title": "A Non-monotone Alternating Updating Method for A Class of Matrix Factorization Problems",
        "abstract": "In this paper we consider a general matrix factorization model which covers a\nlarge class of existing models with many applications in areas such as machine\nlearning and imaging sciences. To solve this possibly nonconvex, nonsmooth and\nnon-Lipschitz problem, we develop a non-monotone alternating updating method\nbased on a potential function. Our method essentially updates two blocks of\nvariables in turn by inexactly minimizing this potential function, and updates\nanother auxiliary block of variables using an explicit formula. The special\nstructure of our potential function allows us to take advantage of efficient\ncomputational strategies for non-negative matrix factorization to perform the\nalternating minimization over the two blocks of variables. A suitable line\nsearch criterion is also incorporated to improve the numerical performance.\nUnder some mild conditions, we show that the line search criterion is well\ndefined, and establish that the sequence generated is bounded and any cluster\npoint of the sequence is a stationary point. Finally, we conduct some numerical\nexperiments using real datasets to compare our method with some existing\nefficient methods for non-negative matrix factorization and matrix completion.\nThe numerical results show that our method can outperform these methods for\nthese specific applications.",
        "url": "http://arxiv.org/pdf/1705.06499v2.pdf"
    },
    {
        "title": "A Revisit of Hashing Algorithms for Approximate Nearest Neighbor Search",
        "abstract": "Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many areas of machine learning and data mining. During the past decade, numerous hashing algorithms are proposed to solve this problem. Every proposed algorithm claims outperform other state-of-the-art hashing methods. However, the evaluation of these hashing papers was not thorough enough, and those claims should be re-examined. The ultimate goal of an ANNS method is returning the most accurate answers (nearest neighbors) in the shortest time. If implemented correctly, almost all the hashing methods will have their performance improved as the code length increases. However, many existing hashing papers only report the performance with the code length shorter than 128. In this paper, we carefully revisit the problem of search with a hash index, and analyze the pros and cons of two popular hash index search procedures. Then we proposed a very simple but effective two level index structures and make a thorough comparison of eleven popular hashing algorithms. Surprisingly, the random-projection-based Locality Sensitive Hashing (LSH) is the best performed algorithm, which is in contradiction to the claims in all the other ten hashing papers. Despite the extreme simplicity of random-projection-based LSH, our results show that the capability of this algorithm has been far underestimated. For the sake of reproducibility, all the codes used in the paper are released on GitHub, which can be used as a testing platform for a fair comparison between various hashing algorithms.",
        "url": "https://arxiv.org/pdf/1612.07545v6.pdf"
    },
    {
        "title": "Crowdbreaks: Tracking Health Trends using Public Social Media Data and Crowdsourcing",
        "abstract": "In the past decade, tracking health trends using social media data has shown\ngreat promise, due to a powerful combination of massive adoption of social\nmedia around the world, and increasingly potent hardware and software that\nenables us to work with these new big data streams. At the same time, many\nchallenging problems have been identified. First, there is often a mismatch\nbetween how rapidly online data can change, and how rapidly algorithms are\nupdated, which means that there is limited reusability for algorithms trained\non past data as their performance decreases over time. Second, much of the work\nis focusing on specific issues during a specific past period in time, even\nthough public health institutions would need flexible tools to assess multiple\nevolving situations in real time. Third, most tools providing such capabilities\nare proprietary systems with little algorithmic or data transparency, and thus\nlittle buy-in from the global public health and research community. Here, we\nintroduce Crowdbreaks, an open platform which allows tracking of health trends\nby making use of continuous crowdsourced labelling of public social media\ncontent. The system is built in a way which automatizes the typical workflow\nfrom data collection, filtering, labelling and training of machine learning\nclassifiers and therefore can greatly accelerate the research process in the\npublic health domain. This work introduces the technical aspects of the\nplatform and explores its future use cases.",
        "url": "http://arxiv.org/pdf/1805.05491v1.pdf"
    },
    {
        "title": "Coulomb Autoencoders",
        "abstract": "Learning the true density in high-dimensional feature spaces is a well-known problem in machine learning. In this work, we consider generative autoencoders based on maximum-mean discrepancy (MMD) and provide theoretical insights. In particular, (i) we prove that MMD coupled with Coulomb kernels has optimal convergence properties, which are similar to convex functionals, thus improving the training of autoencoders, and (ii) we provide a probabilistic bound on the generalization performance, highlighting some fundamental conditions to achieve better generalization. We validate the theory on synthetic examples and on the popular dataset of celebrities' faces, showing that our model, called Coulomb autoencoders, outperform the state-of-the-art.",
        "url": "https://arxiv.org/pdf/1802.03505v6.pdf"
    },
    {
        "title": "A giant with feet of clay: on the validity of the data that feed machine learning in medicine",
        "abstract": "This paper considers the use of Machine Learning (ML) in medicine by focusing\non the main problem that this computational approach has been aimed at solving\nor at least minimizing: uncertainty. To this aim, we point out how uncertainty\nis so ingrained in medicine that it biases also the representation of clinical\nphenomena, that is the very input of ML models, thus undermining the clinical\nsignificance of their output. Recognizing this can motivate both medical\ndoctors, in taking more responsibility in the development and use of these\ndecision aids, and the researchers, in pursuing different ways to assess the\nvalue of these systems. In so doing, both designers and users could take this\nintrinsic characteristic of medicine more seriously and consider alternative\napproaches that do not \"sweep uncertainty under the rug\" within an objectivist\nfiction, which everyone can come up by believing as true.",
        "url": "http://arxiv.org/pdf/1706.06838v3.pdf"
    },
    {
        "title": "Computing the Shattering Coefficient of Supervised Learning Algorithms",
        "abstract": "The Statistical Learning Theory (SLT) provides the theoretical guarantees for\nsupervised machine learning based on the Empirical Risk Minimization Principle\n(ERMP). Such principle defines an upper bound to ensure the uniform convergence\nof the empirical risk Remp(f), i.e., the error measured on a given data sample,\nto the expected value of risk R(f) (a.k.a. actual risk), which depends on the\nJoint Probability Distribution P(X x Y) mapping input examples x in X to class\nlabels y in Y. The uniform convergence is only ensured when the Shattering\ncoefficient N(F,2n) has a polynomial growing behavior. This paper proves the\nShattering coefficient for any Hilbert space H containing the input space X and\ndiscusses its effects in terms of learning guarantees for supervised machine\nalgorithms.",
        "url": "http://arxiv.org/pdf/1805.02627v4.pdf"
    },
    {
        "title": "You are your Metadata: Identification and Obfuscation of Social Media Users using Metadata Information",
        "abstract": "Metadata are associated to most of the information we produce in our daily\ninteractions and communication in the digital world. Yet, surprisingly,\nmetadata are often still catergorized as non-sensitive. Indeed, in the past,\nresearchers and practitioners have mainly focused on the problem of the\nidentification of a user from the content of a message.\n  In this paper, we use Twitter as a case study to quantify the uniqueness of\nthe association between metadata and user identity and to understand the\neffectiveness of potential obfuscation strategies. More specifically, we\nanalyze atomic fields in the metadata and systematically combine them in an\neffort to classify new tweets as belonging to an account using different\nmachine learning algorithms of increasing complexity. We demonstrate that\nthrough the application of a supervised learning algorithm, we are able to\nidentify any user in a group of 10,000 with approximately 96.7% accuracy.\nMoreover, if we broaden the scope of our search and consider the 10 most likely\ncandidates we increase the accuracy of the model to 99.22%. We also found that\ndata obfuscation is hard and ineffective for this type of data: even after\nperturbing 60% of the training data, it is still possible to classify users\nwith an accuracy higher than 95%. These results have strong implications in\nterms of the design of metadata obfuscation strategies, for example for data\nset release, not only for Twitter, but, more generally, for most social media\nplatforms.",
        "url": "http://arxiv.org/pdf/1803.10133v2.pdf"
    },
    {
        "title": "A One-Class Classification Decision Tree Based on Kernel Density Estimation",
        "abstract": "One-class Classification (OCC) is an area of machine learning which addresses prediction based on unbalanced datasets. Basically, OCC algorithms achieve training by means of a single class sample, with potentially some additional counter-examples. The current OCC models give satisfaction in terms of performance, but there is an increasing need for the development of interpretable models. In the present work, we propose a one-class model which addresses concerns of both performance and interpretability. Our hybrid OCC method relies on density estimation as part of a tree-based learning algorithm, called One-Class decision Tree (OC-Tree). Within a greedy and recursive approach, our proposal rests on kernel density estimation to split a data subset on the basis of one or several intervals of interest. Thus, the OC-Tree encloses data within hyper-rectangles of interest which can be described by a set of rules. Against state-of-the-art methods such as Cluster Support Vector Data Description (ClusterSVDD), One-Class Support Vector Machine (OCSVM) and isolation Forest (iForest), the OC-Tree performs favorably on a range of benchmark datasets. Furthermore, we propose a real medical application for which the OC-Tree has demonstrated its effectiveness, through the ability to tackle interpretable diagnosis aid based on unbalanced datasets.",
        "url": "https://arxiv.org/pdf/1805.05021v3.pdf"
    },
    {
        "title": "Mini-Batch Stochastic ADMMs for Nonconvex Nonsmooth Optimization",
        "abstract": "With the large rising of complex data, the nonconvex models such as nonconvex loss function and nonconvex regularizer are widely used in machine learning and pattern recognition. In this paper, we propose a class of mini-batch stochastic ADMMs (alternating direction method of multipliers) for solving large-scale nonconvex nonsmooth problems. We prove that, given an appropriate mini-batch size, the mini-batch stochastic ADMM without variance reduction (VR) technique is convergent and reaches a convergence rate of $O(1/T)$ to obtain a stationary point of the nonconvex optimization, where $T$ denotes the number of iterations. Moreover, we extend the mini-batch stochastic gradient method to both the nonconvex SVRG-ADMM and SAGA-ADMM proposed in our initial manuscript \\cite{huang2016stochastic}, and prove these mini-batch stochastic ADMMs also reaches the convergence rate of $O(1/T)$ without condition on the mini-batch size. In particular, we provide a specific parameter selection for step size $\\eta$ of stochastic gradients and penalty parameter $\\rho$ of augmented Lagrangian function. Finally, extensive experimental results on both simulated and real-world data demonstrate the effectiveness of the proposed algorithms.",
        "url": "https://arxiv.org/pdf/1802.03284v3.pdf"
    },
    {
        "title": "Evaluating Hospital Case Cost Prediction Models Using Azure Machine Learning Studio",
        "abstract": "Ability for accurate hospital case cost modelling and prediction is critical\nfor efficient health care financial management and budgetary planning. A\nvariety of regression machine learning algorithms are known to be effective for\nhealth care cost predictions. The purpose of this experiment was to build an\nAzure Machine Learning Studio tool for rapid assessment of multiple types of\nregression models. The tool offers environment for comparing 14 types of\nregression models in a unified experiment: linear regression, Bayesian linear\nregression, decision forest regression, boosted decision tree regression,\nneural network regression, Poisson regression, Gaussian processes for\nregression, gradient boosted machine, nonlinear least squares regression,\nprojection pursuit regression, random forest regression, robust regression,\nrobust regression with mm-type estimators, support vector regression. The tool\npresents assessment results arranged by model accuracy in a single table using\nfive performance metrics. Evaluation of regression machine learning models for\nperforming hospital case cost prediction demonstrated advantage of robust\nregression model, boosted decision tree regression and decision forest\nregression. The operational tool has been published to the web and openly\navailable for experiments and extensions.",
        "url": "http://arxiv.org/pdf/1804.01825v2.pdf"
    },
    {
        "title": "Automated design of collective variables using supervised machine learning",
        "abstract": "Selection of appropriate collective variables for enhancing sampling of\nmolecular simulations remains an unsolved problem in computational biophysics.\nIn particular, picking initial collective variables (CVs) is particularly\nchallenging in higher dimensions. Which atomic coordinates or transforms there\nof from a list of thousands should one pick for enhanced sampling runs? How\ndoes a modeler even begin to pick starting coordinates for investigation? This\nremains true even in the case of simple two state systems and only increases in\ndifficulty for multi-state systems. In this work, we solve the initial CV\nproblem using a data-driven approach inspired by the filed of supervised\nmachine learning. In particular, we show how the decision functions in\nsupervised machine learning (SML) algorithms can be used as initial CVs\n(SML_cv) for accelerated sampling. Using solvated alanine dipeptide and\nChignolin mini-protein as our test cases, we illustrate how the distance to the\nSupport Vector Machines' decision hyperplane, the output probability estimates\nfrom Logistic Regression, the outputs from deep neural network classifiers, and\nother classifiers may be used to reversibly sample slow structural transitions.\nWe discuss the utility of other SML algorithms that might be useful for\nidentifying CVs for accelerating molecular simulations.",
        "url": "http://arxiv.org/pdf/1802.10510v2.pdf"
    },
    {
        "title": "Extendable Neural Matrix Completion",
        "abstract": "Matrix completion is one of the key problems in signal processing and machine\nlearning, with applications ranging from image pro- cessing and data gathering\nto classification and recommender sys- tems. Recently, deep neural networks\nhave been proposed as la- tent factor models for matrix completion and have\nachieved state- of-the-art performance. Nevertheless, a major problem with\nexisting neural-network-based models is their limited capabilities to extend to\nsamples unavailable at the training stage. In this paper, we propose a deep\ntwo-branch neural network model for matrix completion. The proposed model not\nonly inherits the predictive power of neural net- works, but is also capable of\nextending to partially observed samples outside the training set, without the\nneed of retraining or fine-tuning. Experimental studies on popular movie rating\ndatasets prove the ef- fectiveness of our model compared to the state of the\nart, in terms of both accuracy and extendability.",
        "url": "http://arxiv.org/pdf/1805.04912v1.pdf"
    },
    {
        "title": "AttriGuard: A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning",
        "abstract": "Users in various web and mobile applications are vulnerable to attribute inference attacks, in which an attacker leverages a machine learning classifier to infer a target user's private attributes (e.g., location, sexual orientation, political view) from its public data (e.g., rating scores, page likes). Existing defenses leverage game theory or heuristics based on correlations between the public data and attributes. These defenses are not practical. Specifically, game-theoretic defenses require solving intractable optimization problems, while correlation-based defenses incur large utility loss of users' public data. In this paper, we present AttriGuard, a practical defense against attribute inference attacks. AttriGuard is computationally tractable and has small utility loss. Our AttriGuard works in two phases. Suppose we aim to protect a user's private attribute. In Phase I, for each value of the attribute, we find a minimum noise such that if we add the noise to the user's public data, then the attacker's classifier is very likely to infer the attribute value for the user. We find the minimum noise via adapting existing evasion attacks in adversarial machine learning. In Phase II, we sample one attribute value according to a certain probability distribution and add the corresponding noise found in Phase I to the user's public data. We formulate finding the probability distribution as solving a constrained convex optimization problem. We extensively evaluate AttriGuard and compare it with existing methods using a real-world dataset. Our results show that AttriGuard substantially outperforms existing methods. Our work is the first one that shows evasion attacks can be used as defensive techniques for privacy protection.",
        "url": "https://arxiv.org/pdf/1805.04810v2.pdf"
    },
    {
        "title": "Born Again Neural Networks",
        "abstract": "Knowledge distillation (KD) consists of transferring knowledge from one\nmachine learning model (the teacher}) to another (the student). Commonly, the\nteacher is a high-capacity model with formidable performance, while the student\nis more compact. By transferring knowledge, one hopes to benefit from the\nstudent's compactness. %we desire a compact model with performance close to the\nteacher's. We study KD from a new perspective: rather than compressing models,\nwe train students parameterized identically to their teachers. Surprisingly,\nthese {Born-Again Networks (BANs), outperform their teachers significantly,\nboth on computer vision and language modeling tasks. Our experiments with BANs\nbased on DenseNets demonstrate state-of-the-art performance on the CIFAR-10\n(3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional\nexperiments explore two distillation objectives: (i) Confidence-Weighted by\nTeacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP).\nBoth methods elucidate the essential components of KD, demonstrating a role of\nthe teacher outputs on both predicted and non-predicted classes. We present\nexperiments with students of various capacities, focusing on the under-explored\ncase where students overpower teachers. Our experiments show significant\nadvantages from transferring knowledge between DenseNets and ResNets in either\ndirection.",
        "url": "http://arxiv.org/pdf/1805.04770v2.pdf"
    },
    {
        "title": "Towards Autonomous Reinforcement Learning: Automatic Setting of Hyper-parameters using Bayesian Optimization",
        "abstract": "With the increase of machine learning usage by industries and scientific\ncommunities in a variety of tasks such as text mining, image recognition and\nself-driving cars, automatic setting of hyper-parameter in learning algorithms\nis a key factor for achieving satisfactory performance regardless of user\nexpertise in the inner workings of the techniques and methodologies. In\nparticular, for a reinforcement learning algorithm, the efficiency of an agent\nlearning a control policy in an uncertain environment is heavily dependent on\nthe hyper-parameters used to balance exploration with exploitation. In this\nwork, an autonomous learning framework that integrates Bayesian optimization\nwith Gaussian process regression to optimize the hyper-parameters of a\nreinforcement learning algorithm, is proposed. Also, a bandits-based approach\nto achieve a balance between computational costs and decreasing uncertainty\nabout the Q-values, is presented. A gridworld example is used to highlight how\nhyper-parameter configurations of a learning algorithm (SARSA) are iteratively\nimproved based on two performance functions.",
        "url": "http://arxiv.org/pdf/1805.04748v1.pdf"
    },
    {
        "title": "Pool-Based Sequential Active Learning for Regression",
        "abstract": "Active learning is a machine learning approach for reducing the data labeling\neffort. Given a pool of unlabeled samples, it tries to select the most useful\nones to label so that a model built from them can achieve the best possible\nperformance. This paper focuses on pool-based sequential active learning for\nregression (ALR). We first propose three essential criteria that an ALR\napproach should consider in selecting the most useful unlabeled samples:\ninformativeness, representativeness, and diversity, and compare four existing\nALR approaches against them. We then propose a new ALR approach using passive\nsampling, which considers both the representativeness and the diversity in both\nthe initialization and subsequent iterations. Remarkably, this approach can\nalso be integrated with other existing ALR approaches in the literature to\nfurther improve the performance. Extensive experiments on 11 UCI, CMU StatLib,\nand UFL Media Core datasets from various domains verified the effectiveness of\nour proposed ALR approaches.",
        "url": "http://arxiv.org/pdf/1805.04735v1.pdf"
    },
    {
        "title": "TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation",
        "abstract": "The field of Natural Language Processing (NLP) is growing rapidly, with new\nresearch published daily along with an abundance of tutorials, codebases and\nother online resources. In order to learn this dynamic field or stay up-to-date\non the latest research, students as well as educators and researchers must\nconstantly sift through multiple sources to find valuable, relevant\ninformation. To address this situation, we introduce TutorialBank, a new,\npublicly available dataset which aims to facilitate NLP education and research.\nWe have manually collected and categorized over 6,300 resources on NLP as well\nas the related fields of Artificial Intelligence (AI), Machine Learning (ML)\nand Information Retrieval (IR). Our dataset is notably the largest\nmanually-picked corpus of resources intended for NLP education which does not\ninclude only academic papers. Additionally, we have created both a search\nengine and a command-line tool for the resources and have annotated the corpus\nto include lists of research topics, relevant resources for each topic,\nprerequisite relations among topics, relevant sub-parts of individual\nresources, among other annotations. We are releasing the dataset and present\nseveral avenues for further research.",
        "url": "http://arxiv.org/pdf/1805.04617v1.pdf"
    },
    {
        "title": "Classification of Protein Crystallization X-Ray Images Using Major Convolutional Neural Network Architectures",
        "abstract": "The generation of protein crystals is necessary for the study of protein\nmolecular function and structure. This is done empirically by processing large\nnumbers of crystallization trials and inspecting them regularly in search of\nthose with forming crystals. To avoid missing the hard-gained crystals, this\nvisual inspection of the trial X-ray images is done manually as opposed to the\nexisting less accurate machine learning methods. To achieve higher accuracy for\nautomation, we applied some of the most successful convolutional neural\nnetworks (ResNet, Inception, VGG, and AlexNet) for 10-way classification of the\nX-ray images. We showed that substantial classification accuracy is gained by\nusing such networks compared to two simpler ones previously proposed for this\npurpose. The best accuracy was obtained from ResNet (81.43%), which corresponds\nto a missed crystal rate of 5.9%. This rate could be lowered to less than 0.1%\nby using a top-3 classification strategy. Our dataset consisted of 486,000\ninternally annotated images, which was augmented to more than a million to\naddress class imbalance. We also provide a label-wise analysis of the results,\nidentifying the main sources of error and inaccuracy.",
        "url": "http://arxiv.org/pdf/1805.04563v1.pdf"
    },
    {
        "title": "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems",
        "abstract": "Automatic machine learning systems can inadvertently accentuate and\nperpetuate inappropriate human biases. Past work on examining inappropriate\nbiases has largely focused on just individual systems. Further, there is no\nbenchmark dataset for examining inappropriate biases in systems. Here for the\nfirst time, we present the Equity Evaluation Corpus (EEC), which consists of\n8,640 English sentences carefully chosen to tease out biases towards certain\nraces and genders. We use the dataset to examine 219 automatic sentiment\nanalysis systems that took part in a recent shared task, SemEval-2018 Task 1\n'Affect in Tweets'. We find that several of the systems show statistically\nsignificant bias; that is, they consistently provide slightly higher sentiment\nintensity predictions for one race or one gender. We make the EEC freely\navailable.",
        "url": "http://arxiv.org/pdf/1805.04508v1.pdf"
    },
    {
        "title": "On the Direction of Discrimination: An Information-Theoretic Analysis of Disparate Impact in Machine Learning",
        "abstract": "In the context of machine learning, disparate impact refers to a form of\nsystematic discrimination whereby the output distribution of a model depends on\nthe value of a sensitive attribute (e.g., race or gender). In this paper, we\npropose an information-theoretic framework to analyze the disparate impact of a\nbinary classification model. We view the model as a fixed channel, and quantify\ndisparate impact as the divergence in output distributions over two groups. Our\naim is to find a correction function that can perturb the input distributions\nof each group to align their output distributions. We present an optimization\nproblem that can be solved to obtain a correction function that will make the\noutput distributions statistically indistinguishable. We derive closed-form\nexpressions to efficiently compute the correction function, and demonstrate the\nbenefits of our framework on a recidivism prediction problem based on the\nProPublica COMPAS dataset.",
        "url": "http://arxiv.org/pdf/1801.05398v3.pdf"
    },
    {
        "title": "Improved Predictive Models for Acute Kidney Injury with IDEAs: Intraoperative Data Embedded Analytics",
        "abstract": "Acute kidney injury (AKI) is a common and serious complication after a\nsurgery which is associated with morbidity and mortality. The majority of\nexisting perioperative AKI risk score prediction models are limited in their\ngeneralizability and do not fully utilize the physiological intraoperative\ntime-series data. Thus, there is a need for intelligent, accurate, and robust\nsystems, able to leverage information from large-scale data to predict\npatient's risk of developing postoperative AKI. A retrospective single-center\ncohort of 2,911 adult patients who underwent surgery at the University of\nFlorida Health has been used for this study. We used machine learning and\nstatistical analysis techniques to develop perioperative models to predict the\nrisk of AKI (risk during the first 3 days, 7 days, and until the discharge day)\nbefore and after the surgery. In particular, we examined the improvement in\nrisk prediction by incorporating three intraoperative physiologic time series\ndata, i.e., mean arterial blood pressure, minimum alveolar concentration, and\nheart rate. For an individual patient, the preoperative model produces a\nprobabilistic AKI risk score, which will be enriched by integrating\nintraoperative statistical features through a machine learning stacking\napproach inside a random forest classifier. We compared the performance of our\nmodel based on the area under the receiver operating characteristics curve\n(AUROC), accuracy and net reclassification improvement (NRI). The predictive\nperformance of the proposed model is better than the preoperative data only\nmodel. For AKI-7day outcome: The AUC was 0.86 (accuracy was 0.78) in the\nproposed model, while the preoperative AUC was 0.84 (accuracy 0.76).\nFurthermore, with the integration of intraoperative features, we were able to\nclassify patients who were misclassified in the preoperative model.",
        "url": "http://arxiv.org/pdf/1805.05452v1.pdf"
    },
    {
        "title": "Machine Learning for Public Administration Research, with Application to Organizational Reputation",
        "abstract": "Machine learning methods have gained a great deal of popularity in recent\nyears among public administration scholars and practitioners. These techniques\nopen the door to the analysis of text, image and other types of data that allow\nus to test foundational theories of public administration and to develop new\ntheories. Despite the excitement surrounding machine learning methods, clarity\nregarding their proper use and potential pitfalls is lacking. This paper\nattempts to fill this gap in the literature through providing a machine\nlearning \"guide to practice\" for public administration scholars and\npractitioners. Here, we take a foundational view of machine learning and\ndescribe how these methods can enrich public administration research and\npractice through their ability develop new measures, tap into new sources of\ndata and conduct statistical inference and causal inference in a principled\nmanner. We then turn our attention to the pitfalls of using these methods such\nas unvalidated measures and lack of interpretability. Finally, we demonstrate\nhow machine learning techniques can help us learn about organizational\nreputation in federal agencies through an illustrated example using tweets from\n13 executive federal agencies.",
        "url": "http://arxiv.org/pdf/1805.05409v2.pdf"
    },
    {
        "title": "A Sensorimotor Perspective on Grounding the Semantic of Simple Visual Features",
        "abstract": "In Machine Learning and Robotics, the semantic content of visual features is\nusually provided to the system by a human who interprets its content. On the\ncontrary, strictly unsupervised approaches have difficulties relating the\nstatistics of sensory inputs to their semantic content without also relying on\nprior knowledge introduced in the system. We proposed in this paper to tackle\nthis problem from a sensorimotor perspective. In line with the Sensorimotor\nContingencies Theory, we make the fundamental assumption that the semantic\ncontent of sensory inputs at least partially stems from the way an agent can\nactively transform it. We illustrate our approach by formalizing how simple\nvisual features can induce invariants in a naive agent's sensorimotor\nexperience, and evaluate it on a simple simulated visual system. Without any a\npriori knowledge about the way its sensorimotor information is encoded, we show\nhow an agent can characterize the uniformity and edge-ness of the visual\nfeatures it interacts with.",
        "url": "http://arxiv.org/pdf/1805.04396v1.pdf"
    },
    {
        "title": "An $O(N)$ Sorting Algorithm: Machine Learning Sort",
        "abstract": "We propose an $O(N\\cdot M)$ sorting algorithm by Machine Learning method,\nwhich shows a huge potential sorting big data. This sorting algorithm can be\napplied to parallel sorting and is suitable for GPU or TPU acceleration.\nFurthermore, we discuss the application of this algorithm to sparse hash table.",
        "url": "http://arxiv.org/pdf/1805.04272v2.pdf"
    },
    {
        "title": "Adaptive Selection of Deep Learning Models on Embedded Systems",
        "abstract": "The recent ground-breaking advances in deep learning networks ( DNNs ) make\nthem attractive for embedded systems. However, it can take a long time for DNNs\nto make an inference on resource-limited embedded devices. Offloading the\ncomputation into the cloud is often infeasible due to privacy concerns, high\nlatency, or the lack of connectivity. As such, there is a critical need to find\na way to effectively execute the DNN models locally on the devices. This paper\npresents an adaptive scheme to determine which DNN model to use for a given\ninput, by considering the desired accuracy and inference time. Our approach\nemploys machine learning to develop a predictive model to quickly select a\npre-trained DNN to use for a given input and the optimization constraint. We\nachieve this by first training off-line a predictive model, and then use the\nlearnt model to select a DNN model to use for new, unseen inputs. We apply our\napproach to the image classification task and evaluate it on a Jetson TX2\nembedded deep learning platform using the ImageNet ILSVRC 2012 validation\ndataset. We consider a range of influential DNN models. Experimental results\nshow that our approach achieves a 7.52% improvement in inference accuracy, and\na 1.8x reduction in inference time over the most-capable single DNN model.",
        "url": "http://arxiv.org/pdf/1805.04252v1.pdf"
    },
    {
        "title": "Randomized Smoothing SVRG for Large-scale Nonsmooth Convex Optimization",
        "abstract": "In this paper, we consider the problem of minimizing the average of a large\nnumber of nonsmooth and convex functions. Such problems often arise in typical\nmachine learning problems as empirical risk minimization, but are\ncomputationally very challenging. We develop and analyze a new algorithm that\nachieves robust linear convergence rate, and both its time complexity and\ngradient complexity are superior than state-of-art nonsmooth algorithms and\nsubgradient-based schemes. Besides, our algorithm works without any extra error\nbound conditions on the objective function as well as the common\nstrongly-convex condition. We show that our algorithm has wide applications in\noptimization and machine learning problems, and demonstrate experimentally that\nit performs well on a large-scale ranking problem.",
        "url": "http://arxiv.org/pdf/1805.05189v1.pdf"
    },
    {
        "title": "An Unsupervised Clustering-Based Short-Term Solar Forecasting Methodology Using Multi-Model Machine Learning Blending",
        "abstract": "Solar forecasting accuracy is affected by weather conditions, and weather\nawareness forecasting models are expected to improve the performance. However,\nit may not be available and reliable to classify different forecasting tasks by\nusing only meteorological weather categorization. In this paper, an\nunsupervised clustering-based (UC-based) solar forecasting methodology is\ndeveloped for short-term (1-hour-ahead) global horizontal irradiance (GHI)\nforecasting. This methodology consists of three parts: GHI time series\nunsupervised clustering, pattern recognition, and UC-based forecasting. The\ndaily GHI time series is first clustered by an Optimized Cross-validated\nClUsteRing (OCCUR) method, which determines the optimal number of clusters and\nbest clustering results. Then, support vector machine pattern recognition\n(SVM-PR) is adopted to recognize the category of a certain day using the first\nfew hours' data in the forecasting stage. GHI forecasts are generated by the\nmost suitable models in different clusters, which are built by a two-layer\nMachine learning based Multi-Model (M3) forecasting framework. The developed\nUC-based methodology is validated by using 1-year of data with six solar\nfeatures. Numerical results show that (i) UC-based models outperform non-UC\n(all-in-one) models with the same M3 architecture by approximately 20%; (ii)\nM3-based models also outperform the single-algorithm machine learning (SAML)\nmodels by approximately 20%.",
        "url": "http://arxiv.org/pdf/1805.04193v1.pdf"
    },
    {
        "title": "Quantum Machine Learning",
        "abstract": "Fuelled by increasing computer power and algorithmic advances, machine\nlearning techniques have become powerful tools for finding patterns in data.\nSince quantum systems produce counter-intuitive patterns believed not to be\nefficiently produced by classical systems, it is reasonable to postulate that\nquantum computers may outperform classical computers on machine learning tasks.\nThe field of quantum machine learning explores how to devise and implement\nconcrete quantum software that offers such advantages. Recent work has made\nclear that the hardware and software challenges are still considerable but has\nalso opened paths towards solutions.",
        "url": "http://arxiv.org/pdf/1611.09347v2.pdf"
    },
    {
        "title": "Towards Budget-Driven Hardware Optimization for Deep Convolutional Neural Networks using Stochastic Computing",
        "abstract": "Recently, Deep Convolutional Neural Network (DCNN) has achieved tremendous\nsuccess in many machine learning applications. Nevertheless, the deep structure\nhas brought significant increases in computation complexity. Largescale deep\nlearning systems mainly operate in high-performance server clusters, thus\nrestricting the application extensions to personal or mobile devices. Previous\nworks on GPU and/or FPGA acceleration for DCNNs show increasing speedup, but\nignore other constraints, such as area, power, and energy. Stochastic Computing\n(SC), as a unique data representation and processing technique, has the\npotential to enable the design of fully parallel and scalable hardware\nimplementations of large-scale deep learning systems. This paper proposed an\nautomatic design allocation algorithm driven by budget requirement considering\noverall accuracy performance. This systematic method enables the automatic\ndesign of a DCNN where all design parameters are jointly optimized.\nExperimental results demonstrate that proposed algorithm can achieve a joint\noptimization of all design parameters given the comprehensive budget of a DCNN.",
        "url": "http://arxiv.org/pdf/1805.04142v1.pdf"
    },
    {
        "title": "Exploiting Unintended Feature Leakage in Collaborative Learning",
        "abstract": "Collaborative machine learning and related techniques such as federated\nlearning allow multiple participants, each with his own training dataset, to\nbuild a joint model by training locally and periodically exchanging model\nupdates. We demonstrate that these updates leak unintended information about\nparticipants' training data and develop passive and active inference attacks to\nexploit this leakage. First, we show that an adversarial participant can infer\nthe presence of exact data points -- for example, specific locations -- in\nothers' training data (i.e., membership inference). Then, we show how this\nadversary can infer properties that hold only for a subset of the training data\nand are independent of the properties that the joint model aims to capture. For\nexample, he can infer when a specific person first appears in the photos used\nto train a binary gender classifier. We evaluate our attacks on a variety of\ntasks, datasets, and learning configurations, analyze their limitations, and\ndiscuss possible defenses.",
        "url": "http://arxiv.org/pdf/1805.04049v3.pdf"
    },
    {
        "title": "Deep Learning of Geometric Constellation Shaping including Fiber Nonlinearities",
        "abstract": "A new geometric shaping method is proposed, leveraging unsupervised machine\nlearning to optimize the constellation design. The learned constellation\nmitigates nonlinear effects with gains up to 0.13 bit/4D when trained with a\nsimplified fiber channel model.",
        "url": "http://arxiv.org/pdf/1805.03785v1.pdf"
    },
    {
        "title": "Expectation Propagation for Approximate Inference: Free Probability Framework",
        "abstract": "We study asymptotic properties of expectation propagation (EP) -- a method\nfor approximate inference originally developed in the field of machine\nlearning. Applied to generalized linear models, EP iteratively computes a\nmultivariate Gaussian approximation to the exact posterior distribution. The\ncomputational complexity of the repeated update of covariance matrices severely\nlimits the application of EP to large problem sizes. In this study, we present\na rigorous analysis by means of free probability theory that allows us to\novercome this computational bottleneck if specific data matrices in the problem\nfulfill certain properties of asymptotic freeness. We demonstrate the relevance\nof our approach on the gene selection problem of a microarray dataset.",
        "url": "http://arxiv.org/pdf/1801.05411v2.pdf"
    },
    {
        "title": "Variational Inference: A Review for Statisticians",
        "abstract": "One of the core problems of modern statistics is to approximate\ndifficult-to-compute probability densities. This problem is especially\nimportant in Bayesian statistics, which frames all inference about unknown\nquantities as a calculation involving the posterior density. In this paper, we\nreview variational inference (VI), a method from machine learning that\napproximates probability densities through optimization. VI has been used in\nmany applications and tends to be faster than classical methods, such as Markov\nchain Monte Carlo sampling. The idea behind VI is to first posit a family of\ndensities and then to find the member of that family which is close to the\ntarget. Closeness is measured by Kullback-Leibler divergence. We review the\nideas behind mean-field variational inference, discuss the special case of VI\napplied to exponential family models, present a full example with a Bayesian\nmixture of Gaussians, and derive a variant that uses stochastic optimization to\nscale up to massive data. We discuss modern research in VI and highlight\nimportant open problems. VI is powerful, but it is not yet well understood. Our\nhope in writing this paper is to catalyze statistical research on this class of\nalgorithms.",
        "url": "http://arxiv.org/pdf/1601.00670v9.pdf"
    },
    {
        "title": "Category Theoretic Analysis of Photon-based Decision Making",
        "abstract": "Decision making is a vital function in this age of machine learning and\nartificial intelligence, yet its physical realization and theoretical\nfundamentals are still not completely understood. In our former study, we\ndemonstrated that single-photons can be used to make decisions in uncertain,\ndynamically changing environments. The two-armed bandit problem was\nsuccessfully solved using the dual probabilistic and particle attributes of\nsingle photons. In this study, we present a category theoretic modeling and\nanalysis of single-photon-based decision making, including a quantitative\nanalysis that is in agreement with the experimental results. A category\ntheoretic model reveals the complex interdependencies of subject matter\nentities in a simplified manner, even in dynamically changing environments. In\nparticular, the octahedral and braid structures in triangulated categories\nprovide a better understanding and quantitative metrics of the underlying\nmechanisms of a single-photon decision maker. This study provides both insight\nand a foundation for analyzing more complex and uncertain problems, to further\nmachine learning and artificial intelligence.",
        "url": "http://arxiv.org/pdf/1602.08199v3.pdf"
    },
    {
        "title": "On the Limitation of MagNet Defense against $L_1$-based Adversarial Examples",
        "abstract": "In recent years, defending adversarial perturbations to natural examples in\norder to build robust machine learning models trained by deep neural networks\n(DNNs) has become an emerging research field in the conjunction of deep\nlearning and security. In particular, MagNet consisting of an adversary\ndetector and a data reformer is by far one of the strongest defenses in the\nblack-box oblivious attack setting, where the attacker aims to craft\ntransferable adversarial examples from an undefended DNN model to bypass an\nunknown defense module deployed on the same DNN model. Under this setting,\nMagNet can successfully defend a variety of attacks in DNNs, including the\nhigh-confidence adversarial examples generated by the Carlini and Wagner's\nattack based on the $L_2$ distortion metric. However, in this paper, under the\nsame attack setting we show that adversarial examples crafted based on the\n$L_1$ distortion metric can easily bypass MagNet and mislead the target DNN\nimage classifiers on MNIST and CIFAR-10. We also provide explanations on why\nthe considered approach can yield adversarial examples with superior attack\nperformance and conduct extensive experiments on variants of MagNet to verify\nits lack of robustness to $L_1$ distortion based attacks. Notably, our results\nsubstantially weaken the assumption of effective threat models on MagNet that\nrequire knowing the deployed defense technique when attacking DNNs (i.e., the\ngray-box attack setting).",
        "url": "http://arxiv.org/pdf/1805.00310v2.pdf"
    },
    {
        "title": "High-resolution medical image synthesis using progressively grown generative adversarial networks",
        "abstract": "Generative adversarial networks (GANs) are a class of unsupervised machine\nlearning algorithms that can produce realistic images from randomly-sampled\nvectors in a multi-dimensional space. Until recently, it was not possible to\ngenerate realistic high-resolution images using GANs, which has limited their\napplicability to medical images that contain biomarkers only detectable at\nnative resolution. Progressive growing of GANs is an approach wherein an image\ngenerator is trained to initially synthesize low resolution synthetic images\n(8x8 pixels), which are then fed to a discriminator that distinguishes these\nsynthetic images from real downsampled images. Additional convolutional layers\nare then iteratively introduced to produce images at twice the previous\nresolution until the desired resolution is reached. In this work, we\ndemonstrate that this approach can produce realistic medical images in two\ndifferent domains; fundus photographs exhibiting vascular pathology associated\nwith retinopathy of prematurity (ROP), and multi-modal magnetic resonance\nimages of glioma. We also show that fine-grained details associated with\npathology, such as retinal vessels or tumor heterogeneity, can be preserved and\nenhanced by including segmentation maps as additional channels. We envisage\nseveral applications of the approach, including image augmentation and\nunsupervised classification of pathology.",
        "url": "http://arxiv.org/pdf/1805.03144v2.pdf"
    },
    {
        "title": "TensorLy: Tensor Learning in Python",
        "abstract": "Tensors are higher-order extensions of matrices. While matrix methods form\nthe cornerstone of machine learning and data analysis, tensor methods have been\ngaining increasing traction. However, software support for tensor operations is\nnot on the same footing. In order to bridge this gap, we have developed\n\\emph{TensorLy}, a high-level API for tensor methods and deep tensorized neural\nnetworks in Python. TensorLy aims to follow the same standards adopted by the\nmain projects of the Python scientific community, and seamlessly integrates\nwith them. Its BSD license makes it suitable for both academic and commercial\napplications. TensorLy's backend system allows users to perform computations\nwith NumPy, MXNet, PyTorch, TensorFlow and CuPy. They can be scaled on multiple\nCPU or GPU machines. In addition, using the deep-learning frameworks as backend\nallows users to easily design and train deep tensorized neural networks.\nTensorLy is available at https://github.com/tensorly/tensorly",
        "url": "http://arxiv.org/pdf/1610.09555v2.pdf"
    },
    {
        "title": "Machine Learning in Compiler Optimisation",
        "abstract": "In the last decade, machine learning based compilation has moved from an an\nobscure research niche to a mainstream activity. In this article, we describe\nthe relationship between machine learning and compiler optimisation and\nintroduce the main concepts of features, models, training and deployment. We\nthen provide a comprehensive survey and provide a road map for the wide variety\nof different research areas. We conclude with a discussion on open issues in\nthe area and potential research directions. This paper provides both an\naccessible introduction to the fast moving area of machine learning based\ncompilation and a detailed bibliography of its main achievements.",
        "url": "http://arxiv.org/pdf/1805.03441v1.pdf"
    },
    {
        "title": "SeNA-CNN: Overcoming Catastrophic Forgetting in Convolutional Neural Networks by Selective Network Augmentation",
        "abstract": "Lifelong learning aims to develop machine learning systems that can learn new\ntasks while preserving the performance on previous learned tasks. In this paper\nwe present a method to overcome catastrophic forgetting on convolutional neural\nnetworks, that learns new tasks and preserves the performance on old tasks\nwithout accessing the data of the original model, by selective network\naugmentation. The experiment results showed that SeNA-CNN, in some scenarios,\noutperforms the state-of-art Learning without Forgetting algorithm. Results\nalso showed that in some situations it is better to use SeNA-CNN instead of\ntraining a neural network using isolated learning.",
        "url": "http://arxiv.org/pdf/1802.08250v2.pdf"
    },
    {
        "title": "Neural Networks for Predicting Algorithm Runtime Distributions",
        "abstract": "Many state-of-the-art algorithms for solving hard combinatorial problems in\nartificial intelligence (AI) include elements of stochasticity that lead to\nhigh variations in runtime, even for a fixed problem instance. Knowledge about\nthe resulting runtime distributions (RTDs) of algorithms on given problem\ninstances can be exploited in various meta-algorithmic procedures, such as\nalgorithm selection, portfolios, and randomized restarts. Previous work has\nshown that machine learning can be used to individually predict mean, median\nand variance of RTDs. To establish a new state-of-the-art in predicting RTDs,\nwe demonstrate that the parameters of an RTD should be learned jointly and that\nneural networks can do this well by directly optimizing the likelihood of an\nRTD given runtime observations. In an empirical study involving five algorithms\nfor SAT solving and AI planning, we show that neural networks predict the true\nRTDs of unseen instances better than previous methods, and can even do so when\nonly few runtime observations are available per training instance.",
        "url": "http://arxiv.org/pdf/1709.07615v3.pdf"
    },
    {
        "title": "Learning to Teach",
        "abstract": "Teaching plays a very important role in our society, by spreading human\nknowledge and educating our next generations. A good teacher will select\nappropriate teaching materials, impact suitable methodologies, and set up\ntargeted examinations, according to the learning behaviors of the students. In\nthe field of artificial intelligence, however, one has not fully explored the\nrole of teaching, and pays most attention to machine \\emph{learning}. In this\npaper, we argue that equal attention, if not more, should be paid to teaching,\nand furthermore, an optimization framework (instead of heuristics) should be\nused to obtain good teaching strategies. We call this approach `learning to\nteach'. In the approach, two intelligent agents interact with each other: a\nstudent model (which corresponds to the learner in traditional machine learning\nalgorithms), and a teacher model (which determines the appropriate data, loss\nfunction, and hypothesis space to facilitate the training of the student\nmodel). The teacher model leverages the feedback from the student model to\noptimize its own teaching strategies by means of reinforcement learning, so as\nto achieve teacher-student co-evolution. To demonstrate the practical value of\nour proposed approach, we take the training of deep neural networks (DNN) as an\nexample, and show that by using the learning to teach techniques, we are able\nto use much less training data and fewer iterations to achieve almost the same\naccuracy for different kinds of DNN models (e.g., multi-layer perceptron,\nconvolutional neural networks and recurrent neural networks) under various\nmachine learning tasks (e.g., image classification and text understanding).",
        "url": "http://arxiv.org/pdf/1805.03643v1.pdf"
    },
    {
        "title": "The Three Pillars of Machine Programming",
        "abstract": "In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and(iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.",
        "url": "https://arxiv.org/pdf/1803.07244v3.pdf"
    },
    {
        "title": "Fully Automated Segmentation of Hyperreflective Foci in Optical Coherence Tomography Images",
        "abstract": "The automatic detection of disease related entities in retinal imaging data\nis relevant for disease- and treatment monitoring. It enables the quantitative\nassessment of large amounts of data and the corresponding study of disease\ncharacteristics. The presence of hyperreflective foci (HRF) is related to\ndisease progression in various retinal diseases. Manual identification of HRF\nin spectral-domain optical coherence tomography (SD-OCT) scans is error-prone\nand tedious. We present a fully automated machine learning approach for\nsegmenting HRF in SD-OCT scans. Evaluation on annotated OCT images of the\nretina demonstrates that a residual U-Net allows to segment HRF with high\naccuracy. As our dataset comprised data from different retinal diseases\nincluding age-related macular degeneration, diabetic macular edema and retinal\nvein occlusion, the algorithm can safely be applied in all of them though\ndifferent pathophysiological origins are known.",
        "url": "http://arxiv.org/pdf/1805.03278v1.pdf"
    },
    {
        "title": "Parallel Computation of PDFs on Big Spatial Data Using Spark",
        "abstract": "We consider big spatial data, which is typically produced in scientific areas\nsuch as geological or seismic interpretation. The spatial data can be produced\nby observation (e.g. using sensors or soil instrument) or numerical simulation\nprograms and correspond to points that represent a 3D soil cube area. However,\nerrors in signal processing and modeling create some uncertainty, and thus a\nlack of accuracy in identifying geological or seismic phenomenons. Such\nuncertainty must be carefully analyzed. To analyze uncertainty, the main\nsolution is to compute a Probability Density Function (PDF) of each point in\nthe spatial cube area. However, computing PDFs on big spatial data can be very\ntime consuming (from several hours to even months on a parallel computer). In\nthis paper, we propose a new solution to efficiently compute such PDFs in\nparallel using Spark, with three methods: data grouping, machine learning\nprediction and sampling. We evaluate our solution by extensive experiments on\ndifferent computer clusters using big data ranging from hundreds of GB to\nseveral TB. The experimental results show that our solution scales up very well\nand can reduce the execution time by a factor of 33 (in the order of seconds or\nminutes) compared with a baseline method.",
        "url": "http://arxiv.org/pdf/1805.03141v1.pdf"
    },
    {
        "title": "Tensor Decomposition for Compressing Recurrent Neural Network",
        "abstract": "In the machine learning fields, Recurrent Neural Network (RNN) has become a\npopular architecture for sequential data modeling. However, behind the\nimpressive performance, RNNs require a large number of parameters for both\ntraining and inference. In this paper, we are trying to reduce the number of\nparameters and maintain the expressive power from RNN simultaneously. We\nutilize several tensor decompositions method including CANDECOMP/PARAFAC (CP),\nTucker decomposition and Tensor Train (TT) to re-parameterize the Gated\nRecurrent Unit (GRU) RNN. We evaluate all tensor-based RNNs performance on\nsequence modeling tasks with a various number of parameters. Based on our\nexperiment results, TT-GRU achieved the best results in a various number of\nparameters compared to other decomposition methods.",
        "url": "http://arxiv.org/pdf/1802.10410v2.pdf"
    },
    {
        "title": "Local, algebraic simplifications of Gaussian random fields",
        "abstract": "Many applications of Gaussian random fields and Gaussian random processes are\nlimited by the computational complexity of evaluating the probability density\nfunction, which involves inverting the relevant covariance matrix. In this\nwork, we show how that problem can be completely circumvented for the local\nTaylor coefficients of a Gaussian random field with a Gaussian (or `square\nexponential') covariance function. Our results hold for any dimension of the\nfield and to any order in the Taylor expansion. We present two applications.\nFirst, we show that this method can be used to explicitly generate non-trivial\npotential energy landscapes with many fields. This application is particularly\nuseful when one is concerned with the field locally around special points\n(e.g.~maxima or minima), as we exemplify by the problem of cosmic `manyfield'\ninflation in the early universe. Second, we show that this method has\napplications in machine learning, and greatly simplifies the regression problem\nof determining the hyperparameters of the covariance function given a training\ndata set consisting of local Taylor coefficients at single point. An\naccompanying Mathematica notebook is available at\nhttps://doi.org/10.17863/CAM.22859 .",
        "url": "http://arxiv.org/pdf/1805.03117v1.pdf"
    },
    {
        "title": "Efficient online learning for large-scale peptide identification",
        "abstract": "Motivation: Post-database searching is a key procedure in peptide\ndentification with tandem mass spectrometry (MS/MS) strategies for refining\npeptide-spectrum matches (PSMs) generated by database search engines. Although\nmany statistical and machine learning-based methods have been developed to\nimprove the accuracy of peptide identification, the challenge remains on\nlarge-scale datasets and datasets with an extremely large proportion of false\npositives (hard datasets). A more efficient learning strategy is required for\nimproving the performance of peptide identification on challenging datasets.\n  Results: In this work, we present an online learning method to conquer the\nchallenges remained for exiting peptide identification algorithms. We propose a\ncost-sensitive learning model by using different loss functions for decoy and\ntarget PSMs respectively. A larger penalty for wrongly selecting decoy PSMs\nthan that for target PSMs, and thus the new model can reduce its false\ndiscovery rate on hard datasets. Also, we design an online learning algorithm,\nOLCS-Ranker, to solve the proposed learning model. Rather than taking all\ntraining data samples all at once, OLCS-Ranker iteratively feeds in only one\ntraining sample into the learning model at each round. As a result, the memory\nrequirement is significantly reduced for large-scale problems. Experimental\nstudies show that OLCS-Ranker outperforms benchmark methods, such as CRanker\nand Batch-CS-Ranker, in terms of accuracy and stability. Furthermore,\nOLCS-Ranker is 15--85 times faster than CRanker method on large datasets.\n  Availability and implementation: OLCS-Ranker software is available at no\ncharge for non-commercial use at https://github.com/Isaac-QiXing/CRanker.",
        "url": "http://arxiv.org/pdf/1805.03006v1.pdf"
    },
    {
        "title": "Differential Equations for Modeling Asynchronous Algorithms",
        "abstract": "Asynchronous stochastic gradient descent (ASGD) is a popular parallel\noptimization algorithm in machine learning. Most theoretical analysis on ASGD\ntake a discrete view and prove upper bounds for their convergence rates.\nHowever, the discrete view has its intrinsic limitations: there is no\ncharacterization of the optimization path and the proof techniques are\ninduction-based and thus usually complicated. Inspired by the recent successful\nadoptions of stochastic differential equations (SDE) to the theoretical\nanalysis of SGD, in this paper, we study the continuous approximation of ASGD\nby using stochastic differential delay equations (SDDE). We introduce the\napproximation method and study the approximation error. Then we conduct\ntheoretical analysis on the convergence rates of ASGD algorithm based on the\ncontinuous approximation. There are two methods: moment estimation and energy\nfunction minimization can be used to analyze the convergence rates. Moment\nestimation depends on the specific form of the loss function, while energy\nfunction minimization only leverages the convex property of the loss function,\nand does not depend on its specific form. In addition to the convergence\nanalysis, the continuous view also helps us derive better convergence rates.\nAll of this clearly shows the advantage of taking the continuous view in\ngradient descent algorithms.",
        "url": "http://arxiv.org/pdf/1805.02991v1.pdf"
    },
    {
        "title": "A Self-paced Regularization Framework for Partial-Label Learning",
        "abstract": "Partial label learning (PLL) aims to solve the problem where each training\ninstance is associated with a set of candidate labels, one of which is the\ncorrect label. Most PLL algorithms try to disambiguate the candidate label set,\nby either simply treating each candidate label equally or iteratively\nidentifying the true label. Nonetheless, existing algorithms usually treat all\nlabels and instances equally, and the complexities of both labels and instances\nare not taken into consideration during the learning stage. Inspired by the\nsuccessful application of self-paced learning strategy in machine learning\nfield, we integrate the self-paced regime into the partial label learning\nframework and propose a novel Self-Paced Partial-Label Learning (SP-PLL)\nalgorithm, which could control the learning process to alleviate the problem by\nranking the priorities of the training examples together with their candidate\nlabels during each learning iteration. Extensive experiments and comparisons\nwith other baseline methods demonstrate the effectiveness and robustness of the\nproposed method.",
        "url": "http://arxiv.org/pdf/1804.07759v2.pdf"
    },
    {
        "title": "Online normalizer calculation for softmax",
        "abstract": "The Softmax function is ubiquitous in machine learning, multiple previous\nworks suggested faster alternatives for it. In this paper we propose a way to\ncompute classical Softmax with fewer memory accesses and hypothesize that this\nreduction in memory accesses should improve Softmax performance on actual\nhardware. The benchmarks confirm this hypothesis: Softmax accelerates by up to\n1.3x and Softmax+TopK combined and fused by up to 5x.",
        "url": "http://arxiv.org/pdf/1805.02867v2.pdf"
    },
    {
        "title": "Fighting Accounting Fraud Through Forensic Data Analytics",
        "abstract": "Accounting fraud is a global concern representing a significant threat to the\nfinancial system stability due to the resulting diminishing of the market\nconfidence and trust of regulatory authorities. Several tricks can be used to\ncommit accounting fraud, hence the need for non-static regulatory interventions\nthat take into account different fraudulent patterns. Accordingly, this study\naims to improve the detection of accounting fraud via the implementation of\nseveral machine learning methods to better differentiate between fraud and\nnon-fraud companies, and to further assist the task of examination within the\nriskier firms by evaluating relevant financial indicators. Out-of-sample\nresults suggest there is a great potential in detecting falsified financial\nstatements through statistical modelling and analysis of publicly available\naccounting information. The proposed methodology can be of assistance to public\nauditors and regulatory agencies as it facilitates auditing processes, and\nsupports more targeted and effective examinations of accounting reports.",
        "url": "http://arxiv.org/pdf/1805.02840v1.pdf"
    },
    {
        "title": "Machine Learning for Wireless Connectivity and Security of Cellular-Connected UAVs",
        "abstract": "Cellular-connected unmanned aerial vehicles (UAVs) will inevitably be\nintegrated into future cellular networks as new aerial mobile users. Providing\ncellular connectivity to UAVs will enable a myriad of applications ranging from\nonline video streaming to medical delivery. However, to enable a reliable\nwireless connectivity for the UAVs as well as a secure operation, various\nchallenges need to be addressed such as interference management, mobility\nmanagement and handover, cyber-physical attacks, and authentication. In this\npaper, the goal is to expose the wireless and security challenges that arise in\nthe context of UAV-based delivery systems, UAV-based real-time multimedia\nstreaming, and UAV-enabled intelligent transportation systems. To address such\nchallenges, artificial neural network (ANN) based solution schemes are\nintroduced. The introduced approaches enable the UAVs to adaptively exploit the\nwireless system resources while guaranteeing a secure operation, in real-time.\nPreliminary simulation results show the benefits of the introduced solutions\nfor each of the aforementioned cellular-connected UAV application use case.",
        "url": "http://arxiv.org/pdf/1804.05348v3.pdf"
    },
    {
        "title": "Detecting Compressed Cleartext Traffic from Consumer Internet of Things Devices",
        "abstract": "Data encryption is the primary method of protecting the privacy of consumer\ndevice Internet communications from network observers. The ability to\nautomatically detect unencrypted data in network traffic is therefore an\nessential tool for auditing Internet-connected devices. Existing methods\nidentify network packets containing cleartext but cannot differentiate packets\ncontaining encrypted data from packets containing compressed unencrypted data,\nwhich can be easily recovered by reversing the compression algorithm. This\nmakes it difficult for consumer protection advocates to identify devices that\nrisk user privacy by sending sensitive data in a compressed unencrypted format.\nHere, we present the first technique to automatically distinguish encrypted\nfrom compressed unencrypted network transmissions on a per-packet basis. We\napply three machine learning models and achieve a maximum 66.9% accuracy with a\nconvolutional neural network trained on raw packet data. This result is a\nbaseline for this previously unstudied machine learning problem, which we hope\nwill motivate further attention and accuracy improvements. To facilitate\ncontinuing research on this topic, we have made our training and test datasets\navailable to the public.",
        "url": "http://arxiv.org/pdf/1805.02722v1.pdf"
    },
    {
        "title": "Label Refinery: Improving ImageNet Classification through Label Progression",
        "abstract": "Among the three main components (data, labels, and models) of any supervised\nlearning system, data and models have been the main subjects of active\nresearch. However, studying labels and their properties has received very\nlittle attention. Current principles and paradigms of labeling impose several\nchallenges to machine learning algorithms. Labels are often incomplete,\nambiguous, and redundant. In this paper we study the effects of various\nproperties of labels and introduce the Label Refinery: an iterative procedure\nthat updates the ground truth labels after examining the entire dataset. We\nshow significant gain using refined labels across a wide range of models. Using\na Label Refinery improves the state-of-the-art top-1 accuracy of (1) AlexNet\nfrom 59.3 to 67.2, (2) MobileNet from 70.6 to 73.39, (3) MobileNet-0.25 from\n50.6 to 55.59, (4) VGG19 from 72.7 to 75.46, and (5) Darknet19 from 72.9 to\n74.47.",
        "url": "http://arxiv.org/pdf/1805.02641v1.pdf"
    },
    {
        "title": "Procedural Content Generation via Machine Learning (PCGML)",
        "abstract": "This survey explores Procedural Content Generation via Machine Learning\n(PCGML), defined as the generation of game content using machine learning\nmodels trained on existing content. As the importance of PCG for game\ndevelopment increases, researchers explore new avenues for generating\nhigh-quality content with or without human involvement; this paper addresses\nthe relatively new paradigm of using machine learning (in contrast with\nsearch-based, solver-based, and constructive methods). We focus on what is most\noften considered functional game content such as platformer levels, game maps,\ninteractive fiction stories, and cards in collectible card games, as opposed to\ncosmetic content such as sprites and sound effects. In addition to using PCG\nfor autonomous generation, co-creativity, mixed-initiative design, and\ncompression, PCGML is suited for repair, critique, and content analysis because\nof its focus on modeling existing content. We discuss various data sources and\nrepresentations that affect the resulting generated content. Multiple PCGML\nmethods are covered, including neural networks, long short-term memory (LSTM)\nnetworks, autoencoders, and deep convolutional networks; Markov models,\n$n$-grams, and multi-dimensional Markov chains; clustering; and matrix\nfactorization. Finally, we discuss open problems in the application of PCGML,\nincluding learning from small datasets, lack of training data, multi-layered\nlearning, style-transfer, parameter tuning, and PCG as a game mechanic.",
        "url": "http://arxiv.org/pdf/1702.00539v3.pdf"
    },
    {
        "title": "The Concept of the Deep Learning-Based System \"Artificial Dispatcher\" to Power System Control and Dispatch",
        "abstract": "Year by year control of normal and emergency conditions of up-to-date power\nsystems becomes an increasingly complicated problem. With the increasing\ncomplexity the existing control system of power system conditions which\nincludes operative actions of the dispatcher and work of special automatic\ndevices proves to be insufficiently effective more and more frequently, which\nraises risks of dangerous and emergency conditions in power systems. The paper\nis aimed at compensating for the shortcomings of man (a cognitive barrier,\nexposure to stresses and so on) and automatic devices by combining their strong\npoints, i.e. the dispatcher's intelligence and the speed of automatic devices\nby virtue of development of the intelligent system \"Artificial dispatcher\" on\nthe basis of deep machine learning technology. For realization of the system\n\"Artificial dispatcher\" in addition to deep learning it is planned to attract\nthe game theory approaches to formalize work of the up-to-date power system as\na game problem. The \"gain\" for \"Artificial dispatcher\" will consist in bringing\nin a power system in the normal steady-state or post-emergency conditions by\nmeans of the required control actions.",
        "url": "http://arxiv.org/pdf/1805.05408v1.pdf"
    },
    {
        "title": "Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring",
        "abstract": "Non-uniform blind deblurring for general dynamic scenes is a challenging\ncomputer vision problem as blurs arise not only from multiple object motions\nbut also from camera shake, scene depth variation. To remove these complicated\nmotion blurs, conventional energy optimization based methods rely on simple\nassumptions such that blur kernel is partially uniform or locally linear.\nMoreover, recent machine learning based methods also depend on synthetic blur\ndatasets generated under these assumptions. This makes conventional deblurring\nmethods fail to remove blurs where blur kernel is difficult to approximate or\nparameterize (e.g. object motion boundaries). In this work, we propose a\nmulti-scale convolutional neural network that restores sharp images in an\nend-to-end manner where blur is caused by various sources. Together, we present\nmulti-scale loss function that mimics conventional coarse-to-fine approaches.\nFurthermore, we propose a new large-scale dataset that provides pairs of\nrealistic blurry image and the corresponding ground truth sharp image that are\nobtained by a high-speed camera. With the proposed model trained on this\ndataset, we demonstrate empirically that our method achieves the\nstate-of-the-art performance in dynamic scene deblurring not only\nqualitatively, but also quantitatively.",
        "url": "http://arxiv.org/pdf/1612.02177v2.pdf"
    },
    {
        "title": "Automatic Classification of Object Code Using Machine Learning",
        "abstract": "Recent research has repeatedly shown that machine learning techniques can be\napplied to either whole files or file fragments to classify them for analysis.\nWe build upon these techniques to show that for samples of un-labeled compiled\ncomputer object code, one can apply the same type of analysis to classify\nimportant aspects of the code, such as its target architecture and endianess.\nWe show that using simple byte-value histograms we retain enough information\nabout the opcodes within a sample to classify the target architecture with high\naccuracy, and then discuss heuristic-based features that exploit information\nwithin the operands to determine endianess. We introduce a dataset with over\n16000 code samples from 20 architectures and experimentally show that by using\nour features, classifiers can achieve very high accuracy with relatively small\nsample sizes.",
        "url": "http://arxiv.org/pdf/1805.02146v1.pdf"
    },
    {
        "title": "The Impact of Local Geometry and Batch Size on Stochastic Gradient Descent for Nonconvex Problems",
        "abstract": "In several experimental reports on nonconvex optimization problems in machine\nlearning, stochastic gradient descent (SGD) was observed to prefer minimizers\nwith flat basins in comparison to more deterministic methods, yet there is very\nlittle rigorous understanding of this phenomenon. In fact, the lack of such\nwork has led to an unverified, but widely-accepted stochastic mechanism\ndescribing why SGD prefers flatter minimizers to sharper minimizers. However,\nas we demonstrate, the stochastic mechanism fails to explain this phenomenon.\nHere, we propose an alternative deterministic mechanism that can accurately\nexplain why SGD prefers flatter minimizers to sharper minimizers. We derive\nthis mechanism based on a detailed analysis of a generic stochastic quadratic\nproblem, which generalizes known results for classical gradient descent.\nFinally, we verify the predictions of our deterministic mechanism on two\nnonconvex problems.",
        "url": "http://arxiv.org/pdf/1709.04718v2.pdf"
    },
    {
        "title": "Learning Patient Representations from Text",
        "abstract": "Mining electronic health records for patients who satisfy a set of predefined\ncriteria is known in medical informatics as phenotyping. Phenotyping has\nnumerous applications such as outcome prediction, clinical trial recruitment,\nand retrospective studies. Supervised machine learning for phenotyping\ntypically relies on sparse patient representations such as bag-of-words. We\nconsider an alternative that involves learning patient representations. We\ndevelop a neural network model for learning patient representations and show\nthat the learned representations are general enough to obtain state-of-the-art\nperformance on a standard comorbidity detection task.",
        "url": "http://arxiv.org/pdf/1805.02096v1.pdf"
    },
    {
        "title": "A Cost-Sensitive Deep Belief Network for Imbalanced Classification",
        "abstract": "Imbalanced data with a skewed class distribution are common in many\nreal-world applications. Deep Belief Network (DBN) is a machine learning\ntechnique that is effective in classification tasks. However, conventional DBN\ndoes not work well for imbalanced data classification because it assumes equal\ncosts for each class. To deal with this problem, cost-sensitive approaches\nassign different misclassification costs for different classes without\ndisrupting the true data sample distributions. However, due to lack of prior\nknowledge, the misclassification costs are usually unknown and hard to choose\nin practice. Moreover, it has not been well studied as to how cost-sensitive\nlearning could improve DBN performance on imbalanced data problems. This paper\nproposes an evolutionary cost-sensitive deep belief network (ECS-DBN) for\nimbalanced classification. ECS-DBN uses adaptive differential evolution to\noptimize the misclassification costs based on training data, that presents an\neffective approach to incorporating the evaluation measure (i.e. G-mean) into\nthe objective function. We first optimize the misclassification costs, then\napply them to deep belief network. Adaptive differential evolution optimization\nis implemented as the optimization algorithm that automatically updates its\ncorresponding parameters without the need of prior domain knowledge. The\nexperiments have shown that the proposed approach consistently outperforms the\nstate-of-the-art on both benchmark datasets and real-world dataset for fault\ndiagnosis in tool condition monitoring.",
        "url": "http://arxiv.org/pdf/1804.10801v2.pdf"
    },
    {
        "title": "Various Approaches to Aspect-based Sentiment Analysis",
        "abstract": "The problem of aspect-based sentiment analysis deals with classifying\nsentiments (negative, neutral, positive) for a given aspect in a sentence. A\ntraditional sentiment classification task involves treating the entire sentence\nas a text document and classifying sentiments based on all the words. Let us\nassume, we have a sentence such as \"the acceleration of this car is fast, but\nthe reliability is horrible\". This can be a difficult sentence because it has\ntwo aspects with conflicting sentiments about the same entity. Considering\nmachine learning techniques (or deep learning), how do we encode the\ninformation that we are interested in one aspect and its sentiment but not the\nother? Let us explore various pre-processing steps, features, and methods used\nto facilitate in solving this task.",
        "url": "http://arxiv.org/pdf/1805.01984v1.pdf"
    },
    {
        "title": "A Survey of Machine Learning for Big Code and Naturalness",
        "abstract": "Research at the intersection of machine learning, programming languages, and\nsoftware engineering has recently taken important steps in proposing learnable\nprobabilistic models of source code that exploit code's abundance of patterns.\nIn this article, we survey this work. We contrast programming languages against\nnatural languages and discuss how these similarities and differences drive the\ndesign of probabilistic models. We present a taxonomy based on the underlying\ndesign principles of each model and use it to navigate the literature. Then, we\nreview how researchers have adapted these models to application areas and\ndiscuss cross-cutting and application-specific challenges and opportunities.",
        "url": "http://arxiv.org/pdf/1709.06182v2.pdf"
    },
    {
        "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
        "abstract": "Machine learning algorithms, when applied to sensitive data, pose a distinct\nthreat to privacy. A growing body of prior work demonstrates that models\nproduced by these algorithms may leak specific private information in the\ntraining data to an attacker, either through the models' structure or their\nobservable behavior. However, the underlying cause of this privacy risk is not\nwell understood beyond a handful of anecdotal accounts that suggest overfitting\nand influence might play a role.\n  This paper examines the effect that overfitting and influence have on the\nability of an attacker to learn information about the training data from\nmachine learning models, either through training set membership inference or\nattribute inference attacks. Using both formal and empirical analyses, we\nillustrate a clear relationship between these factors and the privacy risk that\narises in several popular machine learning algorithms. We find that overfitting\nis sufficient to allow an attacker to perform membership inference and, when\nthe target attribute meets certain conditions about its influence, attribute\ninference attacks. Interestingly, our formal analysis also shows that\noverfitting is not necessary for these attacks and begins to shed light on what\nother factors may be in play. Finally, we explore the connection between\nmembership inference and attribute inference, showing that there are deep\nconnections between the two that lead to effective new attacks.",
        "url": "http://arxiv.org/pdf/1709.01604v5.pdf"
    },
    {
        "title": "Machine learning for neural decoding",
        "abstract": "Despite rapid advances in machine learning tools, the majority of neural decoding approaches still use traditional methods. Modern machine learning tools, which are versatile and easy to use, have the potential to significantly improve decoding performance. This tutorial describes how to effectively apply these algorithms for typical decoding problems. We provide descriptions, best practices, and code for applying common machine learning methods, including neural networks and gradient boosting. We also provide detailed comparisons of the performance of various methods at the task of decoding spiking activity in motor cortex, somatosensory cortex, and hippocampus. Modern methods, particularly neural networks and ensembles, significantly outperform traditional approaches, such as Wiener and Kalman filters. Improving the performance of neural decoding algorithms allows neuroscientists to better understand the information contained in a neural population and can help advance engineering applications such as brain machine interfaces.",
        "url": "https://arxiv.org/pdf/1708.00909v4.pdf"
    },
    {
        "title": "Modeling Dengue Vector Population Using Remotely Sensed Data and Machine Learning",
        "abstract": "Mosquitoes are vectors of many human diseases. In particular, Aedes \\ae gypti\n(Linnaeus) is the main vector for Chikungunya, Dengue, and Zika viruses in\nLatin America and it represents a global threat. Public health policies that\naim at combating this vector require dependable and timely information, which\nis usually expensive to obtain with field campaigns. For this reason, several\nefforts have been done to use remote sensing due to its reduced cost. The\npresent work includes the temporal modeling of the oviposition activity\n(measured weekly on 50 ovitraps in a north Argentinean city) of Aedes \\ae gypti\n(Linnaeus), based on time series of data extracted from operational earth\nobservation satellite images. We use are NDVI, NDWI, LST night, LST day and\nTRMM-GPM rain from 2012 to 2016 as predictive variables. In contrast to\nprevious works which use linear models, we employ Machine Learning techniques\nusing completely accessible open source toolkits. These models have the\nadvantages of being non-parametric and capable of describing nonlinear\nrelationships between variables. Specifically, in addition to two linear\napproaches, we assess a Support Vector Machine, an Artificial Neural Networks,\na K-nearest neighbors and a Decision Tree Regressor. Considerations are made on\nparameter tuning and the validation and training approach. The results are\ncompared to linear models used in previous works with similar data sets for\ngenerating temporal predictive models. These new tools perform better than\nlinear approaches, in particular Nearest Neighbor Regression (KNNR) performs\nthe best. These results provide better alternatives to be implemented\noperatively on the Argentine geospatial Risk system that is running since 2012.",
        "url": "http://arxiv.org/pdf/1805.02590v1.pdf"
    },
    {
        "title": "Dynamic Control Flow in Large-Scale Machine Learning",
        "abstract": "Many recent machine learning models rely on fine-grained dynamic control flow\nfor training and inference. In particular, models based on recurrent neural\nnetworks and on reinforcement learning depend on recurrence relations,\ndata-dependent conditional execution, and other features that call for dynamic\ncontrol flow. These applications benefit from the ability to make rapid\ncontrol-flow decisions across a set of computing devices in a distributed\nsystem. For performance, scalability, and expressiveness, a machine learning\nsystem must support dynamic control flow in distributed and heterogeneous\nenvironments.\n  This paper presents a programming model for distributed machine learning that\nsupports dynamic control flow. We describe the design of the programming model,\nand its implementation in TensorFlow, a distributed machine learning system.\nOur approach extends the use of dataflow graphs to represent machine learning\nmodels, offering several distinctive features. First, the branches of\nconditionals and bodies of loops can be partitioned across many machines to run\non a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs.\nSecond, programs written in our model support automatic differentiation and\ndistributed gradient computations, which are necessary for training machine\nlearning models that use control flow. Third, our choice of non-strict\nsemantics enables multiple loop iterations to execute in parallel across\nmachines, and to overlap compute and I/O operations.\n  We have done our work in the context of TensorFlow, and it has been used\nextensively in research and production. We evaluate it using several real-world\napplications, and demonstrate its performance and scalability.",
        "url": "http://arxiv.org/pdf/1805.01772v1.pdf"
    },
    {
        "title": "A brief introduction to the Grey Machine Learning",
        "abstract": "This paper presents a brief introduction to the key points of the Grey\nMachine Learning (GML) based on the kernels. The general formulation of the\ngrey system models have been firstly summarized, and then the nonlinear\nextension of the grey models have been developed also with general\nformulations. The kernel implicit mapping is used to estimate the nonlinear\nfunction of the GML model, by extending the nonparametric formulation of the\nLSSVM, the estimation of the nonlinear function of the GML model can also be\nexpressed by the kernels. A short discussion on the priority of this new\nframework to the existing grey models and LSSVM have also been discussed in\nthis paper. And the perspectives and future orientations of this framework have\nalso been presented.",
        "url": "http://arxiv.org/pdf/1805.01745v2.pdf"
    }
]